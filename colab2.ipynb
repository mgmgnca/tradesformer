{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f99af03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "\n",
    "def convert_1m_csv_to_5m_df(file_path: str) -> pd.DataFrame:\n",
    "    # Column Names (á€žá€„á€·á€ºá€›á€²á€· Data á€¡á€…á€®á€¡á€…á€‰á€ºá€¡á€á€­á€¯á€„á€ºá€¸)\n",
    "    COLUMN_NAMES = ['Date', 'Time', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    DATETIME_FORMAT = '%Y.%m.%d %H:%M'\n",
    "\n",
    "    \"\"\" CSV File á€™á€¾ 1-Minute Data á€€á€­á€¯ Load á€•á€¼á€®á€¸ 5-Minute Candle á€žá€­á€¯á€· á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€žá€Šá€ºá‹ \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"ðŸš¨ Error: File not found at path: {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=',', header=None, names=COLUMN_NAMES,\n",
    "                         dtype={'Open': np.float64, 'High': np.float64, 'Low': np.float64, 'Close': np.float64})\n",
    "        \n",
    "        # Volume column á€€á€­á€¯ á€šá€¬á€šá€® float á€¡á€”á€±á€–á€¼á€„á€·á€º á€žá€á€ºá€™á€¾á€á€ºá€•á€¼á€®á€¸ NA á€™á€»á€¬á€¸á€€á€­á€¯ 0 á€–á€¼á€„á€·á€º á€¡á€…á€¬á€¸á€‘á€­á€¯á€¸á€žá€Šá€ºá‹\n",
    "        df['Volume'] = pd.to_numeric(df['Volume'], errors='coerce').fillna(0)\n",
    "        df['Volume'] = df['Volume'].astype(np.int64) \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ðŸš¨ Error loading CSV file: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Datetime Index á€€á€­á€¯ á€á€Šá€ºá€†á€±á€¬á€€á€ºá€á€¼á€„á€ºá€¸\n",
    "    df['Datetime'] = df['Date'].astype(str) + ' ' + df['Time'].astype(str)\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'], format=DATETIME_FORMAT, errors='coerce')\n",
    "    df.set_index('Datetime', inplace=True)\n",
    "    df.drop(columns=['Date', 'Time'], inplace=True)\n",
    "    #df.dropna(subset=[df.index.name], inplace=True) # Invalid Datetime á€™á€»á€¬á€¸á€€á€­á€¯ á€–á€šá€ºá€›á€¾á€¬á€¸á€á€¼á€„á€ºá€¸\n",
    "\n",
    "    ohlcv_aggregation_rules: Dict[str, Any] = {\n",
    "        'Open': 'first', 'High': 'max', 'Low': 'min', 'Close': 'last', 'Volume': 'sum'\n",
    "    }\n",
    "    df_5m = df.resample('5Min').agg(ohlcv_aggregation_rules)\n",
    "    df_5m.dropna(inplace=True)\n",
    "    df_5m = df_5m[df_5m['Volume'] > 0]\n",
    "    \n",
    "    print(f\"âœ… Conversion successful! 5-Min rows: {len(df_5m)}\")\n",
    "    return df_5m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7510b705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class EnvConfig():\n",
    "    \"\"\"environment configuration from json file\n",
    "       tgym requires you configure your own parameters in json file.\n",
    "        Args:\n",
    "            config_file path/file.json\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,config_file):\n",
    "        self.config = {}\n",
    "        with open(config_file) as j:\n",
    "            self.config = json.load(j)\n",
    "\n",
    "    def env_parameters(self,item=''):\n",
    "        \"\"\"environment variables\n",
    "        \"\"\"\n",
    "        if item:\n",
    "            return self.config[\"env\"][item]\n",
    "        else:\n",
    "            return self.config[\"env\"]\n",
    "\n",
    "    def symbol(self, asset=\"GBPUSD\", item='') :\n",
    "        \"\"\"get trading pair (symbol) information\n",
    "\n",
    "        Args:\n",
    "            asset (str, optional): symbol in config. Defaults to \"GBPUSD\".\n",
    "            item (str, optional): name of item, if '' return dict, else return item value. Defaults to ''.\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        if item:\n",
    "            return self.config[\"symbol\"][asset][item]\n",
    "        else:\n",
    "            return self.config[\"symbol\"][asset]\n",
    "\n",
    "    def data_processing_parameters(self, item=''):\n",
    "        \"\"\"Get data processing config\"\"\"\n",
    "        if item:\n",
    "            return self.config[\"data_processing\"][item]\n",
    "        return self.config[\"data_processing\"]\n",
    "\n",
    "    def trading_hour(self,place=\"NewYork\"):\n",
    "        \"\"\"forex trading hour from different markets\n",
    "\n",
    "        Args:\n",
    "            place (str, optional): [Sydney,Tokyo,London] Defaults to \"New York\".\n",
    "\n",
    "        Returns:\n",
    "            [dict]: from time, to time\n",
    "        \"\"\"\n",
    "        if place:\n",
    "            return self.config[\"trading_hour\"][place]\n",
    "        else:\n",
    "            return self.config[\"trading_hour\"]\n",
    "        \n",
    "    def indicator(self,place=\"sma_fast_period\"):\n",
    "        \"\"\"forex trading hour from different markets\n",
    "\n",
    "        Args:\n",
    "            place (str, optional): [Sydney,Tokyo,London] Defaults to \"New York\".\n",
    "\n",
    "        Returns:\n",
    "            [dict]: from time, to time\n",
    "        \"\"\"\n",
    "        if place:\n",
    "            return self.config[\"data_processing\"][\"indicator\"][place]\n",
    "        else:\n",
    "            return self.config[\"data_processing\"][\"indicator\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "293760f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "from finta import TA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import holidays\n",
    "import os\n",
    "import json\n",
    "\n",
    "import logging\n",
    "# Configure logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def patch_missing_data(df, dt_col_name='time', cf=None):\n",
    "    min_bars = cf.data_processing_parameters(\"min_bars_per_week\")\n",
    "\n",
    "    # [\"time\",\"open\", \"high\", \"low\", \"close\"]\n",
    "    required_cols = cf.data_processing_parameters(\"required_cols\")\n",
    "\n",
    "    # df á€™á€¾á€¬ 6 columns á€›á€¾á€­á€›á€„á€º vol á€•á€«á€‘á€Šá€·á€ºá€™á€šá€º\n",
    "    if df.shape[1] == 6:\n",
    "        df.columns = required_cols + ['vol']\n",
    "    elif df.shape[1] == 5:\n",
    "        df.columns = required_cols\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid number of columns: {df.shape[1]} =>{required_cols}\")\n",
    "\n",
    "    logger.warning(f\"shape of  column: {df.shape[1]}\")\n",
    "    # 1. Column validation\n",
    "    if missing := set(required_cols) - set(df.columns):\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "    # 2. Auto-detect datetime column\n",
    "    dt_candidates = {'time', 'timestamp', 'date', 'datetime'}\n",
    "    if dt_col_name not in df.columns:\n",
    "        found = list(dt_candidates & set(df.columns))\n",
    "        if not found:\n",
    "            raise KeyError(f\"No datetime column found. Tried: {dt_candidates}\")\n",
    "        dt_col_name = found[0]\n",
    "        logger.info(f\"Using datetime column: {dt_col_name}\")\n",
    "\n",
    "    # 3. Convert to datetime index\n",
    "    df[dt_col_name] = pd.to_datetime(df[dt_col_name], utc=True)\n",
    "    df = df.set_index(dt_col_name).sort_index()\n",
    "    groups = df.groupby(pd.Grouper(freq='W-SUN'))\n",
    "\n",
    "    patched_weeks = []  # patched weekly df storage\n",
    "\n",
    "    for w, week_df in groups:\n",
    "        if week_df.empty:\n",
    "            continue\n",
    "\n",
    "        if len(week_df) != min_bars:\n",
    "            logger.warning(f\"Week {w} has {len(week_df)}/{min_bars} bars\")\n",
    "\n",
    "        # Create 5-minute frequency index\n",
    "        new_index = pd.date_range(\n",
    "            start=week_df.index.min(),\n",
    "            end=week_df.index.max(),\n",
    "            freq='5min',\n",
    "            tz='UTC'\n",
    "        )\n",
    "\n",
    "        # Reindex + forward fill\n",
    "        week_df = week_df.reindex(new_index)\n",
    "        week_df.index = week_df.index.tz_localize(None)\n",
    "        fill_limit = 12 # á€¥á€•á€™á€¬: 1 á€”á€¬á€›á€® (12 bars) á€‘á€€á€ºá€•á€­á€¯á€á€²á€· á€€á€½á€€á€ºá€œá€•á€ºá€€á€­á€¯ á€™á€–á€¼á€Šá€·á€ºá€•á€«\n",
    "        fill_cols = ['open', 'high', 'low', 'close', 'vol'] if 'vol' in df.columns else ['open', 'high', 'low', 'close']\n",
    "        # FFill: á€›á€¾á€±á€·á€€ data á€–á€¼á€„á€·á€º á€–á€¼á€Šá€·á€ºá€•á€«\n",
    "        week_df[fill_cols] = week_df[fill_cols].ffill(limit=fill_limit)\n",
    "        patched_weeks.append(week_df)\n",
    "\n",
    "    # Merge back all weeks\n",
    "    if patched_weeks:\n",
    "        all_df = pd.concat(patched_weeks)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ea8bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_time_feature(df_5m: pd.DataFrame, cf=None, source_tz='UTC') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    5M Data Frame (DatetimeIndex á€•á€«á€á€„á€ºá€žá€Šá€ºá€Ÿá€¯ á€šá€°á€†á€•á€«) á€¡á€á€½á€€á€º Temporal features á€‘á€Šá€·á€ºá€žá€½á€„á€ºá€¸á€á€¼á€„á€ºá€¸á‹\n",
    "    \"\"\"\n",
    "    \n",
    "    # DataFrame á Index á€€á€­á€¯ DatetimeIndex á€¡á€–á€¼á€…á€º á€…á€…á€ºá€†á€±á€¸á€á€¼á€„á€ºá€¸\n",
    "    if not isinstance(df_5m.index, pd.DatetimeIndex):\n",
    "         raise TypeError(\"DataFrame á Index á€žá€Šá€º DatetimeIndex á€–á€¼á€…á€ºá€›á€•á€«á€™á€Šá€ºá‹\")\n",
    "\n",
    "    df_5m.index = df_5m.index.tz_localize(None)\n",
    "    # Index á€€á€­á€¯ Timezone aware (UTC) á€¡á€–á€¼á€…á€º á€žá€±á€á€»á€¬á€¡á€±á€¬á€„á€ºá€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸\n",
    "    if df_5m.index.tz is None:\n",
    "        # Timezone-Naive data á€€á€­á€¯ á€™á€°á€›á€„á€ºá€¸ Source Timezone á€–á€¼á€„á€·á€º localize\n",
    "        # Dukascopy data á€†á€­á€¯á€›á€„á€º 'UTC' á€žá€¯á€¶á€¸á€•á€¼á€®á€¸áŠ Broker data á€†á€­á€¯á€›á€„á€º 'GMT+3' á€œá€­á€¯á€™á€»á€­á€¯á€¸ á€žá€¯á€¶á€¸á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€º\n",
    "        df = df_5m.tz_localize(source_tz, ambiguous='NaT', nonexistent='NaT')\n",
    "        df = df.tz_convert('UTC')\n",
    "    else:\n",
    "        df = df_5m.copy()\n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    # I. á€¡á€á€¼á€±á€á€¶ features á€”á€¾á€„á€·á€º Cyclical Encoding á€™á€»á€¬á€¸ (Hour á€€á€­á€¯ Index á€™á€¾ á€á€­á€¯á€€á€ºá€›á€­á€¯á€€á€ºá€šá€°á€á€¼á€„á€ºá€¸)\n",
    "    # ----------------------------------------------------\n",
    "\n",
    "    df['weekday'] = df.index.dayofweek \n",
    "    df['day'] = df.index.day\n",
    "    df['week'] = df.index.isocalendar().week.astype(int)\n",
    "    df['month'] = df.index.month\n",
    "    df['year'] = df.index.year\n",
    "    df['hour'] = df.index.hour\n",
    "    \n",
    "    # á€”á€¬á€›á€®á€¡á€á€½á€€á€º Cyclical features\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24).round(6)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24).round(6)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # III. DST-Aware Market Sessions (Timezone Handling)\n",
    "    # ----------------------------------------------------\n",
    "    \n",
    "    # á€”á€¬á€›á€®á€€á€­á€¯ local time zone á€žá€­á€¯á€· á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€² (Timezone Aware Index á€™á€¾á€žá€¬ tz_convert á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€žá€Šá€º)\n",
    "    london_time = df.index.tz_convert('Europe/London')\n",
    "    ny_time = df.index.tz_convert('America/New_York')\n",
    "\n",
    "    # Session Hours (cf á€™á€¾ Local Time á€”á€¬á€›á€®á€™á€»á€¬á€¸á€€á€­á€¯ á€•á€±á€¸á€•á€­á€¯á€·á€›á€•á€«á€™á€Šá€ºá‹)\n",
    "    ny = cf.trading_hour('NewYork')\n",
    "    ldn = cf.trading_hour('London')\n",
    "\n",
    "    # London Session (Local Time: 08:00 - 16:00)\n",
    "    df['london_session'] = ((london_time.hour >= ldn['from']) & (london_time.hour < ldn['to'])).astype(int)\n",
    "    \n",
    "    # NY Session (Local Time: 13:00 - 21:00 UTC/GMT) -> (9:00 - 17:00 EST/EDT)\n",
    "    # cf á€™á€¾ Local NY Time (á€¥á€•á€™á€¬: 9, 17) á€€á€­á€¯ á€•á€±á€¸á€•á€­á€¯á€·á€›á€™á€Šá€º\n",
    "    df['ny_session'] = ((ny_time.hour >= ny['from']) & (ny_time.hour < ny['to'])).astype(int)\n",
    "\n",
    "    df['overlap_session'] = (df['london_session'] & df['ny_session']).astype(int)\n",
    "\n",
    "    # ... (IV. Holiday features á€€á€­á€¯ á€†á€€á€ºá€œá€€á€ºá€‘á€Šá€·á€ºá€žá€½á€„á€ºá€¸á€”á€­á€¯á€„á€ºá€žá€Šá€º) ...\n",
    "    \n",
    "    #df['symbol'] = symbol\n",
    "    \n",
    "    # á€šá€¬á€šá€® columns á€™á€»á€¬á€¸á€€á€­á€¯ á€–á€šá€ºá€›á€¾á€¬á€¸á€á€¼á€„á€ºá€¸\n",
    "    df = df.drop(columns=['minute_block_15'], errors='ignore') # minute_block_15 á€žá€Šá€º 1M data á€™á€¾ á€œá€¬á€œá€»á€¾á€„á€ºá€žá€¬ á€œá€­á€¯á€¡á€•á€ºá€žá€Šá€ºá‹ 5M á€á€½á€„á€º á€™á€œá€­á€¯á€¡á€•á€ºá€•á€«á‹\n",
    "    \n",
    "    # Index á€€á€­á€¯ reset á€™á€œá€¯á€•á€ºá€˜á€² á€•á€¼á€”á€ºá€•á€­á€¯á€·á€•á€« (Env á€¡á€á€½á€€á€º Datetime Index á€œá€­á€¯á€¡á€•á€ºá€žá€Šá€º)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b91d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finta import TA\n",
    "\n",
    "def tech_indicators(df, cf=None):\n",
    "    \"\"\"\n",
    "    Forex RL á€¡á€á€½á€€á€º Price ActionáŠ Momentum á€”á€¾á€„á€·á€º Long-Term Trend Features á€™á€»á€¬á€¸á€€á€­á€¯ á€‘á€Šá€·á€ºá€žá€½á€„á€ºá€¸á€á€¼á€„á€ºá€¸á‹\n",
    "    \"\"\"\n",
    "    \n",
    "    sma_fast_period = cf.indicator('sma_fast_period')\n",
    "    sma_mid_period = cf.indicator('sma_mid_period')\n",
    "    sma_slow_period = cf.indicator('sma_slow_period')\n",
    "    atr_period = cf.indicator('atr_period')\n",
    "    rsi_period = cf.indicator('rsi_period')\n",
    "    macd_fast_period = cf.indicator('macd_fast_period')\n",
    "    macd_slow_period = cf.indicator('macd_slow_period')\n",
    "    macd_signal_period = cf.indicator('macd_signal_period')\n",
    "    adx_period = cf.indicator('adx_period')\n",
    "    stoch_period = cf.indicator('stoch_period')\n",
    "\n",
    "    \n",
    "    # --- áá‹ Volatility Measure (ATR á€€á€­á€¯ Base á€¡á€–á€¼á€…á€º á€¡á€žá€¯á€¶á€¸á€•á€¼á€¯á€›á€”á€º) ---\n",
    "    df['atr_base'] = TA.ATR(df, period=atr_period).ffill()\n",
    "\n",
    "    # --- á‚á‹ Price Action Features ---\n",
    "    df['log_returns'] = np.log(df['close'] / df['close'].shift(1)).ffill().round(6)\n",
    "    df['price_norm'] = (df['close'] - df['close'].shift(sma_fast_period)) / df['atr_base'] # 100-bar SMA á€€á€­á€¯ á€á€”á€·á€ºá€™á€¾á€”á€ºá€¸á€á€¼á€±á€žá€¯á€¶á€¸\n",
    "    df['spread_ratio'] = (df['high'] - df['low']) / df['atr_base']\n",
    "    df['body_ratio'] = (df['close'] - df['open']) / (df['high'] - df['low']).replace(0, 1e-6)\n",
    "\n",
    "    # --- áƒá‹ Momentum & Trend Features (SMA 100/200 Cross á€€á€­á€¯ á€‘á€Šá€·á€ºá€žá€½á€„á€ºá€¸á€á€¼á€„á€ºá€¸) ---\n",
    "\n",
    "    df['rsi'] = TA.RSI(df, period=rsi_period).ffill().round(6)\n",
    "    macd_data = TA.MACD(df, period_fast=macd_fast_period, period_slow=macd_slow_period, signal=macd_signal_period)\n",
    "    df['macd_hist'] = macd_data.SIGNAL.ffill().round(6)\n",
    "    \n",
    "    # Trend á€›á€¾á€­á€á€¼á€„á€ºá€¸/á€™á€›á€¾á€­á€á€¼á€„á€ºá€¸ á€”á€¾á€„á€·á€º Trend á á€á€­á€¯á€„á€ºá€™á€¬á€™á€¾á€¯ á€€á€­á€¯ á€á€­á€¯á€„á€ºá€¸á€á€¬á€á€¼á€„á€ºá€¸á‹\n",
    "    # SMA Cross á€€ Trend Direction á€€á€­á€¯ á€•á€±á€¸á€žá€±á€¬á€ºá€œá€Šá€ºá€¸áŠ \n",
    "    # ADX á€€ Direction á€›á€²á€· Strength á€€á€­á€¯ á€•á€±á€¸á€žá€Šá€ºá‹ \n",
    "    # ADX á€”á€­á€™á€·á€ºá€•á€«á€€ Range/Sideways á€–á€¼á€…á€ºá€•á€¼á€®á€¸áŠ \n",
    "    # ADX á€™á€¼á€„á€·á€ºá€•á€«á€€ Strong Trend á€–á€¼á€…á€ºá€€á€¼á€±á€¬á€„á€ºá€¸ Agent á€€á€­á€¯ á€žá€­á€…á€±á€žá€Šá€ºá‹\n",
    "    df['adx'] = TA.ADX(df, period=adx_period)  \n",
    "    \n",
    "    # RSI á€–á€¼á€„á€·á€º Overbought/Oversold á€€á€­á€¯ á€…á€…á€ºá€†á€±á€¸á€•á€¼á€®á€¸áŠ \n",
    "    # Stochastic á€–á€¼á€„á€·á€º Momentum Change á€€á€­á€¯ á€¡á€á€Šá€ºá€•á€¼á€¯á€”á€­á€¯á€„á€ºá€žá€Šá€ºá‹ \n",
    "    # á€”á€¾á€…á€ºá€á€¯á€…á€œá€¯á€¶á€¸ Extreme Level á€á€½á€„á€º á€›á€¾á€­á€”á€±á€•á€«á€€ Reversal á€–á€¼á€…á€ºá€”á€­á€¯á€„á€ºá€á€¼á€± á€•á€­á€¯á€™á€»á€¬á€¸á€€á€¼á€±á€¬á€„á€ºá€¸ Agent á€€ á€žá€„á€ºá€šá€°á€”á€­á€¯á€„á€ºá€žá€Šá€ºá‹\n",
    "    df['stoch_k'] = TA.STOCH(df, period=stoch_period)   \n",
    "    # *** á€žá€„á€ºá€á€±á€¬á€„á€ºá€¸á€†á€­á€¯á€žá€±á€¬ Long-Term SMA Cross Feature á€¡á€žá€…á€º ***\n",
    "\n",
    "    ma_mid = TA.SMA(df, period=sma_mid_period)\n",
    "    ma_slow = TA.SMA(df, period=sma_slow_period)\n",
    "    \n",
    "    # SMA 100 á€”á€¾á€„á€·á€º SMA 200 á€á€­á€¯á€·á á€€á€½á€¬á€á€¼á€¬á€¸á€á€»á€€á€ºá€€á€­á€¯ ATR á€–á€¼á€„á€·á€º Normalization á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸\n",
    "    df['ma_cross'] = ((ma_mid - ma_slow) / df['atr_base']).ffill().round(6)\n",
    "    \n",
    "    # --- á„á‹ Data Cleaning ---\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).fillna(0) \n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].clip(lower=-1e5, upper=1e5).round(6)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65b023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_from_data = \"./drive/MyDrive/data/raw/EURUSD_2020_m1_all.csv\"\n",
    "file_path_to_data = \"./drive/MyDrive/data/raw/EURUSD_2020_m5_all.csv\"\n",
    "\n",
    "df_all_5m_data = convert_1m_csv_to_5m_df(file_path_from_data)\n",
    "df_all_5m_data.to_csv(file_path_to_data)\n",
    "\n",
    "\n",
    "cf = EnvConfig('./drive/MyDrive/configure.json')\n",
    "\n",
    "raw = pd.read_csv(file_path_to_data)\n",
    "\n",
    "df = patch_missing_data(raw,cf=cf)\n",
    "\n",
    "symbol = 'EURUSD'\n",
    "# Broker Data (00:00 á€™á€¾ á€…á€žá€±á€¬) á€€á€­á€¯ á€á€±á€«á€ºá€†á€­á€¯á€žá€Šá€·á€ºá€¡á€á€«\n",
    "# GMT+2/GMT+3 á€€á€­á€¯ á€¡á€œá€­á€¯á€¡á€œá€»á€±á€¬á€€á€º á€€á€­á€¯á€„á€ºá€á€½á€šá€ºá€•á€±á€¸á€›á€”á€º\n",
    "axiory_tz = 'Europe/Kiev'  \n",
    "\n",
    "dft = add_time_feature(df, cf=cf, source_tz=axiory_tz)\n",
    "dft.head(3)\n",
    "\n",
    "\n",
    "\n",
    "cf = EnvConfig('./drive/MyDrive/configure.json')\n",
    "\n",
    "df = tech_indicators(dft, cf=cf)\n",
    "df.to_csv('tech.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bff762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f99af03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def convert_1m_csv_to_5m_df(file_path: str) -> pd.DataFrame:\n",
    "    # Column Names (·Äû·ÄÑ·Ä∑·Ä∫·Äõ·Ä≤·Ä∑ Data ·Ä°·ÄÖ·ÄÆ·Ä°·ÄÖ·Äâ·Ä∫·Ä°·Äê·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏)\n",
    "    COLUMN_NAMES = ['Date', 'Time', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    DATETIME_FORMAT = '%Y.%m.%d %H:%M'\n",
    "\n",
    "    \"\"\" CSV File ·Äô·Äæ 1-Minute Data ·ÄÄ·Ä≠·ÄØ Load ·Äï·Äº·ÄÆ·Ä∏ 5-Minute Candle ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äú·Ä≤·Äû·Ää·Ä∫·Åã \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"üö® Error: File not found at path: {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=',', header=None, names=COLUMN_NAMES,\n",
    "                         dtype={'Open': np.float64, 'High': np.float64, 'Low': np.float64, 'Close': np.float64})\n",
    "        \n",
    "        # Volume column ·ÄÄ·Ä≠·ÄØ ·Äö·Ä¨·Äö·ÄÆ float ·Ä°·Äî·Ä±·Äñ·Äº·ÄÑ·Ä∑·Ä∫ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äï·Äº·ÄÆ·Ä∏ NA ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ 0 ·Äñ·Äº·ÄÑ·Ä∑·Ä∫ ·Ä°·ÄÖ·Ä¨·Ä∏·Äë·Ä≠·ÄØ·Ä∏·Äû·Ää·Ä∫·Åã\n",
    "        df['Volume'] = pd.to_numeric(df['Volume'], errors='coerce').fillna(0)\n",
    "        df['Volume'] = df['Volume'].astype(np.int64) \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"üö® Error loading CSV file: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Datetime Index ·ÄÄ·Ä≠·ÄØ ·Äê·Ää·Ä∫·ÄÜ·Ä±·Ä¨·ÄÄ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "    df['Datetime'] = df['Date'].astype(str) + ' ' + df['Time'].astype(str)\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'], format=DATETIME_FORMAT, errors='coerce')\n",
    "    df.set_index('Datetime', inplace=True)\n",
    "    df.drop(columns=['Date', 'Time'], inplace=True)\n",
    "    #df.dropna(subset=[df.index.name], inplace=True) # Invalid Datetime ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Äñ·Äö·Ä∫·Äõ·Äæ·Ä¨·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "\n",
    "    ohlcv_aggregation_rules: Dict[str, Any] = {\n",
    "        'Open': 'first', 'High': 'max', 'Low': 'min', 'Close': 'last', 'Volume': 'sum'\n",
    "    }\n",
    "    df_5m = df.resample('5Min').agg(ohlcv_aggregation_rules)\n",
    "    df_5m.dropna(inplace=True)\n",
    "    df_5m = df_5m[df_5m['Volume'] > 0]\n",
    "    \n",
    "    print(f\"‚úÖ Conversion successful! 5-Min rows: {len(df_5m)}\")\n",
    "    return df_5m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7510b705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class EnvConfig():\n",
    "    \"\"\"environment configuration from json file\n",
    "       tgym requires you configure your own parameters in json file.\n",
    "        Args:\n",
    "            config_file path/file.json\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,config_file):\n",
    "        self.config = {}\n",
    "        with open(config_file) as j:\n",
    "            self.config = json.load(j)\n",
    "\n",
    "    def env_parameters(self,item=''):\n",
    "        \"\"\"environment variables\n",
    "        \"\"\"\n",
    "        if item:\n",
    "            return self.config[\"env\"][item]\n",
    "        else:\n",
    "            return self.config[\"env\"]\n",
    "\n",
    "    def symbol(self, asset=\"GBPUSD\", item='') :\n",
    "        \"\"\"get trading pair (symbol) information\n",
    "\n",
    "        Args:\n",
    "            asset (str, optional): symbol in config. Defaults to \"GBPUSD\".\n",
    "            item (str, optional): name of item, if '' return dict, else return item value. Defaults to ''.\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        if item:\n",
    "            return self.config[\"symbol\"][asset][item]\n",
    "        else:\n",
    "            return self.config[\"symbol\"][asset]\n",
    "\n",
    "    def data_processing_parameters(self, item=''):\n",
    "        \"\"\"Get data processing config\"\"\"\n",
    "        if item:\n",
    "            return self.config[\"data_processing\"][item]\n",
    "        return self.config[\"data_processing\"]\n",
    "\n",
    "    def trading_hour(self,place=\"NewYork\"):\n",
    "        \"\"\"forex trading hour from different markets\n",
    "\n",
    "        Args:\n",
    "            place (str, optional): [Sydney,Tokyo,London] Defaults to \"New York\".\n",
    "\n",
    "        Returns:\n",
    "            [dict]: from time, to time\n",
    "        \"\"\"\n",
    "        if place:\n",
    "            return self.config[\"trading_hour\"][place]\n",
    "        else:\n",
    "            return self.config[\"trading_hour\"]\n",
    "        \n",
    "    def indicator(self,place=\"sma_fast_period\"):\n",
    "        \"\"\"forex trading hour from different markets\n",
    "\n",
    "        Args:\n",
    "            place (str, optional): [Sydney,Tokyo,London] Defaults to \"New York\".\n",
    "\n",
    "        Returns:\n",
    "            [dict]: from time, to time\n",
    "        \"\"\"\n",
    "        if place:\n",
    "            return self.config[\"data_processing\"][\"indicator\"][place]\n",
    "        else:\n",
    "            return self.config[\"data_processing\"][\"indicator\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "293760f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "from finta import TA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import holidays\n",
    "import os\n",
    "import json\n",
    "\n",
    "import logging\n",
    "# Configure logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def patch_missing_data(df, dt_col_name='time', cf=None):\n",
    "    min_bars = cf.data_processing_parameters(\"min_bars_per_week\")\n",
    "\n",
    "    # [\"time\",\"open\", \"high\", \"low\", \"close\"]\n",
    "    required_cols = cf.data_processing_parameters(\"required_cols\")\n",
    "\n",
    "    # df ·Äô·Äæ·Ä¨ 6 columns ·Äõ·Äæ·Ä≠·Äõ·ÄÑ·Ä∫ vol ·Äï·Ä´·Äë·Ää·Ä∑·Ä∫·Äô·Äö·Ä∫\n",
    "    if df.shape[1] == 6:\n",
    "        df.columns = required_cols + ['vol']\n",
    "    elif df.shape[1] == 5:\n",
    "        df.columns = required_cols\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid number of columns: {df.shape[1]} =>{required_cols}\")\n",
    "\n",
    "    logger.warning(f\"shape of  column: {df.shape[1]}\")\n",
    "    # 1. Column validation\n",
    "    if missing := set(required_cols) - set(df.columns):\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "    # 2. Auto-detect datetime column\n",
    "    dt_candidates = {'time', 'timestamp', 'date', 'datetime'}\n",
    "    if dt_col_name not in df.columns:\n",
    "        found = list(dt_candidates & set(df.columns))\n",
    "        if not found:\n",
    "            raise KeyError(f\"No datetime column found. Tried: {dt_candidates}\")\n",
    "        dt_col_name = found[0]\n",
    "        logger.info(f\"Using datetime column: {dt_col_name}\")\n",
    "\n",
    "    # 3. Convert to datetime index\n",
    "    df[dt_col_name] = pd.to_datetime(df[dt_col_name], utc=True)\n",
    "    df = df.set_index(dt_col_name).sort_index()\n",
    "    groups = df.groupby(pd.Grouper(freq='W-SUN'))\n",
    "\n",
    "    patched_weeks = []  # patched weekly df storage\n",
    "\n",
    "    for w, week_df in groups:\n",
    "        if week_df.empty:\n",
    "            continue\n",
    "\n",
    "        if len(week_df) != min_bars:\n",
    "            logger.warning(f\"Week {w} has {len(week_df)}/{min_bars} bars\")\n",
    "\n",
    "        # Create 5-minute frequency index\n",
    "        new_index = pd.date_range(\n",
    "            start=week_df.index.min(),\n",
    "            end=week_df.index.max(),\n",
    "            freq='5min',\n",
    "            tz='UTC'\n",
    "        )\n",
    "\n",
    "        # Reindex + forward fill\n",
    "        week_df = week_df.reindex(new_index)\n",
    "        week_df.index = week_df.index.tz_localize(None)\n",
    "        fill_limit = 12 # ·Ä•·Äï·Äô·Ä¨: 1 ·Äî·Ä¨·Äõ·ÄÆ (12 bars) ·Äë·ÄÄ·Ä∫·Äï·Ä≠·ÄØ·Äê·Ä≤·Ä∑ ·ÄÄ·ÄΩ·ÄÄ·Ä∫·Äú·Äï·Ä∫·ÄÄ·Ä≠·ÄØ ·Äô·Äñ·Äº·Ää·Ä∑·Ä∫·Äï·Ä´\n",
    "        fill_cols = ['open', 'high', 'low', 'close', 'vol'] if 'vol' in df.columns else ['open', 'high', 'low', 'close']\n",
    "        # FFill: ·Äõ·Äæ·Ä±·Ä∑·ÄÄ data ·Äñ·Äº·ÄÑ·Ä∑·Ä∫ ·Äñ·Äº·Ää·Ä∑·Ä∫·Äï·Ä´\n",
    "        week_df[fill_cols] = week_df[fill_cols].ffill(limit=fill_limit)\n",
    "        patched_weeks.append(week_df)\n",
    "\n",
    "    # Merge back all weeks\n",
    "    if patched_weeks:\n",
    "        all_df = pd.concat(patched_weeks)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "52ea8bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_time_feature(df_5m: pd.DataFrame, cf=None, source_tz='UTC') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    5M Data Frame (DatetimeIndex ·Äï·Ä´·Äù·ÄÑ·Ä∫·Äû·Ää·Ä∫·Äü·ÄØ ·Äö·Ä∞·ÄÜ·Äï·Ä´) ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ Temporal features ·Äë·Ää·Ä∑·Ä∫·Äû·ÄΩ·ÄÑ·Ä∫·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏·Åã\n",
    "    \"\"\"\n",
    "    \n",
    "    # DataFrame ·Åè Index ·ÄÄ·Ä≠·ÄØ DatetimeIndex ·Ä°·Äñ·Äº·ÄÖ·Ä∫ ·ÄÖ·ÄÖ·Ä∫·ÄÜ·Ä±·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "    if not isinstance(df_5m.index, pd.DatetimeIndex):\n",
    "         raise TypeError(\"DataFrame ·Åè Index ·Äû·Ää·Ä∫ DatetimeIndex ·Äñ·Äº·ÄÖ·Ä∫·Äõ·Äï·Ä´·Äô·Ää·Ä∫·Åã\")\n",
    "\n",
    "    df_5m.index = df_5m.index.tz_localize(None)\n",
    "    # Index ·ÄÄ·Ä≠·ÄØ Timezone aware (UTC) ·Ä°·Äñ·Äº·ÄÖ·Ä∫ ·Äû·Ä±·ÄÅ·Äª·Ä¨·Ä°·Ä±·Ä¨·ÄÑ·Ä∫·Äú·ÄØ·Äï·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "    if df_5m.index.tz is None:\n",
    "        # Timezone-Naive data ·ÄÄ·Ä≠·ÄØ ·Äô·Ä∞·Äõ·ÄÑ·Ä∫·Ä∏ Source Timezone ·Äñ·Äº·ÄÑ·Ä∑·Ä∫ localize\n",
    "        # Dukascopy data ·ÄÜ·Ä≠·ÄØ·Äõ·ÄÑ·Ä∫ 'UTC' ·Äû·ÄØ·Ä∂·Ä∏·Äï·Äº·ÄÆ·Ä∏·Åä Broker data ·ÄÜ·Ä≠·ÄØ·Äõ·ÄÑ·Ä∫ 'GMT+3' ·Äú·Ä≠·ÄØ·Äô·Äª·Ä≠·ÄØ·Ä∏ ·Äû·ÄØ·Ä∂·Ä∏·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫\n",
    "        df = df_5m.tz_localize(source_tz, ambiguous='NaT', nonexistent='NaT')\n",
    "        df = df.tz_convert('UTC')\n",
    "    else:\n",
    "        df = df_5m.copy()\n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    # I. ·Ä°·ÄÅ·Äº·Ä±·ÄÅ·Ä∂ features ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ Cyclical Encoding ·Äô·Äª·Ä¨·Ä∏ (Hour ·ÄÄ·Ä≠·ÄØ Index ·Äô·Äæ ·Äê·Ä≠·ÄØ·ÄÄ·Ä∫·Äõ·Ä≠·ÄØ·ÄÄ·Ä∫·Äö·Ä∞·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏)\n",
    "    # ----------------------------------------------------\n",
    "\n",
    "    df['weekday'] = df.index.dayofweek \n",
    "    df['day'] = df.index.day\n",
    "    df['week'] = df.index.isocalendar().week.astype(int)\n",
    "    df['month'] = df.index.month\n",
    "    df['year'] = df.index.year\n",
    "    df['hour'] = df.index.hour\n",
    "    \n",
    "    # ·Äî·Ä¨·Äõ·ÄÆ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ Cyclical features\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24).round(6)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24).round(6)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # III. DST-Aware Market Sessions (Timezone Handling)\n",
    "    # ----------------------------------------------------\n",
    "    \n",
    "    # ·Äî·Ä¨·Äõ·ÄÆ·ÄÄ·Ä≠·ÄØ local time zone ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äú·Ä≤ (Timezone Aware Index ·Äô·Äæ·Äû·Ä¨ tz_convert ·Äú·ÄØ·Äï·Ä∫·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äû·Ää·Ä∫)\n",
    "    london_time = df.index.tz_convert('Europe/London')\n",
    "    ny_time = df.index.tz_convert('America/New_York')\n",
    "\n",
    "    # Session Hours (cf ·Äô·Äæ Local Time ·Äî·Ä¨·Äõ·ÄÆ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Äï·Ä±·Ä∏·Äï·Ä≠·ÄØ·Ä∑·Äõ·Äï·Ä´·Äô·Ää·Ä∫·Åã)\n",
    "    ny = cf.trading_hour('NewYork')\n",
    "    ldn = cf.trading_hour('London')\n",
    "\n",
    "    # London Session (Local Time: 08:00 - 16:00)\n",
    "    df['london_session'] = ((london_time.hour >= ldn['from']) & (london_time.hour < ldn['to'])).astype(int)\n",
    "    \n",
    "    # NY Session (Local Time: 13:00 - 21:00 UTC/GMT) -> (9:00 - 17:00 EST/EDT)\n",
    "    # cf ·Äô·Äæ Local NY Time (·Ä•·Äï·Äô·Ä¨: 9, 17) ·ÄÄ·Ä≠·ÄØ ·Äï·Ä±·Ä∏·Äï·Ä≠·ÄØ·Ä∑·Äõ·Äô·Ää·Ä∫\n",
    "    df['ny_session'] = ((ny_time.hour >= ny['from']) & (ny_time.hour < ny['to'])).astype(int)\n",
    "\n",
    "    df['overlap_session'] = (df['london_session'] & df['ny_session']).astype(int)\n",
    "\n",
    "    # ... (IV. Holiday features ·ÄÄ·Ä≠·ÄØ ·ÄÜ·ÄÄ·Ä∫·Äú·ÄÄ·Ä∫·Äë·Ää·Ä∑·Ä∫·Äû·ÄΩ·ÄÑ·Ä∫·Ä∏·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äû·Ää·Ä∫) ...\n",
    "    \n",
    "    #df['symbol'] = symbol\n",
    "    \n",
    "    # ·Äö·Ä¨·Äö·ÄÆ columns ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Äñ·Äö·Ä∫·Äõ·Äæ·Ä¨·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "    df = df.drop(columns=['minute_block_15'], errors='ignore') # minute_block_15 ·Äû·Ää·Ä∫ 1M data ·Äô·Äæ ·Äú·Ä¨·Äú·Äª·Äæ·ÄÑ·Ä∫·Äû·Ä¨ ·Äú·Ä≠·ÄØ·Ä°·Äï·Ä∫·Äû·Ää·Ä∫·Åã 5M ·Äê·ÄΩ·ÄÑ·Ä∫ ·Äô·Äú·Ä≠·ÄØ·Ä°·Äï·Ä∫·Äï·Ä´·Åã\n",
    "    \n",
    "    # Index ·ÄÄ·Ä≠·ÄØ reset ·Äô·Äú·ÄØ·Äï·Ä∫·Äò·Ä≤ ·Äï·Äº·Äî·Ä∫·Äï·Ä≠·ÄØ·Ä∑·Äï·Ä´ (Env ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ Datetime Index ·Äú·Ä≠·ÄØ·Ä°·Äï·Ä∫·Äû·Ää·Ä∫)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6b7b91d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finta import TA\n",
    "\n",
    "def tech_indicators(df, cf=None):\n",
    "    \"\"\"\n",
    "    Forex RL ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ Price Action·Åä Momentum ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ Long-Term Trend Features ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Äë·Ää·Ä∑·Ä∫·Äû·ÄΩ·ÄÑ·Ä∫·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏·Åã\n",
    "    \"\"\"\n",
    "    \n",
    "    sma_fast_period = cf.indicator('sma_fast_period')\n",
    "    sma_mid_period = cf.indicator('sma_mid_period')\n",
    "    sma_slow_period = cf.indicator('sma_slow_period')\n",
    "    atr_period = cf.indicator('atr_period')\n",
    "    rsi_period = cf.indicator('rsi_period')\n",
    "    macd_fast_period = cf.indicator('macd_fast_period')\n",
    "    macd_slow_period = cf.indicator('macd_slow_period')\n",
    "    macd_signal_period = cf.indicator('macd_signal_period')\n",
    "    adx_period = cf.indicator('adx_period')\n",
    "    stoch_period = cf.indicator('stoch_period')\n",
    "\n",
    "    \n",
    "    # --- ·ÅÅ·Åã Volatility Measure (ATR ·ÄÄ·Ä≠·ÄØ Base ·Ä°·Äñ·Äº·ÄÖ·Ä∫ ·Ä°·Äû·ÄØ·Ä∂·Ä∏·Äï·Äº·ÄØ·Äõ·Äî·Ä∫) ---\n",
    "    df['atr_base'] = TA.ATR(df, period=atr_period).ffill()\n",
    "\n",
    "    # --- ·ÅÇ·Åã Price Action Features ---\n",
    "    df['log_returns'] = np.log(df['close'] / df['close'].shift(1)).ffill().round(6)\n",
    "    df['price_norm'] = (df['close'] - df['close'].shift(sma_fast_period)) / df['atr_base'] # 100-bar SMA ·ÄÄ·Ä≠·ÄØ ·ÄÅ·Äî·Ä∑·Ä∫·Äô·Äæ·Äî·Ä∫·Ä∏·ÄÅ·Äº·Ä±·Äû·ÄØ·Ä∂·Ä∏\n",
    "    df['spread_ratio'] = (df['high'] - df['low']) / df['atr_base']\n",
    "    df['body_ratio'] = (df['close'] - df['open']) / (df['high'] - df['low']).replace(0, 1e-6)\n",
    "\n",
    "    # --- ·ÅÉ·Åã Momentum & Trend Features (SMA 100/200 Cross ·ÄÄ·Ä≠·ÄØ ·Äë·Ää·Ä∑·Ä∫·Äû·ÄΩ·ÄÑ·Ä∫·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏) ---\n",
    "\n",
    "    df['rsi'] = TA.RSI(df, period=rsi_period).ffill().round(6)\n",
    "    macd_data = TA.MACD(df, period_fast=macd_fast_period, period_slow=macd_slow_period, signal=macd_signal_period)\n",
    "    df['macd_hist'] = macd_data.SIGNAL.ffill().round(6)\n",
    "    \n",
    "    # Trend ·Äõ·Äæ·Ä≠·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏/·Äô·Äõ·Äæ·Ä≠·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏ ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ Trend ·Åè ·ÄÅ·Ä≠·ÄØ·ÄÑ·Ä∫·Äô·Ä¨·Äô·Äæ·ÄØ ·ÄÄ·Ä≠·ÄØ ·Äê·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏·Äê·Ä¨·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏·Åã\n",
    "    # SMA Cross ·ÄÄ Trend Direction ·ÄÄ·Ä≠·ÄØ ·Äï·Ä±·Ä∏·Äû·Ä±·Ä¨·Ä∫·Äú·Ää·Ä∫·Ä∏·Åä \n",
    "    # ADX ·ÄÄ Direction ·Äõ·Ä≤·Ä∑ Strength ·ÄÄ·Ä≠·ÄØ ·Äï·Ä±·Ä∏·Äû·Ää·Ä∫·Åã \n",
    "    # ADX ·Äî·Ä≠·Äô·Ä∑·Ä∫·Äï·Ä´·ÄÄ Range/Sideways ·Äñ·Äº·ÄÖ·Ä∫·Äï·Äº·ÄÆ·Ä∏·Åä \n",
    "    # ADX ·Äô·Äº·ÄÑ·Ä∑·Ä∫·Äï·Ä´·ÄÄ Strong Trend ·Äñ·Äº·ÄÖ·Ä∫·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏ Agent ·ÄÄ·Ä≠·ÄØ ·Äû·Ä≠·ÄÖ·Ä±·Äû·Ää·Ä∫·Åã\n",
    "    df['adx'] = TA.ADX(df, period=adx_period)  \n",
    "    \n",
    "    # RSI ·Äñ·Äº·ÄÑ·Ä∑·Ä∫ Overbought/Oversold ·ÄÄ·Ä≠·ÄØ ·ÄÖ·ÄÖ·Ä∫·ÄÜ·Ä±·Ä∏·Äï·Äº·ÄÆ·Ä∏·Åä \n",
    "    # Stochastic ·Äñ·Äº·ÄÑ·Ä∑·Ä∫ Momentum Change ·ÄÄ·Ä≠·ÄØ ·Ä°·Äê·Ää·Ä∫·Äï·Äº·ÄØ·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äû·Ää·Ä∫·Åã \n",
    "    # ·Äî·Äæ·ÄÖ·Ä∫·ÄÅ·ÄØ·ÄÖ·Äú·ÄØ·Ä∂·Ä∏ Extreme Level ·Äê·ÄΩ·ÄÑ·Ä∫ ·Äõ·Äæ·Ä≠·Äî·Ä±·Äï·Ä´·ÄÄ Reversal ·Äñ·Äº·ÄÖ·Ä∫·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·ÄÅ·Äº·Ä± ·Äï·Ä≠·ÄØ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏ Agent ·ÄÄ ·Äû·ÄÑ·Ä∫·Äö·Ä∞·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äû·Ää·Ä∫·Åã\n",
    "    df['stoch_k'] = TA.STOCH(df, period=stoch_period)   \n",
    "    # *** ·Äû·ÄÑ·Ä∫·Äê·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÜ·Ä≠·ÄØ·Äû·Ä±·Ä¨ Long-Term SMA Cross Feature ·Ä°·Äû·ÄÖ·Ä∫ ***\n",
    "\n",
    "    df['ma_fast'] = TA.SMA(df, period=sma_mid_period)\n",
    "    df['ma_slow'] = TA.SMA(df, period=sma_slow_period)\n",
    "    \n",
    "    # SMA 100 ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ SMA 200 ·Äê·Ä≠·ÄØ·Ä∑·Åè ·ÄÄ·ÄΩ·Ä¨·ÄÅ·Äº·Ä¨·Ä∏·ÄÅ·Äª·ÄÄ·Ä∫·ÄÄ·Ä≠·ÄØ ATR ·Äñ·Äº·ÄÑ·Ä∑·Ä∫ Normalization ·Äú·ÄØ·Äï·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "    df['ma_cross'] = ((df['ma_fast'] - df['ma_slow']) / df['atr_base']).ffill().round(6)\n",
    "    \n",
    "    # --- ·ÅÑ·Åã Data Cleaning ---\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).fillna(0) \n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].clip(lower=-1e5, upper=1e5).round(6)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "961b46b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_news_feature(df_price: pd.DataFrame, df_news: pd.DataFrame, ahead_bars: int = 6, behind_bars: int = 6) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    High-Impact News Event ·Äô·Äê·Ä≠·ÄØ·ÄÑ·Ä∫·Äô·ÄÆ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ ·Ä°·Äï·Äº·ÄÆ·Ä∏·Äê·ÄΩ·ÄÑ·Ä∫ Binary Feature (1) ·ÄÄ·Ä≠·ÄØ ·Äë·Ää·Ä∑·Ä∫·Äû·ÄΩ·ÄÑ·Ä∫·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏·Åã \n",
    "    searchsorted ·ÄÄ·Ä≠·ÄØ·Äû·ÄØ·Ä∂·Ä∏·Åç Timezone Conflict ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Äõ·Äæ·Ä±·Ä¨·ÄÑ·Ä∫·Äõ·Äæ·Ä¨·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏·Åã\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. News Feature Column ·ÄÄ·Ä≠·ÄØ 0 ·Äñ·Äº·ÄÑ·Ä∑·Ä∫ ·ÄÖ·Äê·ÄÑ·Ä∫·Äñ·Äº·Ää·Ä∑·Ä∫·Äû·ÄΩ·ÄÑ·Ä∫·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "    if 'is_high_impact_news' not in df_price.columns:\n",
    "        df_price['is_high_impact_news'] = 0\n",
    "\n",
    "    # 2. News 'Start' Column ·ÄÄ·Ä≠·ÄØ Datetime ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äú·Ä≤·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "    try:\n",
    "        df_news['Start_dt'] = pd.to_datetime(df_news['Start'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting news 'Start' column to datetime: {e}\")\n",
    "        return df_price\n",
    "\n",
    "    # 3. Timezone ·ÄÄ·Ä≠·ÄØ ·ÄÄ·Ä≠·ÄØ·ÄÑ·Ä∫·Äê·ÄΩ·Äö·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏ (Price Index Timezone ·Ä°·Äê·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏ ·Ää·Äæ·Ä≠·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏)\n",
    "    price_index = df_price.index\n",
    "    \n",
    "    # Price Index ·Äô·Äæ·Ä¨ Timezone ·Äõ·Äæ·Ä≠·Äô·Äõ·Äæ·Ä≠ ·ÄÖ·ÄÖ·Ä∫·ÄÜ·Ä±·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "    if price_index.tz is not None:\n",
    "        # News Time ·ÄÄ·Ä≠·ÄØ Price Index ·Äõ·Ä≤·Ä∑ Timezone ·Ä°·Äê·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏ ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äú·Ä≤·Äï·Ä±·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "        df_news['Start_dt'] = df_news['Start_dt'].dt.tz_localize('UTC').dt.tz_convert(price_index.tz) \n",
    "    else:\n",
    "        # Price Index ·Äô·Äæ·Ä¨ Timezone ·Äô·Äõ·Äæ·Ä≠·Äõ·ÄÑ·Ä∫ News Time ·ÄÄ·Äî·Ä± Timezone ·ÄÄ·Ä≠·ÄØ ·Äñ·Äö·Ä∫·Äë·ÄØ·Äê·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "        df_news['Start_dt'] = df_news['Start_dt'].dt.tz_localize(None)\n",
    "\n",
    "    # 4. Binary Search (searchsorted) ·Äñ·Äº·ÄÑ·Ä∑·Ä∫ Location ·Äõ·Äæ·Ä¨·Äï·Äº·ÄÆ·Ä∏ Feature ·Äë·Ää·Ä∑·Ä∫·Äû·ÄΩ·ÄÑ·Ä∫·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "    \n",
    "    for news_time in df_news['Start_dt']:\n",
    "        \n",
    "        # searchsorted ·Äû·Ää·Ä∫ Integer Index Location ·ÄÄ·Ä≠·ÄØ ·Äï·Äº·Äî·Ä∫·Äï·Ä±·Ä∏·Äû·Ä±·Ä¨·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∑·Ä∫ get_loc() ·Åè method argument error ·ÄÄ·Ä≠·ÄØ ·Äõ·Äæ·Ä±·Ä¨·ÄÑ·Ä∫·Äõ·Äæ·Ä¨·Ä∏·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äû·Ää·Ä∫\n",
    "        loc = price_index.searchsorted(news_time, side='left') \n",
    "        \n",
    "        # loc ·Äû·Ää·Ä∫ Index ·Äõ·Ä≤·Ä∑ ·Ä°·Äï·Äº·ÄÑ·Ä∫·Äò·ÄÄ·Ä∫·ÄÄ·Ä≠·ÄØ ·Äõ·Ä±·Ä¨·ÄÄ·Ä∫·Äû·ÄΩ·Ä¨·Ä∏·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äû·Ä±·Ä¨·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∑·Ä∫ ·Ä°·Äî·Ä¨·Ä∏·Äû·Äê·Ä∫ ·ÄÖ·ÄÖ·Ä∫·ÄÜ·Ä±·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "        if loc >= len(price_index):\n",
    "            continue\n",
    "\n",
    "        # News Event ·Äô·Äê·Ä≠·ÄØ·ÄÑ·Ä∫·Äô·ÄÆ bars\n",
    "        start_index = max(0, loc - ahead_bars)\n",
    "        \n",
    "        # News Event ·Ä°·Äï·Äº·ÄÆ·Ä∏ bars\n",
    "        end_index = min(len(price_index), loc + behind_bars)\n",
    "        \n",
    "        # 1 Value ·ÄÄ·Ä≠·ÄØ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äë·Ä¨·Ä∏·Äû·Ä±·Ä¨ Range ·Ä°·Äê·ÄΩ·ÄÑ·Ä∫·Ä∏ ·Äë·Ää·Ä∑·Ä∫·Äû·ÄΩ·ÄÑ·Ä∫·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏ (Integer Location ·Äñ·Äº·ÄÑ·Ä∑·Ä∫)\n",
    "        df_price.iloc[start_index:end_index, df_price.columns.get_loc('is_high_impact_news')] = 1\n",
    "\n",
    "    return df_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d6174e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conversion successful! 5-Min rows: 149715\n"
     ]
    }
   ],
   "source": [
    "file_path_from_data = \"./drive/MyDrive/data/raw/EURUSD_2020_all.csv\"\n",
    "file_path_to_data = \"./drive/MyDrive/data/raw/EURUSD_2020_all_5.csv\"\n",
    "\n",
    "df_all_5m_data = convert_1m_csv_to_5m_df(file_path_from_data)\n",
    "df_all_5m_data.to_csv(file_path_to_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b5d7670a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "shape of  column: 6\n",
      "Week 2020-01-05 00:00:00+00:00 has 576/1440 bars\n",
      "Week 2020-01-12 00:00:00+00:00 has 1439/1440 bars\n",
      "Week 2020-03-29 00:00:00+00:00 has 1433/1440 bars\n",
      "Week 2020-09-20 00:00:00+00:00 has 1439/1440 bars\n",
      "Week 2020-10-04 00:00:00+00:00 has 1436/1440 bars\n",
      "Week 2020-12-27 00:00:00+00:00 has 1152/1440 bars\n",
      "Week 2021-01-03 00:00:00+00:00 has 1142/1440 bars\n",
      "Week 2021-05-23 00:00:00+00:00 has 1439/1440 bars\n",
      "Week 2021-05-30 00:00:00+00:00 has 1437/1440 bars\n",
      "Week 2021-06-06 00:00:00+00:00 has 1439/1440 bars\n",
      "Week 2021-09-19 00:00:00+00:00 has 1439/1440 bars\n",
      "Week 2021-10-10 00:00:00+00:00 has 1439/1440 bars\n",
      "Week 2021-12-12 00:00:00+00:00 has 1439/1440 bars\n",
      "Week 2021-12-26 00:00:00+00:00 has 1438/1440 bars\n",
      "Week 2022-01-02 00:00:00+00:00 has 1428/1440 bars\n"
     ]
    }
   ],
   "source": [
    "cf = EnvConfig('./drive/MyDrive/configure.json')\n",
    "\n",
    "raw = pd.read_csv(file_path_to_data)\n",
    "\n",
    "df = patch_missing_data(raw,cf=cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8a891862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broker Data (00:00 ·Äô·Äæ ·ÄÖ·Äû·Ä±·Ä¨) ·ÄÄ·Ä≠·ÄØ ·ÄÅ·Ä±·Ä´·Ä∫·ÄÜ·Ä≠·ÄØ·Äû·Ää·Ä∑·Ä∫·Ä°·ÄÅ·Ä´\n",
    "# GMT+2/GMT+3 ·ÄÄ·Ä≠·ÄØ ·Ä°·Äú·Ä≠·ÄØ·Ä°·Äú·Äª·Ä±·Ä¨·ÄÄ·Ä∫ ·ÄÄ·Ä≠·ÄØ·ÄÑ·Ä∫·Äê·ÄΩ·Äö·Ä∫·Äï·Ä±·Ä∏·Äõ·Äî·Ä∫\n",
    "axiory_tz = 'Europe/Kiev'  \n",
    "\n",
    "dft = add_time_feature(df, cf=cf, source_tz=axiory_tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a3b52a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tech_indicators(dft, cf=cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d9cab379",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_from_news = \"./drive/MyDrive/data/raw/2020_2021_News.csv\"\n",
    "news = pd.read_csv(file_path_from_news)\n",
    "df_news = add_news_feature(df, news)\n",
    "df_news.to_csv('final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65b023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cf = EnvConfig('./drive/MyDrive/configure.json')\n",
    "\n",
    "raw = pd.read_csv(file_path_to_data)\n",
    "\n",
    "df = patch_missing_data(raw,cf=cf)\n",
    "\n",
    "symbol = 'EURUSD'\n",
    "# Broker Data (00:00 ·Äô·Äæ ·ÄÖ·Äû·Ä±·Ä¨) ·ÄÄ·Ä≠·ÄØ ·ÄÅ·Ä±·Ä´·Ä∫·ÄÜ·Ä≠·ÄØ·Äû·Ää·Ä∑·Ä∫·Ä°·ÄÅ·Ä´\n",
    "# GMT+2/GMT+3 ·ÄÄ·Ä≠·ÄØ ·Ä°·Äú·Ä≠·ÄØ·Ä°·Äú·Äª·Ä±·Ä¨·ÄÄ·Ä∫ ·ÄÄ·Ä≠·ÄØ·ÄÑ·Ä∫·Äê·ÄΩ·Äö·Ä∫·Äï·Ä±·Ä∏·Äõ·Äî·Ä∫\n",
    "axiory_tz = 'Europe/Kiev'  \n",
    "\n",
    "dft = add_time_feature(df, cf=cf, source_tz=axiory_tz)\n",
    "dft.head(3)\n",
    "\n",
    "\n",
    "\n",
    "cf = EnvConfig('./drive/MyDrive/configure.json')\n",
    "\n",
    "df = tech_indicators(dft, cf=cf)\n",
    "df.to_csv('tech.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bff762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Start</th>\n",
       "      <th>Name</th>\n",
       "      <th>Impact</th>\n",
       "      <th>Currency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>be36572c-0dbf-4b11-aca1-bf854160ba4a</td>\n",
       "      <td>01/03/2020 13:00:00</td>\n",
       "      <td>Harmonized Index of Consumer Prices (YoY)</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>EUR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5b125588-01ca-4aca-a365-8902ef7839f1</td>\n",
       "      <td>01/03/2020 15:00:00</td>\n",
       "      <td>ISM Manufacturing PMI</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3fa72395-467b-48b9-b0a3-29529d398237</td>\n",
       "      <td>01/03/2020 19:00:00</td>\n",
       "      <td>FOMC Minutes</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2d6440df-69b8-4322-b175-002ae17088e3</td>\n",
       "      <td>01/07/2020 10:00:00</td>\n",
       "      <td>Core Harmonized Index of Consumer Prices (YoY)</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>EUR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id                Start  \\\n",
       "0  be36572c-0dbf-4b11-aca1-bf854160ba4a  01/03/2020 13:00:00   \n",
       "1  5b125588-01ca-4aca-a365-8902ef7839f1  01/03/2020 15:00:00   \n",
       "2  3fa72395-467b-48b9-b0a3-29529d398237  01/03/2020 19:00:00   \n",
       "3  2d6440df-69b8-4322-b175-002ae17088e3  01/07/2020 10:00:00   \n",
       "\n",
       "                                             Name Impact Currency  \n",
       "0       Harmonized Index of Consumer Prices (YoY)   HIGH      EUR  \n",
       "1                           ISM Manufacturing PMI   HIGH      USD  \n",
       "2                                    FOMC Minutes   HIGH      USD  \n",
       "3  Core Harmonized Index of Consumer Prices (YoY)   HIGH      EUR  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "11b6c290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>vol</th>\n",
       "      <th>weekday</th>\n",
       "      <th>day</th>\n",
       "      <th>week</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>atr_base</th>\n",
       "      <th>log_returns</th>\n",
       "      <th>price_norm</th>\n",
       "      <th>spread_ratio</th>\n",
       "      <th>body_ratio</th>\n",
       "      <th>rsi</th>\n",
       "      <th>macd_hist</th>\n",
       "      <th>adx</th>\n",
       "      <th>stoch_k</th>\n",
       "      <th>ma_cross</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01 22:00:00+00:00</th>\n",
       "      <td>1.12117</td>\n",
       "      <td>1.12128</td>\n",
       "      <td>1.12087</td>\n",
       "      <td>1.12114</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.073171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              open     high      low    close   vol  weekday  \\\n",
       "2020-01-01 22:00:00+00:00  1.12117  1.12128  1.12087  1.12114  31.0        2   \n",
       "\n",
       "                           day  week  month  year  ...  atr_base  log_returns  \\\n",
       "2020-01-01 22:00:00+00:00    1     1      1  2020  ...       0.0          0.0   \n",
       "\n",
       "                           price_norm  spread_ratio  body_ratio  rsi  \\\n",
       "2020-01-01 22:00:00+00:00         0.0           0.0   -0.073171  0.0   \n",
       "\n",
       "                           macd_hist  adx  stoch_k  ma_cross  \n",
       "2020-01-01 22:00:00+00:00        0.0  0.0      0.0       0.0  \n",
       "\n",
       "[1 rows x 26 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b5e0f4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = df.index.get_loc('01/03/2020 13:00:00')\n",
    "a # 2020-01-03 12:50:00+00:00,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b12c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e47141",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = add_news_feature(df, news)\n",
    "df_news.to_csv('df_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6d775ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.head(4)\n",
    "df_news.to_csv('df_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c98e7cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dbfe9603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "01/03/2020 13:00:00\n",
      "<class 'str'>\n",
      "01/03/2020 15:00:00\n",
      "<class 'str'>\n",
      "01/03/2020 19:00:00\n",
      "<class 'str'>\n",
      "01/07/2020 10:00:00\n",
      "<class 'str'>\n",
      "01/07/2020 10:00:00\n",
      "<class 'str'>\n",
      "01/07/2020 15:00:00\n",
      "<class 'str'>\n",
      "01/10/2020 13:30:00\n",
      "<class 'str'>\n",
      "01/10/2020 13:30:00\n",
      "<class 'str'>\n",
      "01/14/2020 13:30:00\n",
      "<class 'str'>\n",
      "01/14/2020 13:30:00\n",
      "<class 'str'>\n",
      "01/16/2020 07:00:00\n",
      "<class 'str'>\n",
      "01/16/2020 12:30:00\n",
      "<class 'str'>\n",
      "01/16/2020 18:00:00\n",
      "<class 'str'>\n",
      "01/20/2020 18:30:00\n",
      "<class 'str'>\n",
      "01/21/2020 09:00:00\n",
      "<class 'str'>\n",
      "01/21/2020 10:00:00\n",
      "<class 'str'>\n",
      "01/23/2020 12:45:00\n",
      "<class 'str'>\n",
      "01/23/2020 12:45:00\n",
      "<class 'str'>\n",
      "01/23/2020 13:30:00\n",
      "<class 'str'>\n",
      "01/24/2020 08:30:00\n",
      "<class 'str'>\n",
      "01/24/2020 08:30:00\n",
      "<class 'str'>\n",
      "01/24/2020 09:00:00\n",
      "<class 'str'>\n",
      "01/24/2020 10:30:00\n",
      "<class 'str'>\n",
      "01/28/2020 13:30:00\n",
      "<class 'str'>\n",
      "01/29/2020 19:00:00\n",
      "<class 'str'>\n",
      "01/29/2020 19:30:00\n",
      "<class 'str'>\n",
      "01/30/2020 13:00:00\n",
      "<class 'str'>\n",
      "01/30/2020 13:30:00\n",
      "<class 'str'>\n",
      "01/31/2020 10:00:00\n",
      "<class 'str'>\n",
      "01/31/2020 10:00:00\n",
      "<class 'str'>\n",
      "01/31/2020 10:00:00\n",
      "<class 'str'>\n",
      "01/31/2020 10:00:00\n",
      "<class 'str'>\n",
      "02/03/2020 15:00:00\n",
      "<class 'str'>\n",
      "02/05/2020 12:15:00\n",
      "<class 'str'>\n",
      "02/05/2020 15:00:00\n",
      "<class 'str'>\n",
      "02/06/2020 08:00:00\n",
      "<class 'str'>\n",
      "02/07/2020 13:30:00\n",
      "<class 'str'>\n",
      "02/07/2020 13:30:00\n",
      "<class 'str'>\n",
      "02/11/2020 14:00:00\n",
      "<class 'str'>\n",
      "02/11/2020 15:00:00\n",
      "<class 'str'>\n",
      "02/12/2020 14:30:00\n",
      "<class 'str'>\n",
      "02/13/2020 07:00:00\n",
      "<class 'str'>\n",
      "02/13/2020 13:30:00\n",
      "<class 'str'>\n",
      "02/13/2020 13:30:00\n",
      "<class 'str'>\n",
      "02/14/2020 07:00:00\n",
      "<class 'str'>\n",
      "02/14/2020 10:00:00\n",
      "<class 'str'>\n",
      "02/14/2020 10:00:00\n",
      "<class 'str'>\n",
      "02/18/2020 10:00:00\n",
      "<class 'str'>\n",
      "02/19/2020 19:00:00\n",
      "<class 'str'>\n",
      "02/20/2020 12:30:00\n",
      "<class 'str'>\n",
      "02/21/2020 08:30:00\n",
      "<class 'str'>\n",
      "02/21/2020 08:30:00\n",
      "<class 'str'>\n",
      "02/21/2020 09:00:00\n",
      "<class 'str'>\n",
      "02/26/2020 13:30:00\n",
      "<class 'str'>\n",
      "02/27/2020 09:45:00\n",
      "<class 'str'>\n",
      "02/27/2020 13:30:00\n",
      "<class 'str'>\n",
      "02/27/2020 13:30:00\n",
      "<class 'str'>\n",
      "02/28/2020 13:00:00\n",
      "<class 'str'>\n",
      "03/02/2020 15:00:00\n",
      "<class 'str'>\n",
      "03/03/2020 10:00:00\n",
      "<class 'str'>\n",
      "03/03/2020 10:00:00\n",
      "<class 'str'>\n",
      "03/03/2020 15:00:00\n",
      "<class 'str'>\n",
      "03/03/2020 16:00:00\n",
      "<class 'str'>\n",
      "03/04/2020 15:00:00\n",
      "<class 'str'>\n",
      "03/06/2020 13:30:00\n",
      "<class 'str'>\n",
      "03/06/2020 13:30:00\n",
      "<class 'str'>\n",
      "03/10/2020 10:00:00\n",
      "<class 'str'>\n",
      "03/10/2020 10:00:00\n",
      "<class 'str'>\n",
      "03/11/2020 12:30:00\n",
      "<class 'str'>\n",
      "03/11/2020 12:30:00\n",
      "<class 'str'>\n",
      "03/12/2020 12:45:00\n",
      "<class 'str'>\n",
      "03/12/2020 12:45:00\n",
      "<class 'str'>\n",
      "03/12/2020 13:30:00\n",
      "<class 'str'>\n",
      "03/13/2020 07:00:00\n",
      "<class 'str'>\n",
      "03/15/2020 21:00:00\n",
      "<class 'str'>\n",
      "03/15/2020 21:00:00\n",
      "<class 'str'>\n",
      "03/15/2020 22:30:00\n",
      "<class 'str'>\n",
      "03/17/2020 10:00:00\n",
      "<class 'str'>\n",
      "03/19/2020 12:30:00\n",
      "<class 'str'>\n",
      "03/24/2020 08:30:00\n",
      "<class 'str'>\n",
      "03/24/2020 08:30:00\n",
      "<class 'str'>\n",
      "03/24/2020 09:00:00\n",
      "<class 'str'>\n",
      "03/24/2020 13:45:00\n",
      "<class 'str'>\n",
      "03/24/2020 13:45:00\n",
      "<class 'str'>\n",
      "03/24/2020 13:45:00\n",
      "<class 'str'>\n",
      "03/25/2020 12:30:00\n",
      "<class 'str'>\n",
      "03/26/2020 11:05:00\n",
      "<class 'str'>\n",
      "03/26/2020 12:30:00\n",
      "<class 'str'>\n",
      "03/26/2020 12:30:00\n",
      "<class 'str'>\n",
      "03/30/2020 12:00:00\n",
      "<class 'str'>\n",
      "03/31/2020 07:55:00\n",
      "<class 'str'>\n",
      "03/31/2020 07:55:00\n",
      "<class 'str'>\n",
      "03/31/2020 09:00:00\n",
      "<class 'str'>\n",
      "03/31/2020 09:00:00\n",
      "<class 'str'>\n",
      "04/01/2020 12:15:00\n",
      "<class 'str'>\n",
      "04/01/2020 14:00:00\n",
      "<class 'str'>\n",
      "04/02/2020 12:30:00\n",
      "<class 'str'>\n",
      "04/03/2020 12:30:00\n",
      "<class 'str'>\n",
      "04/03/2020 12:30:00\n",
      "<class 'str'>\n",
      "04/03/2020 14:00:00\n",
      "<class 'str'>\n",
      "04/03/2020 14:00:00\n",
      "<class 'str'>\n",
      "04/08/2020 18:00:00\n",
      "<class 'str'>\n",
      "04/09/2020 11:30:00\n",
      "<class 'str'>\n",
      "04/09/2020 12:30:00\n",
      "<class 'str'>\n",
      "04/09/2020 14:00:00\n",
      "<class 'str'>\n",
      "04/10/2020 12:30:00\n",
      "<class 'str'>\n",
      "04/10/2020 12:30:00\n",
      "<class 'str'>\n",
      "04/16/2020 06:00:00\n",
      "<class 'str'>\n",
      "04/16/2020 12:30:00\n",
      "<class 'str'>\n",
      "04/21/2020 09:00:00\n",
      "<class 'str'>\n",
      "04/23/2020 07:30:00\n",
      "<class 'str'>\n",
      "04/23/2020 07:30:00\n",
      "<class 'str'>\n",
      "04/23/2020 08:00:00\n",
      "<class 'str'>\n",
      "04/23/2020 12:30:00\n",
      "<class 'str'>\n",
      "04/24/2020 12:30:00\n",
      "<class 'str'>\n",
      "04/28/2020 08:00:00\n",
      "<class 'str'>\n",
      "04/29/2020 12:00:00\n",
      "<class 'str'>\n",
      "04/29/2020 12:30:00\n",
      "<class 'str'>\n",
      "04/29/2020 18:00:00\n",
      "<class 'str'>\n",
      "04/29/2020 18:30:00\n",
      "<class 'str'>\n",
      "04/30/2020 07:55:00\n",
      "<class 'str'>\n",
      "04/30/2020 07:55:00\n",
      "<class 'str'>\n",
      "04/30/2020 09:00:00\n",
      "<class 'str'>\n",
      "04/30/2020 09:00:00\n",
      "<class 'str'>\n",
      "04/30/2020 09:00:00\n",
      "<class 'str'>\n",
      "04/30/2020 09:00:00\n",
      "<class 'str'>\n",
      "04/30/2020 09:00:00\n",
      "<class 'str'>\n",
      "04/30/2020 11:45:00\n",
      "<class 'str'>\n",
      "04/30/2020 11:45:00\n",
      "<class 'str'>\n",
      "04/30/2020 12:30:00\n",
      "<class 'str'>\n",
      "04/30/2020 12:30:00\n",
      "<class 'str'>\n",
      "05/01/2020 14:00:00\n",
      "<class 'str'>\n",
      "05/05/2020 14:00:00\n",
      "<class 'str'>\n",
      "05/05/2020 14:00:00\n",
      "<class 'str'>\n",
      "05/06/2020 12:15:00\n",
      "<class 'str'>\n",
      "05/07/2020 12:30:00\n",
      "<class 'str'>\n",
      "05/08/2020 11:00:00\n",
      "<class 'str'>\n",
      "05/08/2020 12:30:00\n",
      "<class 'str'>\n",
      "05/08/2020 12:30:00\n",
      "<class 'str'>\n",
      "05/12/2020 12:30:00\n",
      "<class 'str'>\n",
      "05/12/2020 12:30:00\n",
      "<class 'str'>\n",
      "05/13/2020 13:00:00\n",
      "<class 'str'>\n",
      "05/14/2020 06:00:00\n",
      "<class 'str'>\n",
      "05/14/2020 12:30:00\n",
      "<class 'str'>\n",
      "05/15/2020 08:00:11\n",
      "<class 'str'>\n",
      "05/15/2020 09:00:00\n",
      "<class 'str'>\n",
      "05/15/2020 09:00:00\n",
      "<class 'str'>\n",
      "05/19/2020 09:00:00\n",
      "<class 'str'>\n",
      "05/19/2020 14:00:00\n",
      "<class 'str'>\n",
      "05/20/2020 18:00:00\n",
      "<class 'str'>\n",
      "05/21/2020 07:30:00\n",
      "<class 'str'>\n",
      "05/21/2020 07:30:00\n",
      "<class 'str'>\n",
      "05/21/2020 08:00:00\n",
      "<class 'str'>\n",
      "05/21/2020 12:30:00\n",
      "<class 'str'>\n",
      "05/21/2020 18:30:00\n",
      "<class 'str'>\n",
      "05/22/2020 11:30:00\n",
      "<class 'str'>\n",
      "05/27/2020 07:30:00\n",
      "<class 'str'>\n",
      "05/28/2020 12:00:00\n",
      "<class 'str'>\n",
      "05/28/2020 12:30:00\n",
      "<class 'str'>\n",
      "05/28/2020 12:30:00\n",
      "<class 'str'>\n",
      "05/28/2020 12:30:00\n",
      "<class 'str'>\n",
      "05/29/2020 09:00:00\n",
      "<class 'str'>\n",
      "05/29/2020 09:00:00\n",
      "<class 'str'>\n",
      "05/29/2020 15:00:00\n",
      "<class 'str'>\n",
      "06/01/2020 14:00:00\n",
      "<class 'str'>\n",
      "06/03/2020 07:55:00\n",
      "<class 'str'>\n",
      "06/03/2020 07:55:00\n",
      "<class 'str'>\n",
      "06/03/2020 09:00:00\n",
      "<class 'str'>\n",
      "06/03/2020 12:15:00\n",
      "<class 'str'>\n",
      "06/03/2020 14:00:00\n",
      "<class 'str'>\n",
      "06/03/2020 14:00:00\n",
      "<class 'str'>\n",
      "06/04/2020 11:45:00\n",
      "<class 'str'>\n",
      "06/04/2020 11:45:00\n",
      "<class 'str'>\n",
      "06/04/2020 12:30:00\n",
      "<class 'str'>\n",
      "06/04/2020 12:30:00\n",
      "<class 'str'>\n",
      "06/05/2020 12:30:00\n",
      "<class 'str'>\n",
      "06/05/2020 12:30:00\n",
      "<class 'str'>\n",
      "06/08/2020 13:45:00\n",
      "<class 'str'>\n",
      "06/08/2020 15:45:00\n",
      "<class 'str'>\n",
      "06/09/2020 09:00:00\n",
      "<class 'str'>\n",
      "06/09/2020 09:00:00\n",
      "<class 'str'>\n",
      "06/10/2020 12:30:00\n",
      "<class 'str'>\n",
      "06/10/2020 12:30:00\n",
      "<class 'str'>\n",
      "06/10/2020 18:00:00\n",
      "<class 'str'>\n",
      "06/10/2020 18:00:00\n",
      "<class 'str'>\n",
      "06/10/2020 18:00:00\n",
      "<class 'str'>\n",
      "06/10/2020 18:00:00\n",
      "<class 'str'>\n",
      "06/10/2020 18:00:00\n",
      "<class 'str'>\n",
      "06/10/2020 18:00:00\n",
      "<class 'str'>\n",
      "06/10/2020 18:00:00\n",
      "<class 'str'>\n",
      "06/10/2020 18:30:00\n",
      "<class 'str'>\n",
      "06/11/2020 12:30:00\n",
      "<class 'str'>\n",
      "06/16/2020 06:00:00\n",
      "<class 'str'>\n",
      "06/16/2020 09:00:00\n",
      "<class 'str'>\n",
      "06/16/2020 14:00:00\n",
      "<class 'str'>\n",
      "06/17/2020 16:00:00\n",
      "<class 'str'>\n",
      "06/18/2020 12:30:00\n",
      "<class 'str'>\n",
      "06/19/2020 17:00:00\n",
      "<class 'str'>\n",
      "06/23/2020 07:30:00\n",
      "<class 'str'>\n",
      "06/23/2020 07:30:00\n",
      "<class 'str'>\n",
      "06/23/2020 08:00:00\n",
      "<class 'str'>\n",
      "06/25/2020 11:30:00\n",
      "<class 'str'>\n",
      "06/25/2020 12:30:00\n",
      "<class 'str'>\n",
      "06/25/2020 12:30:00\n",
      "<class 'str'>\n",
      "06/25/2020 12:30:00\n",
      "<class 'str'>\n",
      "06/25/2020 20:30:00\n",
      "<class 'str'>\n",
      "06/26/2020 07:00:00\n",
      "<class 'str'>\n",
      "06/29/2020 12:00:00\n",
      "<class 'str'>\n",
      "06/30/2020 09:00:00\n",
      "<class 'str'>\n",
      "06/30/2020 09:00:00\n",
      "<class 'str'>\n",
      "06/30/2020 16:30:00\n",
      "<class 'str'>\n",
      "07/01/2020 07:55:00\n",
      "<class 'str'>\n",
      "07/01/2020 07:55:00\n",
      "<class 'str'>\n",
      "07/01/2020 12:15:00\n",
      "<class 'str'>\n",
      "07/01/2020 14:00:00\n",
      "<class 'str'>\n",
      "07/01/2020 18:00:00\n",
      "<class 'str'>\n",
      "07/02/2020 12:30:00\n",
      "<class 'str'>\n",
      "07/02/2020 12:30:00\n",
      "<class 'str'>\n",
      "07/04/2020 10:40:00\n",
      "<class 'str'>\n",
      "07/06/2020 14:00:00\n",
      "<class 'str'>\n",
      "07/14/2020 06:00:00\n",
      "<class 'str'>\n",
      "07/14/2020 08:00:00\n",
      "<class 'str'>\n",
      "07/14/2020 09:00:00\n",
      "<class 'str'>\n",
      "07/14/2020 12:30:00\n",
      "<class 'str'>\n",
      "07/14/2020 12:30:00\n",
      "<class 'str'>\n",
      "07/16/2020 11:45:00\n",
      "<class 'str'>\n",
      "07/16/2020 11:45:00\n",
      "<class 'str'>\n",
      "07/16/2020 12:30:00\n",
      "<class 'str'>\n",
      "07/22/2020 13:15:00\n",
      "<class 'str'>\n",
      "07/24/2020 07:30:00\n",
      "<class 'str'>\n",
      "07/24/2020 07:30:00\n",
      "<class 'str'>\n",
      "07/24/2020 08:00:00\n",
      "<class 'str'>\n",
      "07/27/2020 12:30:00\n",
      "<class 'str'>\n",
      "07/27/2020 12:30:00\n",
      "<class 'str'>\n",
      "07/29/2020 18:00:00\n",
      "<class 'str'>\n",
      "07/29/2020 18:00:00\n",
      "<class 'str'>\n",
      "07/29/2020 18:30:00\n",
      "<class 'str'>\n",
      "07/30/2020 08:00:00\n",
      "<class 'str'>\n",
      "07/30/2020 12:00:00\n",
      "<class 'str'>\n",
      "07/30/2020 12:30:00\n",
      "<class 'str'>\n",
      "07/31/2020 09:00:00\n",
      "<class 'str'>\n",
      "07/31/2020 09:00:00\n",
      "<class 'str'>\n",
      "07/31/2020 09:00:00\n",
      "<class 'str'>\n",
      "07/31/2020 09:00:00\n",
      "<class 'str'>\n",
      "08/03/2020 14:00:00\n",
      "<class 'str'>\n",
      "08/05/2020 12:15:00\n",
      "<class 'str'>\n",
      "08/05/2020 14:00:00\n",
      "<class 'str'>\n",
      "08/07/2020 12:30:00\n",
      "<class 'str'>\n",
      "08/12/2020 12:30:00\n",
      "<class 'str'>\n",
      "08/12/2020 12:30:00\n",
      "<class 'str'>\n",
      "08/13/2020 06:00:00\n",
      "<class 'str'>\n",
      "08/14/2020 09:00:00\n",
      "<class 'str'>\n",
      "08/14/2020 09:00:00\n",
      "<class 'str'>\n",
      "08/19/2020 18:00:00\n",
      "<class 'str'>\n",
      "08/21/2020 07:30:00\n",
      "<class 'str'>\n",
      "08/21/2020 07:30:00\n",
      "<class 'str'>\n",
      "08/21/2020 08:00:00\n",
      "<class 'str'>\n",
      "08/26/2020 12:30:00\n",
      "<class 'str'>\n",
      "08/26/2020 12:30:00\n",
      "<class 'str'>\n",
      "08/27/2020 00:00:00\n",
      "<class 'str'>\n",
      "08/27/2020 12:30:00\n",
      "<class 'str'>\n",
      "08/27/2020 13:10:00\n",
      "<class 'str'>\n",
      "08/28/2020 00:00:00\n",
      "<class 'str'>\n",
      "08/31/2020 12:00:00\n",
      "<class 'str'>\n",
      "09/01/2020 09:00:00\n",
      "<class 'str'>\n",
      "09/01/2020 09:00:00\n",
      "<class 'str'>\n",
      "09/01/2020 14:00:00\n",
      "<class 'str'>\n",
      "09/02/2020 12:15:00\n",
      "<class 'str'>\n",
      "09/03/2020 14:00:00\n",
      "<class 'str'>\n",
      "09/04/2020 12:30:00\n",
      "<class 'str'>\n",
      "09/08/2020 09:00:00\n",
      "<class 'str'>\n",
      "09/08/2020 09:00:00\n",
      "<class 'str'>\n",
      "09/10/2020 11:45:00\n",
      "<class 'str'>\n",
      "09/10/2020 11:45:00\n",
      "<class 'str'>\n",
      "09/10/2020 12:30:00\n",
      "<class 'str'>\n",
      "09/10/2020 17:00:00\n",
      "<class 'str'>\n",
      "09/11/2020 06:00:00\n",
      "<class 'str'>\n",
      "09/11/2020 12:30:00\n",
      "<class 'str'>\n",
      "09/11/2020 12:30:00\n",
      "<class 'str'>\n",
      "09/16/2020 18:00:00\n",
      "<class 'str'>\n",
      "09/16/2020 18:00:00\n",
      "<class 'str'>\n",
      "09/16/2020 18:00:00\n",
      "<class 'str'>\n",
      "09/16/2020 18:00:00\n",
      "<class 'str'>\n",
      "09/16/2020 18:00:00\n",
      "<class 'str'>\n",
      "09/16/2020 18:00:00\n",
      "<class 'str'>\n",
      "09/16/2020 18:00:00\n",
      "<class 'str'>\n",
      "09/16/2020 18:00:00\n",
      "<class 'str'>\n",
      "09/16/2020 18:30:00\n",
      "<class 'str'>\n",
      "09/21/2020 12:25:00\n",
      "<class 'str'>\n",
      "09/21/2020 14:00:00\n",
      "<class 'str'>\n",
      "09/22/2020 14:30:00\n",
      "<class 'str'>\n",
      "09/23/2020 07:30:00\n",
      "<class 'str'>\n",
      "09/23/2020 07:30:00\n",
      "<class 'str'>\n",
      "09/23/2020 08:00:00\n",
      "<class 'str'>\n",
      "09/23/2020 14:00:00\n",
      "<class 'str'>\n",
      "09/24/2020 14:00:00\n",
      "<class 'str'>\n",
      "09/25/2020 12:30:00\n",
      "<class 'str'>\n",
      "09/25/2020 12:30:00\n",
      "<class 'str'>\n",
      "09/28/2020 13:45:00\n",
      "<class 'str'>\n",
      "09/29/2020 12:00:00\n",
      "<class 'str'>\n",
      "09/30/2020 07:20:00\n",
      "<class 'str'>\n",
      "09/30/2020 12:15:00\n",
      "<class 'str'>\n",
      "09/30/2020 12:30:00\n",
      "<class 'str'>\n",
      "10/01/2020 14:00:00\n",
      "<class 'str'>\n",
      "10/02/2020 09:00:00\n",
      "<class 'str'>\n",
      "10/02/2020 09:00:00\n",
      "<class 'str'>\n",
      "10/02/2020 12:30:00\n",
      "<class 'str'>\n",
      "10/05/2020 14:00:00\n",
      "<class 'str'>\n",
      "10/06/2020 13:00:00\n",
      "<class 'str'>\n",
      "10/06/2020 14:40:00\n",
      "<class 'str'>\n",
      "10/07/2020 12:10:00\n",
      "<class 'str'>\n",
      "10/07/2020 18:00:00\n",
      "<class 'str'>\n",
      "10/12/2020 11:00:00\n",
      "<class 'str'>\n",
      "10/13/2020 06:00:00\n",
      "<class 'str'>\n",
      "10/13/2020 12:30:00\n",
      "<class 'str'>\n",
      "10/13/2020 12:30:00\n",
      "<class 'str'>\n",
      "10/14/2020 08:00:00\n",
      "<class 'str'>\n",
      "10/15/2020 16:00:00\n",
      "<class 'str'>\n",
      "10/18/2020 13:05:00\n",
      "<class 'str'>\n",
      "10/19/2020 12:00:00\n",
      "<class 'str'>\n",
      "10/19/2020 12:40:00\n",
      "<class 'str'>\n",
      "10/19/2020 12:45:00\n",
      "<class 'str'>\n",
      "10/21/2020 07:30:00\n",
      "<class 'str'>\n",
      "10/23/2020 07:30:00\n",
      "<class 'str'>\n",
      "10/23/2020 07:30:00\n",
      "<class 'str'>\n",
      "10/23/2020 08:00:00\n",
      "<class 'str'>\n",
      "10/27/2020 09:00:00\n",
      "<class 'str'>\n",
      "10/27/2020 12:30:00\n",
      "<class 'str'>\n",
      "10/27/2020 12:30:00\n",
      "<class 'str'>\n",
      "10/29/2020 12:30:00\n",
      "<class 'str'>\n",
      "10/29/2020 12:45:00\n",
      "<class 'str'>\n",
      "10/29/2020 12:45:00\n",
      "<class 'str'>\n",
      "10/29/2020 13:00:00\n",
      "<class 'str'>\n",
      "10/29/2020 13:30:00\n",
      "<class 'str'>\n",
      "10/30/2020 09:00:00\n",
      "<class 'str'>\n",
      "10/30/2020 10:00:00\n",
      "<class 'str'>\n",
      "10/30/2020 10:00:00\n",
      "<class 'str'>\n",
      "10/30/2020 10:00:00\n",
      "<class 'str'>\n",
      "10/30/2020 10:00:00\n",
      "<class 'str'>\n",
      "11/02/2020 15:00:00\n",
      "<class 'str'>\n",
      "11/04/2020 13:15:00\n",
      "<class 'str'>\n",
      "11/04/2020 15:00:00\n",
      "<class 'str'>\n",
      "11/05/2020 19:00:00\n",
      "<class 'str'>\n",
      "11/05/2020 19:00:00\n",
      "<class 'str'>\n",
      "11/05/2020 19:30:00\n",
      "<class 'str'>\n",
      "11/06/2020 13:30:00\n",
      "<class 'str'>\n",
      "11/09/2020 09:25:00\n",
      "<class 'str'>\n",
      "11/11/2020 13:00:00\n",
      "<class 'str'>\n",
      "11/12/2020 07:00:00\n",
      "<class 'str'>\n",
      "11/12/2020 13:30:00\n",
      "<class 'str'>\n",
      "11/12/2020 13:30:00\n",
      "<class 'str'>\n",
      "11/12/2020 16:45:00\n",
      "<class 'str'>\n",
      "11/12/2020 16:45:00\n",
      "<class 'str'>\n",
      "11/13/2020 10:00:00\n",
      "<class 'str'>\n",
      "11/13/2020 10:00:00\n",
      "<class 'str'>\n",
      "11/16/2020 13:00:00\n",
      "<class 'str'>\n",
      "11/17/2020 16:00:00\n",
      "<class 'str'>\n",
      "11/17/2020 18:00:00\n",
      "<class 'str'>\n",
      "11/19/2020 08:00:00\n",
      "<class 'str'>\n",
      "11/19/2020 15:15:00\n",
      "<class 'str'>\n",
      "11/20/2020 08:35:00\n",
      "<class 'str'>\n",
      "11/23/2020 08:30:00\n",
      "<class 'str'>\n",
      "11/23/2020 08:30:00\n",
      "<class 'str'>\n",
      "11/23/2020 09:00:00\n",
      "<class 'str'>\n",
      "11/24/2020 14:00:00\n",
      "<class 'str'>\n",
      "11/25/2020 13:30:00\n",
      "<class 'str'>\n",
      "11/25/2020 13:30:00\n",
      "<class 'str'>\n",
      "11/25/2020 13:30:00\n",
      "<class 'str'>\n",
      "11/25/2020 19:00:00\n",
      "<class 'str'>\n",
      "11/30/2020 10:30:00\n",
      "<class 'str'>\n",
      "11/30/2020 13:00:00\n",
      "<class 'str'>\n",
      "12/01/2020 10:00:00\n",
      "<class 'str'>\n",
      "12/01/2020 10:00:00\n",
      "<class 'str'>\n",
      "12/01/2020 15:00:00\n",
      "<class 'str'>\n",
      "12/01/2020 15:00:00\n",
      "<class 'str'>\n",
      "12/01/2020 17:00:00\n",
      "<class 'str'>\n",
      "12/02/2020 13:15:00\n",
      "<class 'str'>\n",
      "12/03/2020 15:00:00\n",
      "<class 'str'>\n",
      "12/04/2020 13:30:00\n",
      "<class 'str'>\n",
      "12/08/2020 10:00:00\n",
      "<class 'str'>\n",
      "12/08/2020 10:00:00\n",
      "<class 'str'>\n",
      "12/10/2020 12:45:00\n",
      "<class 'str'>\n",
      "12/10/2020 12:45:00\n",
      "<class 'str'>\n",
      "12/10/2020 13:30:00\n",
      "<class 'str'>\n",
      "12/10/2020 13:30:00\n",
      "<class 'str'>\n",
      "12/10/2020 13:30:00\n",
      "<class 'str'>\n",
      "12/11/2020 07:00:00\n",
      "<class 'str'>\n",
      "12/16/2020 08:30:00\n",
      "<class 'str'>\n",
      "12/16/2020 08:30:00\n",
      "<class 'str'>\n",
      "12/16/2020 09:00:00\n",
      "<class 'str'>\n",
      "12/16/2020 19:00:00\n",
      "<class 'str'>\n",
      "12/16/2020 19:00:00\n",
      "<class 'str'>\n",
      "12/16/2020 19:00:00\n",
      "<class 'str'>\n",
      "12/16/2020 19:00:00\n",
      "<class 'str'>\n",
      "12/16/2020 19:00:00\n",
      "<class 'str'>\n",
      "12/16/2020 19:00:00\n",
      "<class 'str'>\n",
      "12/16/2020 19:00:00\n",
      "<class 'str'>\n",
      "12/16/2020 19:00:00\n",
      "<class 'str'>\n",
      "12/16/2020 19:30:00\n",
      "<class 'str'>\n",
      "12/18/2020 21:30:00\n",
      "<class 'str'>\n",
      "12/22/2020 13:30:00\n",
      "<class 'str'>\n",
      "12/23/2020 13:30:00\n",
      "<class 'str'>\n",
      "12/23/2020 13:30:00\n"
     ]
    }
   ],
   "source": [
    "for news_time in news['Start']:\n",
    "    print(type(news_time))\n",
    "    print(news_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "05deb424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_to_file(**kwargs):\n",
    "    log_header                  =   kwargs.get(\"log_header\",False)\n",
    "    log_filename                =   kwargs.get(\"log_filename\",\"\")\n",
    "    printout                    =   kwargs.get(\"printout\",False)\n",
    "    balance                     =   kwargs.get(\"balance\")\n",
    "    balance_initial             =   kwargs.get(\"balance_initial\")\n",
    "    transaction_close_this_step =   kwargs.get(\"transaction_close_this_step\",[])\n",
    "    done_information            =   kwargs.get(\"done_information\",\"\")\n",
    "    profit                      =   balance - balance_initial\n",
    "\n",
    "    tr_lines                    =   \"\"\n",
    "    tr_lines_comma              =   \"\"\n",
    "    _header                     =   \"\"\n",
    "    _header_comma               =   \"\"\n",
    "    if log_header:\n",
    "        _header = f'{\"Ticket\":>8} {\"Type\":>4} {\"ActionStep\":16} \\\n",
    "                    {\"ActionPrice\":>12} {\"CloseStep\":8} {\"ClosePrice\":>12} \\\n",
    "                    {\"OpenBal\":>12} {\"CloseBal\":>12} {\"Status\":8} {\"Info\":>8} {\"PIPS\":>6} {\"SL\":>6} {\"PT\":>6} {\"DeltaStep\":8}\\n'\n",
    "\n",
    "\n",
    "        _header_comma = f'{\"Ticket,Type,ActionTime,ActionStep,ActionPrice,CloseTime,ClosePrice, OpenBal, CloseBal, Status, Info, PIPS,SL,PT,CloseStep,DeltaStep\"}\\n'\n",
    "    if transaction_close_this_step:\n",
    "        for _tr in transaction_close_this_step:\n",
    "            if _tr[\"CloseStep\"] >=0:\n",
    "                tr_lines += f'{_tr[\"Ticket\"]:>8} {_tr[\"Type\"]:>4} {_tr[\"ActionStep\"]:16} \\\n",
    "                    {_tr[\"ActionPrice\"]:.5f} {_tr[\"CloseStep\"]:8} {_tr[\"ClosePrice\"]:.5f} \\\n",
    "                    {_tr[\"OpenBal\"]:.2f} {_tr[\"CloseBal\"]:.2f} {_tr[\"Status\"]:8}  {_tr[\"Info\"]:>8}  {_tr[\"PIPS\"]:4.0f} {_tr[\"SL\"]:4.0f} {_tr[\"PT\"]:4.0f} {_tr[\"DeltaStep\"]:8}\\n'\n",
    "\n",
    "                tr_lines_comma += f'{_tr[\"Ticket\"]},{_tr[\"Type\"]},{_tr[\"ActionTime\"]},{_tr[\"ActionStep\"]}, \\\n",
    "                    {_tr[\"ActionPrice\"]},{_tr[\"CloseTime\"]},{_tr[\"ClosePrice\"]}, \\\n",
    "                    {_tr[\"OpenBal\"]},{_tr[\"CloseBal\"]}, {_tr[\"Status\"]},{_tr[\"Info\"]},{_tr[\"PIPS\"]},{_tr[\"SL\"]},{_tr[\"PT\"]},{_tr[\"CloseStep\"]},{_tr[\"DeltaStep\"]}\\n'\n",
    "\n",
    "    log = _header_comma + tr_lines_comma\n",
    "    # log = f\"Step: {current_step}   Balance: {balance}, Profit: {profit} \\\n",
    "    #     MDD: {max_draw_down_pct}\\n{tr_lines_comma}\\n\"\n",
    "    if done_information:\n",
    "        log += done_information\n",
    "    if log:\n",
    "        # os.makedirs(log_filename, exist_ok=True)\n",
    "        dir_path = os.path.dirname(log_filename)\n",
    "        if dir_path and not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "        with open(log_filename, 'a+') as _f:\n",
    "            _f.write(log)\n",
    "            _f.close()\n",
    "\n",
    "    tr_lines = _header + tr_lines\n",
    "    if printout and tr_lines:\n",
    "        print(tr_lines)\n",
    "        if done_information:\n",
    "            print(done_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c0b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import logging\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "# from stable_baselines3.common.callbacks import LearningRateSchedule\n",
    "from stable_baselines3 import PPO\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "class ForexTradingEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, file, cf, asset, logger_show=False, save_plot=False, scaler=None):\n",
    "        # scaler parameter ·ÄÄ·Ä≠·ÄØ ·Äë·Äï·Ä∫·Äë·Ää·Ä∑·Ä∫·Äõ·Äï·Ä´·Äô·Ää·Ä∫·Åã ·Åé·ÄÑ·Ä∫·Ä∏·Äû·Ää·Ä∫ Global Train Set ·Äê·ÄΩ·ÄÑ·Ä∫ Fit ·Äú·ÄØ·Äï·Ä∫·Äë·Ä¨·Ä∏·Äû·Ä±·Ä¨ Scaler ·Äñ·Äº·ÄÖ·Ä∫·Äõ·Äô·Ää·Ä∫·Åã\n",
    "        # 'scaler' ·Äû·Ää·Ä∫ Global Train Set ·Äê·ÄΩ·ÄÑ·Ä∫ Fit ·Äú·ÄØ·Äï·Ä∫·Äë·Ä¨·Ä∏·Äû·Ä±·Ä¨ StandardScaler instance ·Äñ·Äº·ÄÖ·Ä∫·Äõ·Äô·Ää·Ä∫·Åã\n",
    "        self.scaler = scaler\n",
    "        if self.scaler is None:\n",
    "             raise ValueError(\"A fitted StandardScaler instance must be provided to the Environment.\")  \n",
    "         \n",
    "        super(ForexTradingEnv, self).__init__()\n",
    "        # ·ÄÄ·Ä≠·Äî·Ä∫·Ä∏·Äõ·Äæ·ÄÑ·Ä∫·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·ÄÖ·Äê·ÄÑ·Ä∫·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äû·Ää·Ä∫·Åã\n",
    "        self._initialize_parameters(file, cf, asset, logger_show, save_plot)\n",
    "        \n",
    "        # [NEW ACTION] Raw Data ·ÄÄ·Ä≠·ÄØ Scaler ·Äñ·Äº·ÄÑ·Ä∑·Ä∫ Transform ·Äú·ÄØ·Äï·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "        # OHLCV features ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ Scaling ·Äú·ÄØ·Äï·Ä∫·Äõ·Äï·Ä´·Äô·Äö·Ä∫ (·Ä•·Äï·Äô·Ä¨: open, high, low, close, vol, atr_base, log_returns, price_norm, etc.)\n",
    "        # Scaling ·Äú·ÄØ·Äï·Ä∫·Äõ·Äî·Ä∫ features ·Äô·Äª·Ä¨·Ä∏·ÄÖ·Ä¨·Äõ·ÄÑ·Ä∫·Ä∏·ÄÄ·Ä≠·ÄØ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äï·Ä´·Åã\n",
    "        # ·Ä§·Äî·Ä±·Äõ·Ä¨·Äê·ÄΩ·ÄÑ·Ä∫ ·ÄÄ·Äª·ÄΩ·Äî·Ä∫·Äê·Ä±·Ä¨·Ä∫·Äê·Ä≠·ÄØ·Ä∑·Äû·Ää·Ä∫ OHLCV ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ Indicator Features ·Ä°·Ä¨·Ä∏·Äú·ÄØ·Ä∂·Ä∏·ÄÄ·Ä≠·ÄØ Scaling ·Äú·ÄØ·Äï·Ä∫·Äô·Ää·Ä∫·Äü·ÄØ ·Äö·Ä∞·ÄÜ·Äï·Ä´·Äô·Ää·Ä∫·Åã\n",
    "        self._scale_data()\n",
    "        \n",
    "        \n",
    "        # Action ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ Observation Spaces ·ÄÄ·Ä≠·ÄØ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äû·Ää·Ä∫·Åã\n",
    "        self._initialize_spaces()\n",
    "        # Environment ·ÄÄ·Ä≠·ÄØ ·Ä°·ÄÖ·Äï·Äº·ÄØ·Ä°·ÄÅ·Äº·Ä±·Ä°·Äî·Ä±·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Äî·Ä∫·Äú·Ää·Ä∫·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äû·Ää·Ä∫·Åã\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def _scale_data(self):\n",
    "        \"\"\"Raw Data (self.data) ·Äô·Äæ features ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ Global Scaler ·Äñ·Äº·ÄÑ·Ä∑·Ä∫ Transform ·Äú·ÄØ·Äï·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\"\"\"\n",
    "        # [self.features] ·Äê·ÄΩ·ÄÑ·Ä∫ OHLCV ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ Indicator Features ·Ä°·Ä¨·Ä∏·Äú·ÄØ·Ä∂·Ä∏ ·Äï·Ä´·Äù·ÄÑ·Ä∫·Äõ·Äô·Ää·Ä∫·Åã\n",
    "        # [NOTE]: 'time' ·ÄÄ·Ä≤·Ä∑·Äû·Ä≠·ÄØ·Ä∑·Äû·Ä±·Ä¨ Non-Numeric features ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ self.features ·Äê·ÄΩ·ÄÑ·Ä∫ ·Äô·Äï·Ä´·Äù·ÄÑ·Ä∫·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏ ·Äû·Ä±·ÄÅ·Äª·Ä¨·Äï·Ä´·ÄÖ·Ä±·Åã\n",
    "        if not self.scaler.scale_.any():\n",
    "             logger.warning(\"Scaler is not properly fitted. Continuing with raw data.\")\n",
    "             return\n",
    "             \n",
    "        # Scaled Features ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ Original DataFrame ·Äê·ÄΩ·ÄÑ·Ä∫ ·Ä°·ÄÖ·Ä¨·Ä∏·Äë·Ä≠·ÄØ·Ä∏·Äû·Ää·Ä∫·Åã\n",
    "        # Scaled Data ·Äê·ÄΩ·ÄÑ·Ä∫ NaN/Inf ·Äô·Äñ·Äº·ÄÖ·Ä∫·ÄÖ·Ä±·Äõ·Äî·Ä∫ Data Frame ·ÄÄ·Ä≠·ÄØ ·ÄÄ·Äº·Ä≠·ÄØ·Äê·ÄÑ·Ä∫·ÄÖ·ÄÖ·Ä∫·ÄÜ·Ä±·Ä∏·Äë·Ä¨·Ä∏·Äû·ÄÑ·Ä∑·Ä∫·Äï·Ä´·Äû·Ää·Ä∫·Åã\n",
    "        self.data[self.features_scaled] = self.scaler.transform(self.data[self.features_scaled])\n",
    "        # logger.info(f\"Data scaled successfully using fitted StandardScaler.\")\n",
    "        \n",
    "    # ·ÄÄ·Ä≠·Äî·Ä∫·Ä∏·Äõ·Äæ·ÄÑ·Ä∫·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·ÄÖ·Äê·ÄÑ·Ä∫·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äû·Ää·Ä∫·Åã\n",
    "    def _initialize_parameters(self, file, cf, asset, logger_show, save_plot):\n",
    "        # Params to variables\n",
    "        self.csv_file               =   file\n",
    "        self.cf                     =   cf\n",
    "        self.symbol_col             =   asset\n",
    "        self.features_scaled        =   self.cf.env_parameters('features_scaled') # Time-Series Features List\n",
    "        self.features_unscaled      =   self.cf.env_parameters('features_unscaled')\n",
    "        self.features_filter        =   self.cf.env_parameters('features_filter')\n",
    "        # Scaled Data Frame ·Åè Feature List\n",
    "        self.obs_features           =   self.features_scaled + self.features_unscaled\n",
    "        self.sequence_length        =   self.cf.data_processing_parameters(\"sequence_length\") # Transformer Lookback Window (100)\n",
    "        self.logger_show            =   logger_show\n",
    "        self.save_plot              =   save_plot\n",
    "\n",
    "        self.data_raw = pd.read_csv(file)\n",
    "        if 'time' in self.data_raw.columns:\n",
    "            self.data_raw = self.data_raw.set_index(pd.to_datetime(self.data_raw['time'], utc=True)).drop(columns=['time'])\n",
    "        \n",
    "        # self.data ·ÄÄ·Ä≠·ÄØ Scaling ·Äú·ÄØ·Äï·Ä∫·Äõ·Äî·Ä∫ Copy ·Äö·Ä∞·Äï·Ä´·Äô·Ää·Ä∫·Åã\n",
    "        self.data = self.data_raw.copy()\n",
    "            \n",
    "        # We use sequence transformer, so max steps will be this\n",
    "        self.max_steps              =   len(self.data) - self.sequence_length - 1\n",
    "\n",
    "        # Configs to variables\n",
    "        # Agent ·ÄÄ Action ·ÄÄ Continuous Action ·ÄÄ·Ä≠·ÄØ Discrete Action ·Äû·Ä≠·ÄØ·Ä∑·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Ä±·Ä∏·Äû·Ä±·Ä¨ threshold\n",
    "        self.action_threshold       =   self.cf.env_parameters('action_threshold')\n",
    "        self.balance_initial        =   self.cf.env_parameters('balance')\n",
    "\n",
    "        # position close ·Äô·Äñ·Äº·ÄÖ·Ä∫·Äû·Ä±·Ä∏·Äõ·ÄÑ·Ä∫\n",
    "        # buy ·Äë·Ä¨·Ä∏·Äï·Äº·ÄÆ·Ä∏ price up ·Äñ·Äº·ÄÖ·Ä∫·Äî·Ä±·Äõ·ÄÑ·Ä∫ reward ·Äï·Ä±·Ä∏·Åã sell ·Äë·Ä¨·Ä∏·Äï·Äº·ÄÆ·Ä∏ price down ·Äñ·Äº·ÄÖ·Ä∫·Äî·Ä±·Äõ·ÄÑ·Ä∫ reward ·Äï·Ä±·Ä∏\n",
    "        # position management ·Äô·Äæ·Ä¨·Äú·Ää·Ä∫·Ä∏ ·Äû·ÄØ·Ä∂·Ä∏·Åã\n",
    "        # buy ·Äô·Äæ·Ä¨ ·Äô·Äº·Äê·Ä∫·Äî·Ä±·Äõ·ÄÑ·Ä∫ tp ·Ä°·Äï·Ä±·Ä´·Ä∫·Äõ·ÄΩ·Ä±·Ä∑ sl ·Ä°·Äï·Ä±·Ä´·Ä∫·Äõ·ÄΩ·Ä±·Ä∑·Åã  ·Äõ·Äæ·ÄØ·Ä∂·Ä∏·Äî·Ä±·Äõ·ÄÑ·Ä∫ tp ·Ä°·Ä±·Ä¨·ÄÄ·Ä∫·Äõ·ÄΩ·Ä±·Ä∑ sl ·Ä°·Äï·Ä±·Ä´·Ä∫·Äê·ÄÑ·Ä∫,\n",
    "        # sell ·Äô·Äæ·Ä¨ ·Äô·Äº·Äê·Ä∫·Äî·Ä±·Äõ·ÄÑ·Ä∫ tp ·Ä°·Ä±·Ä¨·ÄÄ·Ä∫·Äõ·ÄΩ·Ä±·Ä∑ sl ·Ä°·Ä±·Ä¨·ÄÄ·Ä∫·Äõ·ÄΩ·Ä±·Ä∑·Åã ·Äõ·Äæ·ÄØ·Ä∂·Ä∏·Äî·Ä±·Äõ·ÄÑ·Ä∫ tp ·Ä°·Äï·Ä±·Ä´·Ä∫·Äê·ÄÑ·Ä∫ sl ·Ä°·Ä±·Ä¨·ÄÄ·Ä∫·ÄÅ·Äª\n",
    "        self.good_position_reward_scale = self.cf.env_parameters(\"good_position_reward_scale\") # ·Ä•·Äï·Äô·Ä¨: 0.01\n",
    "        # ·Äõ·Ää·Ä∫·Äõ·ÄΩ·Äö·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫ ·ÅÇ: SL/PT Trailing ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Äê·Äî·Ä∫·Äñ·Ä≠·ÄØ·Ä∏ (Move Step Size)\n",
    "        self.trailing_distance = self.cf.env_parameters(\"trailing_stop_distance_points\")\n",
    "\n",
    "        # ·Ä°·Äõ·Äæ·ÄØ·Ä∂·Ä∏·Äî·Ä≤·Ä∑·Ä°·Äô·Äº·Äê·Ä∫ ·Äô·Äª·Äæ·Äê·Äô·Äæ·ÄØ·Äõ·Äæ·Ä≠·Äê·Ä≤·Ä∑ trading performance ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Äï·Ä±·Ä∏·Äê·Ä≤·Ä∑ bonus reward 0.01\n",
    "        # self.consistency_reward = self.cf.env_parameters(\"consistency_reward\")\n",
    "        self.stop_loss = self.cf.symbol(self.symbol_col, \"stop_loss_max\")\n",
    "        self.profit_taken = self.cf.symbol(self.symbol_col, \"profit_taken_max\")\n",
    "        self.point = self.cf.symbol(self.symbol_col, \"point\")\n",
    "        self.transaction_fee = self.cf.symbol(self.symbol_col, \"transaction_fee\")\n",
    "        self.over_night_penalty = self.cf.symbol(self.symbol_col, \"over_night_penalty\")\n",
    "        self.max_current_holding = self.cf.symbol(self.symbol_col, \"max_current_holding\")\n",
    "        # Drawdown Penalty Factor\n",
    "        self.drawdown_penalty_factor = self.cf.env_parameters(\"drawdown_penalty_factor\")\n",
    "        self.margin_requirement = self.cf.env_parameters('margin_requirement')\n",
    "\n",
    "\n",
    "    # Action ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ Observation Spaces ·ÄÄ·Ä≠·ÄØ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äû·Ää·Ä∫·Åã\n",
    "    def _initialize_spaces(self):\n",
    "        # Continuous actions: [1 -> 0.5] LONG | [0.5 -> -0.5] HOLD |[-0.5 -> -1] SHORT\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1,\n",
    "            high=1,\n",
    "            shape=(1,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # [MODIFIED] Transformer Observation Space: Time Series (100) + Context (4)\n",
    "        N_FEATURES_TS = len(self.obs_features)\n",
    "        N_FEATURES_CONTEXT = 4 # [Equity, Drawdown, Open_Pos_Ratio, Time_Context]\n",
    "        \n",
    "        # [MODIFIED] Observation Space (Time Series Sequence Only)\n",
    "        # Transformer ·Äû·ÄØ·Ä∂·Ä∏·Äë·Ä¨·Ä∏·Äû·Ä±·Ä¨ features ·Äê·ÄΩ·Ä±·Äõ·Ä≤·Ä∑ previous sequence length candle ·ÄÄ·Ä≠·ÄØ·Äï·Ä´ ·Äê·Äï·Äº·Ä≠·ÄØ·ÄÑ·Ä∫·Äê·Ää·Ä∫·Ä∏·ÄÄ·Äº·Ää·Ä∑·Ä∫\n",
    "        obs_shape = (self.sequence_length, N_FEATURES_TS + N_FEATURES_CONTEXT)\n",
    "        \n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, \n",
    "            high=np.inf, # Scaled Data ·Äô·Äª·Ä¨·Ä∏·Äû·Ää·Ä∫ Theoretical Inf/ -Inf ·Äõ·Äæ·Ä≠·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äû·Ä±·Ä¨·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∑·Ä∫ np.inf ·ÄÄ·Ä≠·ÄØ ·Äû·ÄØ·Ä∂·Ä∏·Äï·Ä´\n",
    "            shape=obs_shape,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    # Environment ·ÄÄ·Ä≠·ÄØ ·Ä°·ÄÖ·Äï·Äº·ÄØ·Ä°·ÄÅ·Äº·Ä±·Ä°·Äî·Ä±·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Äî·Ä∫·Äú·Ää·Ä∫·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äû·Ää·Ä∫·Åã\n",
    "    def reset(self, *, seed = None, options = None):\n",
    "        super().reset(seed=seed, options=options)\n",
    "\n",
    "        self.ticket_id          =   0\n",
    "        self.ttl_rewards        =   0 # total rewards\n",
    "\n",
    "        self.balance            =   self.balance_initial\n",
    "        self.positions          =   []\n",
    "\n",
    "        # equity tracking\n",
    "        self.equity_curve       =   [self.balance_initial] # Starting with initial balance\n",
    "        # ·Ä°·Äô·Äº·ÄÑ·Ä∑·Ä∫·ÄÜ·ÄØ·Ä∂·Ä∏·Äõ·Ä±·Ä¨·ÄÄ·Ä∫·Äñ·Ä∞·Ä∏·Äê·Ä≤·Ä∑ eq value\n",
    "        self.peak_equity        =   self.balance_initial # Start with initial balance as peak\n",
    "\n",
    "        self.max_drawdown       =   0.0\n",
    "        self.current_drawdown   =   0.0\n",
    "\n",
    "        # transformer ·Ä°·Äû·ÄØ·Ä∂·Ä∏·Äï·Äº·ÄØ·Äë·Ä¨·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∑\n",
    "        self.current_step       =   self.sequence_length\n",
    "        logger.info(f\"--- Environment reset. Starting at step {self.current_step} --total rewards: {self.ttl_rewards}\")\n",
    "\n",
    "        observation             =   self._next_observation()\n",
    "        info                    =   {}\n",
    "        return  observation, info\n",
    "\n",
    "\n",
    "# AI model ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Äú·ÄÄ·Ä∫·Äõ·Äæ·Ä≠ market condition ·ÄÄ·Ä≠·ÄØ·ÄÄ·Ä≠·ÄØ·Äö·Ä∫·ÄÖ·Ä¨·Ä∏·Äï·Äº·ÄØ·Äê·Ä≤·Ä∑ observation data ·ÄÄ·Ä≠·ÄØ ·Äï·Äº·ÄÑ·Ä∫·ÄÜ·ÄÑ·Ä∫·Äï·Ä±·Ä∏·Äñ·Ä≠·ÄØ·Ä∑·Äñ·Äº·ÄÖ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫·Åã\n",
    "    def _next_observation(self):\n",
    "        \n",
    "        # 1. Time Series Observation (Scaled Data)\n",
    "        obs_ts = self.data.iloc[\n",
    "            self.current_step - self.sequence_length: self.current_step\n",
    "        ][self.obs_features].values # Shape: (100, N_Features_TS)\n",
    "\n",
    "        # 2. Account State (Non-Time-Series / Context Vector)\n",
    "        current_equity = self._calculate_current_equity()\n",
    "        open_positions_count = sum(1 for p in self.positions if p['Status'] == 0)\n",
    "\n",
    "        obs_context = np.array([\n",
    "            current_equity / self.balance_initial, # 1. Normalized Equity\n",
    "            self.current_drawdown,                 # 2. Current Drawdown (Percentage)\n",
    "            open_positions_count / self.max_current_holding, # 3. Open Positions Ratio\n",
    "            self.data.iloc[self.current_step]['hour_cos']   # 4. Time Context (Scaled)\n",
    "        ], dtype=np.float32) # Shape: (4,)\n",
    "\n",
    "        # 3. Final Observation Construction (Time Series + Context)\n",
    "        \n",
    "        # Context features ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ Sequence Length (100) ·Ä°·Äú·Ä≠·ÄØ·ÄÄ·Ä∫ ·Äñ·Äº·Äî·Ä∑·Ä∫·ÄÄ·Äª·ÄÄ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏ (Broadcasting)\n",
    "        obs_context_expanded = np.tile(obs_context, (self.sequence_length, 1)) # Shape: (100, 4)\n",
    "        \n",
    "        # Horizontal Stack (Sequence length, N_Features_TS + N_Features_Context)\n",
    "        obs_final = np.hstack([obs_ts, obs_context_expanded])\n",
    "\n",
    "        # 4. PyTorch/Device Conversion and Validation\n",
    "        try:\n",
    "            # NumPy array ‚Üí PyTorch tensor ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äô·Äö·Ä∫\n",
    "            obs_tensor = torch.tensor(obs_final, dtype=torch.float32).to(device)\n",
    "            # Data Validation ·ÄÖ·ÄÖ·Ä∫·ÄÜ·Ä±·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "            if torch.isnan(obs_tensor).any() or torch.isinf(obs_tensor).any():\n",
    "                logger.error(f\"Invalid observation (NaN/Inf) at step {self.current_step}\")\n",
    "                raise ValueError(f\"Invalid observation (NaN/Inf) at step {self.current_step}\")\n",
    "            return obs_tensor.cpu().numpy()\n",
    "            \n",
    "        except NameError:\n",
    "            # Torch ·ÄÄ·Ä≠·ÄØ ·Äô·Äû·ÄØ·Ä∂·Ä∏·Äï·Ä´·ÄÄ NumPy ·ÄÄ·Ä≠·ÄØ·Äû·Ä¨ ·Äï·Äº·Äî·Ä∫·Äï·Ä±·Ä∏·Äï·Ä´·Åã\n",
    "            if np.isnan(obs_final).any() or np.isinf(obs_final).any():\n",
    "                logger.error(f\"Invalid observation (NaN/Inf) at step {self.current_step}\")\n",
    "                raise ValueError(f\"Invalid observation (NaN/Inf) at step {self.current_step}\")\n",
    "            return obs_final # Final NumPy array\n",
    "        \n",
    "            \n",
    "    def _ray_mask(self, a, c, bounds):\n",
    "        \"\"\"\n",
    "        Ray Mask ·Äú·ÄØ·Äï·Ä∫·Äï·Ä´·Åã\n",
    "        a: ·Äô·Ä∞·Äõ·ÄÑ·Ä∫·Ä∏ action (np.array)\n",
    "        c: ·Äû·ÄÄ·Ä∫·ÄÜ·Ä≠·ÄØ·ÄÑ·Ä∫·Äõ·Ä¨ ·Ä°·ÄÖ·ÄØ ·Äõ·Ä≤·Ä∑ ·Ä°·Äú·Äö·Ä∫·Äó·Äü·Ä≠·ÄØ\n",
    "        A_r_boundary_func: lambda_A_r ·Äê·ÄΩ·ÄÄ·Ä∫·Äê·Ä≤·Ä∑ func\n",
    "        A_boundary_func: lambda_A ·Äê·ÄΩ·ÄÄ·Ä∫·Äê·Ä≤·Ä∑ func\n",
    "        \"\"\"\n",
    "        if np.allclose(a, c):\n",
    "            return c  # ·Ä°·Äú·Äö·Ä∫·Äô·Äæ·Ä¨ ·ÄÜ·Ä≠·ÄØ ·Äô·Äõ·ÄΩ·Äæ·Ä±·Ä∑\n",
    "\n",
    "        direction = a - c\n",
    "        norm_dir = direction / np.linalg.norm(direction)\n",
    "\n",
    "        lambda_A_r = bounds[1] - c if norm_dir > 0 else c - bounds[0]\n",
    "        lambda_A = 1 - c if norm_dir > 0 else c - (-1)\n",
    "\n",
    "\n",
    "        if lambda_A_r <= 0 or lambda_A <= 0:\n",
    "            return c  # ·Ä°·Äô·Äæ·Ä¨·Ä∏ ·Äõ·Äæ·Ä±·Ä¨·ÄÑ·Ä∫\n",
    "\n",
    "        scale = lambda_A_r / lambda_A\n",
    "        a_r = c + scale * direction\n",
    "        return np.clip(a_r, -1, 1)  # Action space ·ÄÄ·Äî·Ä∫·Ä∑·Äû·Äê·Ä∫\n",
    "\n",
    "\n",
    "    def _get_action_name(self, _action, ma_first, ma_slow):\n",
    "\n",
    "        c = 0.0  # ·Ä°·Äú·Äö·Ä∫·Äó·Äü·Ä≠·ÄØ (hold)\n",
    "        if ma_first > ma_slow:  # Uptrend: buy only [0, 1]\n",
    "            bounds = [0, 1]\n",
    "        else:  # Downtrend: sell only [-1, 0]\n",
    "            bounds = [-1, 0]\n",
    "\n",
    "        a_masked = self._ray_mask(_action, c, bounds)\n",
    "\n",
    "        \"\"\"Convert continuous action to discrete action name\"\"\"\n",
    "        if a_masked >= self.action_threshold:\n",
    "            return \"BUY\"\n",
    "        elif a_masked <= -self.action_threshold:\n",
    "            return \"SELL\"\n",
    "        else:\n",
    "            return \"HOLD\"\n",
    "\n",
    "    def step(self, action):\n",
    "        # self.data ·Äû·Ää·Ä∫ Index ·Äê·ÄΩ·ÄÑ·Ä∫ 'time' ·ÄÄ·Ä≠·ÄØ ·Äë·Ä¨·Ä∏·Äõ·Äæ·Ä≠·Äï·Äº·ÄÆ·Ä∏ drop ·Äú·ÄØ·Äï·Ä∫·Äë·Ä¨·Ä∏·Äû·Ä±·Ä¨·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∑·Ä∫·Åä Index ·Äô·Äæ time ·ÄÄ·Ä≠·ÄØ ·Äö·Ä∞·Äõ·Äî·Ä∫·Äú·Ä≠·ÄØ·Äû·Ää·Ä∫·Åã\n",
    "        current_row_raw = self.data_raw.iloc[self.current_step]\n",
    "        \n",
    "        # Unscaled Price Features\n",
    "        _o, _h, _l, _c, ma_fast, ma_slow = current_row_raw[['open', 'high', 'low', 'close', 'ma_fast', 'ma_slow']]\n",
    "        \n",
    "        _t = self.data.index[self.current_step] # Get time from index    \n",
    "            \n",
    "        reward                      =   0 # ·Äí·ÄÆ step ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·ÄÖ·ÄØ·ÄÖ·ÄØ·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏ reward\n",
    "        position_reward             =   0 # Position ·Äï·Ä≠·Äê·Ä∫·Äõ·ÄÑ·Ä∫ ·Äõ·Äê·Ä≤·Ä∑ reward\n",
    "        action_hold_reward          =   0 # Hold action ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ reward/penalty\n",
    "\n",
    "        _msg                        =   []\n",
    "        _action                     =   action[0] # action value eg. [0.75]\n",
    "        open_position               =   0\n",
    "        for position in self.positions:\n",
    "            if position['Status']   ==  0:\n",
    "                position_reward, closed, _msg   =   self._calculate_reward(position)\n",
    "                if not closed: open_position += 1  # Count what we already knew\n",
    "                reward += position_reward\n",
    "\n",
    "        # Continuous actions: [1 -> 0.5] LONG | [0.5 -> -0.5] HOLD |[-0.5 -> -1] SHORT\n",
    "        action_name = self._get_action_name(action, ma_fast, ma_slow)\n",
    "\n",
    "        if open_position < self.max_current_holding and action_name in ['BUY', 'SELL']:\n",
    "            self.ticket_id  +=  1\n",
    "\n",
    "            # Real trading ·Äô·Äæ·Ä¨ margin requirement ·Äõ·Äæ·Ä≠·Äû·Äú·Ä≠·ÄØ·Äô·Äª·Ä≠·ÄØ·Ä∏\n",
    "            # Position ·Äñ·ÄΩ·ÄÑ·Ä∑·Ä∫·Äõ·ÄÑ·Ä∫ capital ·ÄÅ·Äª·ÄØ·Äï·Ä∫·ÄÑ·Äº·Ä¨·Ä∏·Äî·Ä±·Äõ·Äê·Äö·Ä∫\n",
    "            # Position ·Äï·Ä≠·Äê·Ä∫·Äê·Ä≤·Ä∑·Ä°·ÄÅ·Ä´ ·Äï·Äº·Äî·Ä∫·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏·Äë·Ää·Ä∑·Ä∫·Äï·Ä±·Ä∏·Äê·Äö·Ä∫\n",
    "            self.balance -= self.margin_requirement # hold up, this will make sure model can not open a lot of\n",
    "\n",
    "            position        =   {\n",
    "                \"Ticket\"        :   self.ticket_id,\n",
    "                \"Symbol\"        :   self.symbol_col,\n",
    "                \"ActionTime\"    :   _t,\n",
    "                \"Type\"          :   action_name,\n",
    "                \"Lot\"           :   1,\n",
    "                \"ActionPrice\"   :   _c,\n",
    "                \"SL\"            :   self.stop_loss,\n",
    "                \"PT\"            :   self.profit_taken,\n",
    "                \"MaxDD\"         :   0,\n",
    "                \"Swap\"          :   0.0,\n",
    "                \"CloseTime\"     :   \"\",\n",
    "                \"ClosePrice\"    :   0.0,\n",
    "                \"Point\"         :   self.point,\n",
    "                \"Reward\"        :   self.transaction_fee,\n",
    "                \"DateDuration\"  :   _t.date().isoformat(),\n",
    "                \"Status\"        :   0, # 0 is Position is currently OPEN and active\n",
    "                #\"PIPS\"          :   self.transaction_fee, # Price Interest Point (profit/loss ·ÄÄ·Ä≠·ÄØ measure ·Äú·ÄØ·Äï·Ä∫·Äê·Ä≤·Ä∑ unit)\n",
    "                \"PIPS\"          :   0,\n",
    "                \"ActionStep\"    :   self.current_step,\n",
    "                \"CloseStep\"     :   -1, # Step number when position closed, not close yet is -1\n",
    "                \"DeltaStep\"     :   0,\n",
    "                \"OpenBal\"       :   self.balance,\n",
    "                \"CloseBal\"       :   0,\n",
    "                \"HighestPrice\"  :   _c,\n",
    "                \"LowestPrice\"   :   _c,\n",
    "            }\n",
    "\n",
    "            self.positions.append(position)\n",
    "            # do not use transaction_fee penalty\n",
    "            # reward = self.transaction_fee #open cost\n",
    "            # model ·ÄÄ ·Ä°·Äú·ÄΩ·Äî·Ä∫·Ä°·ÄÄ·Äª·ÄΩ·Ä∂ position ·Äê·ÄΩ·Ä± ·Äô·Äñ·ÄΩ·ÄÑ·Ä∑·Ä∫·Äô·Ä≠·Ä°·Ä±·Ä¨·ÄÑ·Ä∫ ·Äë·Ä≠·Äî·Ä∫·Ä∏·ÄÅ·Äª·ÄØ·Äï·Ä∫·Äê·Ä≤·Ä∑ mechanism ·Äñ·Äº·ÄÖ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫·Åã\n",
    "            _msg.append(f'Step:{self.current_step} Tkt:{position[\"Ticket\"]} {position[\"Type\"]} Rwd:{position[\"PIPS\"]} SL:{position[\"SL\"]} PT:{position[\"PT\"]}')\n",
    "\n",
    "        # HOLD Penalty ·ÄÄ·Ä≠·ÄØ ·Ä°·Äú·ÄΩ·Äî·Ä∫·Äû·Ä±·Ä∏·ÄÑ·Äö·Ä∫·Äû·Ä±·Ä¨ ·Äê·Äî·Ä∫·Äñ·Ä≠·ÄØ·Ä∏\n",
    "        # (·Ä•·Äï·Äô·Ä¨: -0.0001) ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Ä´·Åã ·Ä°·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÜ·ÄØ·Ä∂·Ä∏·Äô·Äæ·Ä¨\n",
    "        # Trading ·Äô·Äú·ÄØ·Äï·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ Penalty ·Äô·Äï·Ä±·Ä∏·Äò·Ä≤ action_hold_reward = 0 ·Äë·Ä¨·Ä∏·Äï·Ä´·Åã\n",
    "        elif open_position < self.max_current_holding and action_name == \"HOLD\":\n",
    "            action_hold_reward  =   0  # no open any position, encourage open position\n",
    "        else:\n",
    "            action_hold_reward  =   0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        reward              +=  action_hold_reward\n",
    "\n",
    "        # Move to the next time step\n",
    "        self.current_step   +=  1\n",
    "\n",
    "        # check if episode is done\n",
    "        terminated          =   (self.balance <= 0)\n",
    "        truncated           =   (self.current_step > self.max_steps)\n",
    "\n",
    "        # get next observation\n",
    "        obs                 =   self._next_observation()\n",
    "        _msg.append(f'---idle----step:{self.current_step}, RF:{action_name} Action:{_action} Balance: {self.balance} reward:{reward} total_rewards:{self.ttl_rewards} position_reward:{position_reward} action_hold_reward:{action_hold_reward}')\n",
    "\n",
    "\n",
    "        current_equity = self._calculate_current_equity()\n",
    "        self.equity_curve.append(current_equity)\n",
    "        self._calculate_drawdown()  # This updates peak_equity and drawdowns\n",
    "\n",
    "        # =========================================================================\n",
    "        # START: Drawdown Penalty Logic\n",
    "        # =========================================================================\n",
    "        # self.current_drawdown ·Äû·Ää·Ä∫ Percentage (0.0 ·Äô·Äæ 1.0) ·Äñ·Äº·ÄÖ·Ä∫·Äû·Ää·Ä∫·Åã\n",
    "\n",
    "\n",
    "        drawdown_penalty = self.current_drawdown * self.drawdown_penalty_factor\n",
    "        # Reward ·Äê·ÄΩ·ÄÑ·Ä∫ ·Äî·ÄØ·Äê·Ä∫·Äï·Ä±·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "        reward -= drawdown_penalty\n",
    "\n",
    "        # Log the penalty for debugging\n",
    "        _msg.append(f'Drawdown Penalty: -{drawdown_penalty:.4f} (DD:{self.current_drawdown:.4f})')\n",
    "        # =========================================================================\n",
    "        # END: Drawdown Penalty Logic\n",
    "        # =========================================================================\n",
    "        # Drawdown Penalty ·Äî·ÄØ·Äê·Ä∫·Äï·Äº·ÄÆ·Ä∏·Äô·Äæ·Äû·Ä¨ ·ÄÖ·ÄØ·ÄÖ·ÄØ·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏ Reward ·ÄÄ·Ä≠·ÄØ ·Ä°·Äï·Ä∫·Äí·Ä≠·Äê·Ä∫·Äú·ÄØ·Äï·Ä∫·Äï·Ä´\n",
    "        self.ttl_rewards += reward  # <--- ·Ä§·Äî·Ä±·Äõ·Ä¨·Äê·ÄΩ·ÄÑ·Ä∫ ·Äï·Äº·Äî·Ä∫·Äë·Ää·Ä∑·Ä∫·Äï·Ä´\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        if terminated or truncated:\n",
    "            buy_positions = [p for p in self.positions if p[\"Type\"] == \"BUY\"]\n",
    "            sell_positions = [p for p in self.positions if p[\"Type\"] == \"SELL\"]\n",
    "\n",
    "            buy_count = len(buy_positions)\n",
    "            sell_count = len(sell_positions)\n",
    "            total_positions = len(self.positions)\n",
    "\n",
    "            # Calculate win rates\n",
    "            buy_wins = len([p for p in buy_positions if p[\"PIPS\"] > 0])\n",
    "            sell_wins = len([p for p in sell_positions if p[\"PIPS\"] > 0])\n",
    "\n",
    "            buy_win_rate = buy_wins / buy_count if buy_count > 0 else 0\n",
    "            sell_win_rate = sell_wins / sell_count if sell_count > 0 else 0\n",
    "\n",
    "            _m = f'--- Positions: {total_positions} (Buy:{buy_count}, Sell:{sell_count}) | '\n",
    "            _m += f'WinRates: Buy:{buy_win_rate:.1%}, Sell:{sell_win_rate:.1%} | '\n",
    "            _m += f'TotalRewards: {self.ttl_rewards} Balance: {self.balance}'\n",
    "\n",
    "            logger.info(_m)\n",
    "            _msg.append(_m)\n",
    "\n",
    "            # Additional info\n",
    "            if self.logger_show:\n",
    "                for _m in _msg:\n",
    "                    logger.info(_m)\n",
    "\n",
    "            info[\"info\"]                = _msg\n",
    "            info[\"sharpe\"]              = self._calculate_sharpe()  # ‚úÖ Now works! üí° 'sharpe_ratio' ·Äô·Äæ 'sharpe' ·Äû·Ä≠·ÄØ·Ä∑·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Ä´·Åã\n",
    "            info[\"drawdown\"]            = self.max_drawdown         # ‚úÖ Now accurate!'max_drawdown' ·Äô·Äæ 'drawdown' ·Äû·Ä≠·ÄØ·Ä∑·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Ä´·Åã\n",
    "            info[\"current_equity\"]      = current_equity            # ‚úÖ For debugging\n",
    "            info[\"peak_equity\"]         = self.peak_equity          # ‚úÖ For debugging\n",
    "            info[\"equity_curve_length\"] = len(self.equity_curve)    # ‚úÖ Monitor growth\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _calculate_reward(self, position):\n",
    "        _o, _h, _l, _c              = self.data_raw.iloc[self.current_step][['open', 'high', 'low', 'close']]\n",
    "        _t                          = self.data.index[self.current_step]\n",
    "        _msg                        =   []\n",
    "\n",
    "        entry_price                 =   position['ActionPrice']\n",
    "        direction                   =   position['Type']\n",
    "        profit_target_price         =   entry_price + position['PT']/ self.point if direction == 'BUY' else entry_price - position['PT']/self.point\n",
    "        stop_loss_price             =   entry_price + position['SL']/ self.point if direction == 'BUY' else entry_price - position['SL']/self.point\n",
    "        closed                      =   False\n",
    "        close_position_reward       =   0.0\n",
    "        good_position_reward        =   0.0\n",
    "\n",
    "        # Check for stoploss hit\n",
    "        if (direction == 'BUY' and _l <= stop_loss_price) or (direction == 'SELL' and _h >= stop_loss_price):\n",
    "            close_position_reward   =   position['SL'] # position sl ·ÄÄ minus value ·Äñ·Äº·ÄÖ·Ä∫·Äê·Äö·Ä∫\n",
    "\n",
    "            position['CloseTime']   =   _t\n",
    "            position['ClosePrice']  =   stop_loss_price\n",
    "            position['Status']      =   1   # Status ·ÄÄ open ·ÄÜ·Ä≠·ÄØ 0 close ·ÄÜ·Ä≠·ÄØ 1\n",
    "            position['CloseStep']   =   self.current_step\n",
    "            position['PIPS']        =   close_position_reward - self.transaction_fee\n",
    "            position['DeltaStep']   =   self.current_step - position['ActionStep']\n",
    "            position['Info']        =   f'{profit_target_price:.5f} | {stop_loss_price:.5f}'\n",
    "\n",
    "            self.balance            +=  self.margin_requirement + position['PIPS'] # return 100 is margin hold\n",
    "            position['CloseBal']    =   self.balance\n",
    "            closed                  =   True\n",
    "            _msg.append(f'Step:{self.current_step} Tkt:{position[\"Ticket\"]}: Rwd:{position[\"PIPS\"]}, SL:{position[\"SL\"]}, DeltaStep:{position[\"DeltaStep\"]}')\n",
    "\n",
    "        elif (direction == 'BUY' and _h >= profit_target_price) or (direction == 'SELL' and _l <= profit_target_price):\n",
    "            close_position_reward   =    position['PT'] # position tp ·ÄÄ plus value ·Äñ·Äº·ÄÖ·Ä∫·Äê·Äö·Ä∫\n",
    "\n",
    "            position['CloseTime']   =   _t\n",
    "            position['ClosePrice']  =   profit_target_price\n",
    "            position['Status']      =   2   # Status ·ÄÄ open ·ÄÜ·Ä≠·ÄØ 0 close ·ÄÜ·Ä≠·ÄØ 1\n",
    "            position['CloseStep']   =   self.current_step\n",
    "            position['PIPS']        =   close_position_reward - self.transaction_fee\n",
    "            position['DeltaStep']   =   self.current_step - position['ActionStep']\n",
    "            position['Info']        =   f'{profit_target_price:.5f} | {stop_loss_price:.5f}'\n",
    "\n",
    "            self.balance            +=  self.margin_requirement + position['PIPS'] # return 100 is margin hold\n",
    "            position['CloseBal']    =   self.balance\n",
    "            closed                  =   True\n",
    "            _msg.append(f'Step:{self.current_step} Tkt:{position[\"Ticket\"]}: Rwd:{position[\"PIPS\"]}, SL:{position[\"SL\"]}, DeltaStep:{position[\"DeltaStep\"]}')\n",
    "\n",
    "        else:\n",
    "            if self.current_step + 5 + self.sequence_length >= len(self.data):\n",
    "                close_position_reward   =   (_c - position[\"ActionPrice\"] if direction == 'BUY' else position[\"ActionPrice\"] - _c)* self.point\n",
    "\n",
    "                position['CloseTime']   =   _t\n",
    "                position['ClosePrice']  =   _c\n",
    "                position['Status']      =   3   # Status ·ÄÄ open ·ÄÜ·Ä≠·ÄØ 0 close ·ÄÜ·Ä≠·ÄØ 1, force close 2\n",
    "                position['CloseStep']   =   self.current_step\n",
    "                position['PIPS']        =   close_position_reward - self.transaction_fee\n",
    "                position['DeltaStep']   =   self.current_step - position['ActionStep']\n",
    "                position['Info']        =   f'{profit_target_price:.5f} | {stop_loss_price:.5f}'\n",
    "                self.balance            +=  self.margin_requirement + position[\"PIPS\"] # return 100 is margin hold\n",
    "                position['CloseBal']    =   self.balance\n",
    "\n",
    "                closed                  =   True\n",
    "                _msg.append(f'Step:{self.current_step} Tkt:{position[\"Ticket\"]}: Rwd:{position[\"PIPS\"]}, Cls:End, DeltaStep:{position[\"DeltaStep\"]}')\n",
    "\n",
    "            else:\n",
    "                # =========================================================================\n",
    "                # Real Trailing Stop Logic (·Ä°·Äô·Äº·ÄÑ·Ä∑·Ä∫·ÄÜ·ÄØ·Ä∂·Ä∏ ·Äõ·Ä±·Ä¨·ÄÄ·Ä∫·Äñ·Ä∞·Ä∏·Äû·Ä±·Ä¨ ·Äà·Ä±·Ä∏·Äî·Äæ·ÄØ·Äî·Ä∫·Ä∏·ÄÄ·Ä≠·ÄØ ·Äô·Äæ·Äê·Ä∫·Äê·Äô·Ä∫·Ä∏·Äê·ÄÑ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏)\n",
    "                # =========================================================================\n",
    "                # 1. Highest/Lowest Price Update\n",
    "\n",
    "                if direction == \"BUY\":\n",
    "                  # Buy position ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Ä°·Äô·Äº·ÄÑ·Ä∑·Ä∫·ÄÜ·ÄØ·Ä∂·Ä∏ ·Äõ·Ä±·Ä¨·ÄÄ·Ä∫·Äñ·Ä∞·Ä∏·Äû·Ä±·Ä¨ ·Äà·Ä±·Ä∏·Äî·Äæ·ÄØ·Äî·Ä∫·Ä∏·ÄÄ·Ä≠·ÄØ ·Äô·Äæ·Äê·Ä∫·Äê·Äô·Ä∫·Ä∏·Äê·ÄÑ·Ä∫\n",
    "                  if _c > position[\"HighestPrice\"]:\n",
    "                      position[\"HighestPrice\"] = _c\n",
    "\n",
    "                  # 2. New SL Target Price (Trailing Price) ·ÄÄ·Ä≠·ÄØ ·Äê·ÄΩ·ÄÄ·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "                  # New_SL_Price = HighestPrice - (Trailing Distance Pips ·ÄÄ·Ä≠·ÄØ Price Change ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏)\n",
    "                  trailing_price = position[\"HighestPrice\"] - self.trailing_distance / self.point\n",
    "\n",
    "                  # 3. SL ·ÄÄ·Ä≠·ÄØ ·Ä°·ÄÜ·ÄÑ·Ä∑·Ä∫·Äô·Äº·Äæ·ÄÑ·Ä∑·Ä∫·Äê·ÄÑ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "                  # ·Äú·ÄÄ·Ä∫·Äõ·Äæ·Ä≠ SL ·Äë·ÄÄ·Ä∫ ·Äï·Ä≠·ÄØ·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äô·Äæ·Äû·Ä¨ ·Äõ·ÄΩ·Ä±·Ä∑·Äï·Ä´\n",
    "                  if trailing_price > stop_loss_price:\n",
    "\n",
    "                      stop_loss_price = trailing_price\n",
    "                      # SL_Price ·Ä°·Äû·ÄÖ·Ä∫·ÄÄ·Ä≠·ÄØ Points ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Äî·Ä∫·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Äº·ÄÆ·Ä∏ position['SL'] ·ÄÄ·Ä≠·ÄØ ·Ä°·Äï·Ä∫·Äí·Ä≠·Äê·Ä∫·Äú·ÄØ·Äï·Ä∫·Äï·Ä´\n",
    "                      position[\"SL\"] = (stop_loss_price - entry_price) * self.point\n",
    "                    #   if position[\"SL\"] > 0:\n",
    "                    #       position[\"SL\"]    =   -abs(position[\"SL\"])\n",
    "                      trailing_happened = True\n",
    "                  else:\n",
    "                      trailing_happened = False\n",
    "\n",
    "\n",
    "                elif direction == \"SELL\":\n",
    "                  # Sell position ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Ä°·Äî·Ä≠·Äô·Ä∑·Ä∫·ÄÜ·ÄØ·Ä∂·Ä∏ ·Äõ·Ä±·Ä¨·ÄÄ·Ä∫·Äñ·Ä∞·Ä∏·Äû·Ä±·Ä¨ ·Äà·Ä±·Ä∏·Äî·Äæ·ÄØ·Äî·Ä∫·Ä∏·ÄÄ·Ä≠·ÄØ ·Äô·Äæ·Äê·Ä∫·Äê·Äô·Ä∫·Ä∏·Äê·ÄÑ·Ä∫\n",
    "                  if _c < position[\"LowestPrice\"]:\n",
    "                      position[\"LowestPrice\"] = _c\n",
    "\n",
    "                  # New SL Target Price (Trailing Price) ·ÄÄ·Ä≠·ÄØ ·Äê·ÄΩ·ÄÄ·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "                  trailing_price = position[\"LowestPrice\"] + self.trailing_distance / self.point\n",
    "\n",
    "                  # SL ·ÄÄ·Ä≠·ÄØ ·Ä°·ÄÜ·ÄÑ·Ä∑·Ä∫·Äô·Äº·Äæ·ÄÑ·Ä∑·Ä∫·Äê·ÄÑ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "                  if trailing_price < stop_loss_price:\n",
    "                      stop_loss_price = trailing_price\n",
    "                      # SL_Price ·Ä°·Äû·ÄÖ·Ä∫·ÄÄ·Ä≠·ÄØ Points ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Äî·Ä∫·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Äº·ÄÆ·Ä∏ position['SL'] ·ÄÄ·Ä≠·ÄØ ·Ä°·Äï·Ä∫·Äí·Ä≠·Äê·Ä∫·Äú·ÄØ·Äï·Ä∫·Äï·Ä´\n",
    "                      position[\"SL\"] = (entry_price - stop_loss_price) * self.point\n",
    "                    #   if position[\"SL\"] > 0:\n",
    "                    #       position[\"SL\"]    =   -abs(position[\"SL\"])\n",
    "                      trailing_happened = True\n",
    "                  else:\n",
    "                      trailing_happened = False\n",
    "\n",
    "                # =========================================================================\n",
    "                # Reward Logic (Trailing ·Äú·ÄØ·Äï·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ Bonus ·Äï·Ä±·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏)\n",
    "                # =========================================================================\n",
    "                # Reward Sign ·ÄÄ·Ä≠·ÄØ ·Äö·ÄÅ·ÄÑ·Ä∫·Ä°·Äê·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏ ·Äê·ÄΩ·ÄÄ·Ä∫·Äï·Ä´·Åã\n",
    "                delta = _c - entry_price\n",
    "                if direction == \"BUY\":\n",
    "                    reward_sign = 1 if delta >= 0 else -1\n",
    "                elif direction == \"SELL\":\n",
    "                    reward_sign = -1 if delta >= 0 else 1\n",
    "\n",
    "                good_position_reward = reward_sign * self.good_position_reward_scale\n",
    "\n",
    "                # Trailing ·Ä°·Äô·Äæ·Äî·Ä∫·Äê·ÄÄ·Äö·Ä∫ ·Äñ·Äº·ÄÖ·Ä∫·Äû·ÄΩ·Ä¨·Ä∏·Äô·Äæ·Äû·Ä¨ Bonus Reward ·ÄÄ·Ä≠·ÄØ ·Äï·Ä±·Ä∏·Äï·Ä´\n",
    "                if trailing_happened:\n",
    "                    good_position_reward += 0.001\n",
    "\n",
    "                position['Info']        =   f'{profit_target_price:.5f} | {stop_loss_price:.5f}'\n",
    "                position['CloseBal']    =   self.balance\n",
    "                _msg.append(f'Step:{self.current_step} Tkt:{position[\"Ticket\"]}: NO_Close, PT:{position[\"PT\"]}, SL:{position[\"SL\"]}')\n",
    "\n",
    "        return close_position_reward + good_position_reward, closed, _msg\n",
    "\n",
    "\n",
    "    def _calculate_sharpe(self, risk_free_rate=0.0):\n",
    "        \"\"\"Calculate Sharpe ratio for the current episode\"\"\"\n",
    "        if len(self.equity_curve) < 2:\n",
    "            return 0.0\n",
    "\n",
    "        returns = np.diff(self.equity_curve) / self.equity_curve[:-1]\n",
    "\n",
    "        if np.std(returns) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        sharpe = (np.mean(returns) - risk_free_rate) / np.std(returns)\n",
    "        return float(sharpe * np.sqrt(288))  # Annualized (5-min bars ‚Üí 288/day)\n",
    "\n",
    "    def _calculate_drawdown(self):\n",
    "        \"\"\"Update max drawdown during episode\"\"\"\n",
    "        current_equity          =   self.equity_curve[-1]\n",
    "        self.peak_equity        =   max(self.peak_equity, current_equity)\n",
    "        self.current_drawdown   =   (self.peak_equity - current_equity) / self.peak_equity\n",
    "        self.max_drawdown       =   max(self.max_drawdown, self.current_drawdown)\n",
    "\n",
    "\n",
    "    def _calculate_current_equity(self):\n",
    "        \"\"\"Calculate total current equity (balance + unrealized P/L)\"\"\"\n",
    "        total_equity = self.balance  # Start with cash balance\n",
    "\n",
    "        # Add unrealized P/L from open positions\n",
    "        for position in self.positions:\n",
    "            if position['Status'] == 0:  # Only open positions\n",
    "                current_price = self.data.iloc[self.current_step][\"close\"]\n",
    "                entry_price = position['ActionPrice']\n",
    "\n",
    "                if position['Type'] == 'BUY':\n",
    "                    unrealized_pnl = (current_price - entry_price) * self.point\n",
    "                else:  # Sell\n",
    "                    unrealized_pnl = (entry_price - current_price) * self.point\n",
    "\n",
    "                total_equity += unrealized_pnl\n",
    "\n",
    "        return total_equity\n",
    "\n",
    "    def render(self, mode='human', title=None, **kwargs):\n",
    "        # Render the environment to the screen\n",
    "        if mode in ('human', 'file'):\n",
    "            log_header      =   True\n",
    "            printout        =   False\n",
    "            if mode == 'human':\n",
    "                printout    =   True\n",
    "\n",
    "            log_file = self.csv_file.replace(\"split/\", \"log/\")\n",
    "            pm = {\n",
    "                \"log_header\": log_header,\n",
    "                \"log_filename\": log_file,\n",
    "                \"printout\": printout,\n",
    "                \"balance\": self.balance,\n",
    "                \"balance_initial\": self.balance_initial,\n",
    "                \"transaction_close_this_step\": self.positions,\n",
    "                \"done_information\": False\n",
    "            }\n",
    "            render_to_file(**pm)\n",
    "            if log_header:\n",
    "                    log_header = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

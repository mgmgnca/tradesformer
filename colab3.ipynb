{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be11ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa5f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stable_baselines3\n",
    "!pip install gymnasium\n",
    "!pip install torch\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install finta\n",
    "!pip install mplfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab5ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import logging\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "# from stable_baselines3.common.callbacks import LearningRateSchedule\n",
    "from stable_baselines3 import PPO\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# Configure logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer-based model for time series data.\n",
    "    This class projects input features to an embedding, adds positional\n",
    "    encodings, and then processes the inputs using a Transformer encoder.\n",
    "    Finally, a decoder layer is used to produce the output.\n",
    "    Args:\n",
    "        input_size (int): Number of features in the input time series data.\n",
    "        embed_dim (int): Dimensionality of the learned embedding space.\n",
    "        num_heads (int): Number of attention heads in each Transformer layer.\n",
    "        num_layers (int): Number of Transformer encoder layers.\n",
    "        sequence_length (int): Length of the input sequences (time steps).\n",
    "        dropout (float, optional): Dropout probability to apply in the\n",
    "            Transformer encoder layers. Defaults to 0.1.\n",
    "    Attributes:\n",
    "        model_type (str): Identifier for the model type ('Transformer').\n",
    "        embedding (nn.Linear): Linear layer for input feature embedding.\n",
    "        positional_encoding (torch.nn.Parameter): Parameter storing the\n",
    "            positional encodings used to retain temporal information.\n",
    "        transformer_encoder (nn.TransformerEncoder): Stack of Transformer\n",
    "            encoder layers with optional final LayerNorm.\n",
    "        decoder (nn.Linear): Linear layer used to produce the final output\n",
    "            dimensions.\n",
    "    Forward Inputs:\n",
    "        src (torch.Tensor): Input tensor of shape (batch_size, sequence_length,\n",
    "            input_size).\n",
    "    Forward Returns:\n",
    "        torch.Tensor: Output tensor of shape (batch_size, embed_dim) from the\n",
    "            last time step.\n",
    "    Raises:\n",
    "        ValueError: If the model output contains NaN or Inf values, indicating\n",
    "            numerical instability.\n",
    "    \"\"\"\n",
    "    # input_size: Input features ·Ä°·Äõ·Ä±·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ (·Ä•·Äï·Äô·Ä¨ 10·Åä price + SMA/RSI indicators ·ÄÖ·Äê·Ä¨)·Åã\n",
    "    # embed_dim: Internal embedding ·Ä°·Äê·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏·Ä°·Äê·Ä¨ (·Ä•·Äï·Äô·Ä¨ 64·Åä data ·ÄÄ·Ä≠·ÄØ ·Äï·Ä≠·ÄØ·Äî·ÄÄ·Ä∫·Äõ·Äæ·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏ ·Ä°·Ä±·Ä¨·ÄÑ·Ä∫ ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏)·Åã\n",
    "    # num_heads: Attention heads ·Ä°·Äõ·Ä±·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ (multi-head attention ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫·Åä ·Äô·Äê·Ä∞·Ää·ÄÆ ·Ä°·Äî·Ä±·Äî·Ä≤·Ä∑ ·Ä°·Ä¨·Äõ·ÄØ·Ä∂ ·ÄÖ·Ä≠·ÄØ·ÄÄ·Ä∫)·Åã\n",
    "    # num_layers: Encoder layers ·Ä°·Äõ·Ä±·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ (·Ä•·Äï·Äô·Ä¨ 2·Åä ·Äõ·Ä≠·ÄØ·Ä∏·Äõ·Äæ·ÄÑ·Ä∫·Ä∏ ·Äë·Ä¨·Ä∏·Äê·Ä¨)·Åã\n",
    "    # sequence_length: Input sequence ·Ä°·Äõ·Äæ·Ää·Ä∫ (·Ä•·Äï·Äô·Ä¨ 20 timesteps)·Åã\n",
    "    # dropout=0.1: Overfitting ·ÄÄ·Äî·Ä± ·ÄÄ·Ä¨·ÄÄ·ÄΩ·Äö·Ä∫ ·Äê·Ä≤·Ä∑ dropout rate·Åã\n",
    "    def __init__(self, input_size, embed_dim, num_heads, num_layers,sequence_length, dropout=0.1):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Embedding layer to project input features to embed_dim dimensions\n",
    "        self.embedding = nn.Linear(input_size, embed_dim).to(device)\n",
    "\n",
    "        # Positional encoding parameter\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, sequence_length, embed_dim).to(device))\n",
    "\n",
    "        # Transformer encoder layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dropout=dropout,\n",
    "            norm_first=True  # Apply LayerNorm before attention and feedforward\n",
    "        ).to(device)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "            norm=nn.LayerNorm(embed_dim).to(device) # Add LayerNorm at the end of the encoder\n",
    "        )\n",
    "\n",
    "        # Decoder layer to produce final output\n",
    "        self.decoder = nn.Linear(embed_dim, embed_dim).to(device)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Apply embedding layer and add positional encoding\n",
    "        src = self.embedding(src) + self.positional_encoding\n",
    "\n",
    "        # Pass through the transformer encoder\n",
    "        output = self.transformer_encoder(src)\n",
    "\n",
    "        # Pass through the decoder layer\n",
    "        output = self.decoder(output)\n",
    "\n",
    "        # Check for NaN or Inf values for debugging\n",
    "        if torch.isnan(output).any() or torch.isinf(output).any():\n",
    "            logger.error(\"Transformer output contains NaN or Inf values\")\n",
    "            raise ValueError(\"Transformer output contains NaN or Inf values\")\n",
    "\n",
    "        # Return the output from the last time step\n",
    "        return output[:, -1, :]\n",
    "\n",
    "class CustomCombinedExtractor(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    A custom feature extractor that normalizes input observations and processes them\n",
    "    using a transformer-based architecture for dimensionality reduction and enhanced\n",
    "    feature representation.\n",
    "    Parameters:\n",
    "        observation_space (gym.spaces.Box): Defines the shape and limits of input data.\n",
    "        sequence_length (int): The length of the time series to be processed.\n",
    "    Attributes:\n",
    "        layernorm_before (nn.LayerNorm): Normalizes input data to improve training stability.\n",
    "        transformer (TimeSeriesTransformer): Processes normalized input sequences and extracts features.\n",
    "    Methods:\n",
    "        forward(observations):\n",
    "            Applies layer normalization to the incoming observations, then passes them\n",
    "            through the transformer. Raises a ValueError if invalid values (NaNs or inf)\n",
    "            are detected in the output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Box, sequence_length):\n",
    "        super(CustomCombinedExtractor, self).__init__(observation_space, features_dim=64)\n",
    "        num_features = observation_space.shape[1]  # Should be 10 in this case\n",
    "\n",
    "        # Ensure that embed_dim is divisible by num_heads\n",
    "        embed_dim = 64\n",
    "        num_heads = 2\n",
    "\n",
    "        self.layernorm_before = nn.LayerNorm(num_features) # Added Layer Normalization before transformer\n",
    "\n",
    "        self.transformer = TimeSeriesTransformer(\n",
    "            input_size=num_features,\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            num_layers=2,\n",
    "            sequence_length =sequence_length\n",
    "        )\n",
    "\n",
    "    def forward(self, observations):\n",
    "        # ·Äô·Ä∞·Äõ·ÄÑ·Ä∫·Ä∏ input tensor ·Äõ·Ä≤·Ä∑ device ·ÄÄ·Ä≠·ÄØ ·Äô·Äæ·Äê·Ä∫·Äû·Ä¨·Ä∏·Äë·Ä¨·Ä∏·Äï·Ä´\n",
    "        input_device = observations.device\n",
    "\n",
    "        # Apply layer normalization\n",
    "        # Apply layer normalization, ·Äù·ÄÑ·Ä∫·Äú·Ä¨·Äê·Ä≤·Ä∑ observations ·ÄÄ·Ä≠·ÄØ Transformer ·Äõ·Ä≤·Ä∑ device ·Äï·Ä±·Ä´·Ä∫·ÄÄ·Ä≠·ÄØ ·Äõ·ÄΩ·Äæ·Ä±·Ä∑·Äï·Ä´\n",
    "        normalized_observations = self.layernorm_before(observations.float().to(device)) # Ensure float type\n",
    "\n",
    "        x = self.transformer(normalized_observations)\n",
    "        if torch.isnan(x).any() or torch.isinf(x).any():\n",
    "            logger.error(\"Invalid values in transformer output\")\n",
    "            raise ValueError(\"Invalid values in transformer output\")\n",
    "\n",
    "        # ‚ö†Ô∏è ·Äï·Äº·ÄÑ·Ä∫·ÄÜ·ÄÑ·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫: Output tensor ·ÄÄ·Ä≠·ÄØ ·Äô·Ä∞·Äõ·ÄÑ·Ä∫·Ä∏ input tensor ·Äõ·Ä≤·Ä∑ device ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Äî·Ä∫·Äï·Ä≠·ÄØ·Ä∑·Äï·Ä´\n",
    "        # PPO Agent ·Äõ·Ä≤·Ä∑ Policy/Value Network ·ÄÄ ·Ä°·Äú·ÄØ·Äï·Ä∫·Äú·ÄØ·Äï·Ä∫·Äê·Ä≤·Ä∑ device ·Äï·Ä±·Ä´·Ä∫·ÄÄ·Ä≠·ÄØ ·Äï·Äº·Äî·Ä∫·Äï·Ä≠·ÄØ·Ä∑·Äñ·Ä≠·ÄØ·Ä∑ ·Äú·Ä≠·ÄØ·Äï·Ä´·Äê·Äö·Ä∫·Åã\n",
    "        # ·Äû·Ä≠·ÄØ·Ä∑·Äû·Ä±·Ä¨·Ä∫·Äú·Ää·Ä∫·Ä∏·Åä Stable-Baselines3 ·ÄÄ Policy/Value Network ·ÄÄ·Ä≠·ÄØ ·Äî·Ä±·Ä¨·ÄÄ·Ä∫·Äï·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏·Äô·Äæ·Ä¨ to(device) ·Äî·Ä≤·Ä∑ ·Äõ·ÄΩ·Äæ·Ä±·Ä∑·Äê·Ä≤·Ä∑·Ä°·Äê·ÄΩ·ÄÄ·Ä∫\n",
    "        # ·Äí·ÄÆ·Äî·Ä±·Äõ·Ä¨·Äô·Äæ·Ä¨ ·Ä°·Äî·Äπ·Äê·Äõ·Ä¨·Äö·Ä∫·ÄÄ·ÄÑ·Ä∫·Ä∏·Ä°·Ä±·Ä¨·ÄÑ·Ä∫ ·Äô·Ä∞·Äõ·ÄÑ·Ä∫·Ä∏ input device ·ÄÄ·Ä≠·ÄØ ·Äï·Äº·Äî·Ä∫·Äï·Ä≠·ÄØ·Ä∑·Äê·Ä¨ ·Äí·Ä´·Äô·Äæ·Äô·Äü·ÄØ·Äê·Ä∫ Agent ·Äû·ÄØ·Ä∂·Ä∏·Äô·Äö·Ä∑·Ä∫ device ·Äï·Ä±·Ä´·Ä∫·Äô·Äæ·Ä¨·Äï·Ä≤ ·Äë·Ä¨·Ä∏·Äê·Ä¨ ·Äî·Äæ·ÄÖ·Ä∫·Äô·Äª·Ä≠·ÄØ·Ä∏ ·Äú·ÄØ·Äï·Ä∫·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫·Åã\n",
    "        # ·Ä°·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÜ·ÄØ·Ä∂·Ä∏·ÄÄ·Äê·Ä±·Ä¨·Ä∑ Policy Network ·Äê·ÄΩ·Ä±·ÄÄ GPU ·Äï·Ä±·Ä´·Ä∫·Äô·Äæ·Ä¨·Äõ·Äæ·Ä≠·Äõ·ÄÑ·Ä∫ GPU ·Äô·Äæ·Ä¨·Äï·Ä≤ ·Äë·Ä¨·Ä∏·ÄÅ·Ä≤·Ä∑·Äê·Ä¨·Äï·Ä´·Åã\n",
    "\n",
    "        # ·Äû·Ä≠·ÄØ·Ä∑·Äû·Ä±·Ä¨·Ä∫·Äú·Ää·Ä∫·Ä∏·Åä SB3 ·Äõ·Ä≤·Ä∑ ·ÄÖ·Ä∂·Äî·Äæ·ÄØ·Äî·Ä∫·Ä∏·ÄÄ·Ä≠·ÄØ ·Äú·Ä≠·ÄØ·ÄÄ·Ä∫·Äî·Ä¨·Äñ·Ä≠·ÄØ·Ä∑·Åä CPU ·Äï·Ä±·Ä´·Ä∫·ÄÄ·Äú·Ä¨·Äõ·ÄÑ·Ä∫ CPU ·ÄÄ·Ä≠·ÄØ ·Äï·Äº·Äî·Ä∫·Äï·Ä≠·ÄØ·Ä∑·Äê·Ä¨ ·Äï·Ä≠·ÄØ·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Ä´·Äê·Äö·Ä∫·Åã\n",
    "        if str(input_device) == 'cpu':\n",
    "            return x.to(input_device)\n",
    "        else:\n",
    "             # Agent ·ÄÄ GPU ·Äô·Äæ·Ä¨ Run ·Äõ·ÄÑ·Ä∫·Äê·Ä±·Ä¨·Ä∑ GPU ·Äô·Äæ·Ä¨·Äï·Ä≤ ·Äë·Ä¨·Ä∏·ÄÅ·Ä≤·Ä∑·Äï·Ä´\n",
    "            return x\n",
    "\n",
    "class EnvConfig():\n",
    "    \"\"\"environment configuration from json file\n",
    "       tgym requires you configure your own parameters in json file.\n",
    "        Args:\n",
    "            config_file path/file.json\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,config_file):\n",
    "        self.config = {}\n",
    "        with open(config_file) as j:\n",
    "            self.config = json.load(j)\n",
    "\n",
    "    def env_parameters(self,item=''):\n",
    "        \"\"\"environment variables\n",
    "        \"\"\"\n",
    "        if item:\n",
    "            return self.config[\"env\"][item]\n",
    "        else:\n",
    "            return self.config[\"env\"]\n",
    "\n",
    "    def symbol(self, asset=\"GBPUSD\", item='') :\n",
    "        \"\"\"get trading pair (symbol) information\n",
    "\n",
    "        Args:\n",
    "            asset (str, optional): symbol in config. Defaults to \"GBPUSD\".\n",
    "            item (str, optional): name of item, if '' return dict, else return item value. Defaults to ''.\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        if item:\n",
    "            return self.config[\"symbol\"][asset][item]\n",
    "        else:\n",
    "            return self.config[\"symbol\"][asset]\n",
    "\n",
    "    def data_processing_parameters(self, item=''):\n",
    "        \"\"\"Get data processing config\"\"\"\n",
    "        if item:\n",
    "            return self.config[\"data_processing\"][item]\n",
    "        return self.config[\"data_processing\"]\n",
    "\n",
    "    def trading_hour(self,place=\"NewYork\"):\n",
    "        \"\"\"forex trading hour from different markets\n",
    "\n",
    "        Args:\n",
    "            place (str, optional): [Sydney,Tokyo,London] Defaults to \"New York\".\n",
    "\n",
    "        Returns:\n",
    "            [dict]: from time, to time\n",
    "        \"\"\"\n",
    "        if place:\n",
    "            return self.config[\"trading_hour\"][place]\n",
    "        else:\n",
    "            return self.config[\"trading_hour\"]\n",
    "\n",
    "    def indicator(self,place=\"sma_fast_period\"):\n",
    "        \"\"\"forex trading hour from different markets\n",
    "\n",
    "        Args:\n",
    "            place (str, optional): [Sydney,Tokyo,London] Defaults to \"New York\".\n",
    "\n",
    "        Returns:\n",
    "            [dict]: from time, to time\n",
    "        \"\"\"\n",
    "        if place:\n",
    "            return self.config[\"data_processing\"][\"indicator\"][place]\n",
    "        else:\n",
    "            return self.config[\"data_processing\"][\"indicator\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e20c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import numpy as np\n",
    "class TrainingMetricsCallback(BaseCallback):\n",
    "    def __init__(self, check_freq=1000, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.sharpe_ratios = []\n",
    "        self.drawdowns = []\n",
    "        self.episode_count = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Track metrics only when episodes complete\n",
    "        if \"sharpe\" in self.locals['infos'][0] and \"drawdown\" in self.locals['infos'][0]:\n",
    "            self.episode_count += 1\n",
    "            self.sharpe_ratios.append(self.locals['infos'][0]['sharpe'])\n",
    "            self.drawdowns.append(self.locals['infos'][0]['drawdown'])\n",
    "\n",
    "            # Log to tensorboard every N episodes\n",
    "            if self.episode_count % 10 == 0:\n",
    "                self.logger.record('train/mean_sharpe', np.mean(self.sharpe_ratios[-10:]))\n",
    "                self.logger.record('train/max_drawdown', np.mean(self.drawdowns[-10:]))\n",
    "                self.logger.record('train/episodes', self.episode_count)\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a4a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_to_file(**kwargs):\n",
    "    log_header                  =   kwargs.get(\"log_header\",False)\n",
    "    log_filename                =   kwargs.get(\"log_filename\",\"\")\n",
    "    printout                    =   kwargs.get(\"printout\",False)\n",
    "    balance                     =   kwargs.get(\"balance\")\n",
    "    balance_initial             =   kwargs.get(\"balance_initial\")\n",
    "    transaction_close_this_step =   kwargs.get(\"transaction_close_this_step\",[])\n",
    "    done_information            =   kwargs.get(\"done_information\",\"\")\n",
    "    profit                      =   balance - balance_initial\n",
    "\n",
    "    tr_lines                    =   \"\"\n",
    "    tr_lines_comma              =   \"\"\n",
    "    _header                     =   \"\"\n",
    "    _header_comma               =   \"\"\n",
    "    if log_header:\n",
    "        _header = f'{\"Ticket\":>8} {\"Type\":>4} {\"ActionStep\":16} \\\n",
    "                    {\"ActionPrice\":>12} {\"CloseStep\":8} {\"ClosePrice\":>12} \\\n",
    "                    {\"OpenBal\":>12} {\"CloseBal\":>12} {\"Status\":8} {\"Info\":>8} {\"PIPS\":>6} {\"SL\":>6} {\"PT\":>6} {\"DeltaStep\":8}\\n'\n",
    "\n",
    "\n",
    "        _header_comma = f'{\"Ticket,Type,ActionTime,ActionStep,ActionPrice,CloseTime,ClosePrice, OpenBal, CloseBal, Status, Info, PIPS,SL,PT,CloseStep,DeltaStep\"}\\n'\n",
    "    if transaction_close_this_step:\n",
    "        for _tr in transaction_close_this_step:\n",
    "            if _tr[\"CloseStep\"] >=0:\n",
    "                tr_lines += f'{_tr[\"Ticket\"]:>8} {_tr[\"Type\"]:>4} {_tr[\"ActionStep\"]:16} \\\n",
    "                    {_tr[\"ActionPrice\"]:.5f} {_tr[\"CloseStep\"]:8} {_tr[\"ClosePrice\"]:.5f} \\\n",
    "                    {_tr[\"OpenBal\"]:.2f} {_tr[\"CloseBal\"]:.2f} {_tr[\"Status\"]:8}  {_tr[\"Info\"]:>8}  {_tr[\"PIPS\"]:4.0f} {_tr[\"SL\"]:4.0f} {_tr[\"PT\"]:4.0f} {_tr[\"DeltaStep\"]:8}\\n'\n",
    "\n",
    "                tr_lines_comma += f'{_tr[\"Ticket\"]},{_tr[\"Type\"]},{_tr[\"ActionTime\"]},{_tr[\"ActionStep\"]}, \\\n",
    "                    {_tr[\"ActionPrice\"]},{_tr[\"CloseTime\"]},{_tr[\"ClosePrice\"]}, \\\n",
    "                    {_tr[\"OpenBal\"]},{_tr[\"CloseBal\"]}, {_tr[\"Status\"]},{_tr[\"Info\"]},{_tr[\"PIPS\"]},{_tr[\"SL\"]},{_tr[\"PT\"]},{_tr[\"CloseStep\"]},{_tr[\"DeltaStep\"]}\\n'\n",
    "\n",
    "    log = _header_comma + tr_lines_comma\n",
    "    # log = f\"Step: {current_step}   Balance: {balance}, Profit: {profit} \\\n",
    "    #     MDD: {max_draw_down_pct}\\n{tr_lines_comma}\\n\"\n",
    "    if done_information:\n",
    "        log += done_information\n",
    "    if log:\n",
    "        # os.makedirs(log_filename, exist_ok=True)\n",
    "        dir_path = os.path.dirname(log_filename)\n",
    "        if dir_path and not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "        with open(log_filename, 'a+') as _f:\n",
    "            _f.write(log)\n",
    "            _f.close()\n",
    "\n",
    "    tr_lines = _header + tr_lines\n",
    "    if printout and tr_lines:\n",
    "        print(tr_lines)\n",
    "        if done_information:\n",
    "            print(done_information)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9bce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForexTradingEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, file, cf, asset, logger_show=False, scaler=None):\n",
    "        # scaler parameter ·ÄÄ·Ä≠·ÄØ ·Äë·Äï·Ä∫·Äë·Ää·Ä∑·Ä∫·Äõ·Äï·Ä´·Äô·Ää·Ä∫·Åã ·Åé·ÄÑ·Ä∫·Ä∏·Äû·Ää·Ä∫ Global Train Set ·Äê·ÄΩ·ÄÑ·Ä∫ Fit ·Äú·ÄØ·Äï·Ä∫·Äë·Ä¨·Ä∏·Äû·Ä±·Ä¨ Scaler ·Äñ·Äº·ÄÖ·Ä∫·Äõ·Äô·Ää·Ä∫·Åã\n",
    "        # 'scaler' ·Äû·Ää·Ä∫ Global Train Set ·Äê·ÄΩ·ÄÑ·Ä∫ Fit ·Äú·ÄØ·Äï·Ä∫·Äë·Ä¨·Ä∏·Äû·Ä±·Ä¨ StandardScaler instance ·Äñ·Äº·ÄÖ·Ä∫·Äõ·Äô·Ää·Ä∫·Åã\n",
    "        self.scaler = scaler\n",
    "        if self.scaler is None:\n",
    "             raise ValueError(\"A fitted StandardScaler instance must be provided to the Environment.\")\n",
    "        super(ForexTradingEnv, self).__init__()\n",
    "        # ·ÄÄ·Ä≠·Äî·Ä∫·Ä∏·Äõ·Äæ·ÄÑ·Ä∫·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·ÄÖ·Äê·ÄÑ·Ä∫·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äû·Ää·Ä∫·Åã\n",
    "        self._initialize_parameters(file, cf, asset, logger_show)\n",
    "        # [NEW ACTION] Raw Data ·ÄÄ·Ä≠·ÄØ Scaler ·Äñ·Äº·ÄÑ·Ä∑·Ä∫ Transform ·Äú·ÄØ·Äï·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "        # OHLCV features ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ Scaling ·Äú·ÄØ·Äï·Ä∫·Äõ·Äï·Ä´·Äô·Äö·Ä∫ (·Ä•·Äï·Äô·Ä¨: open, high, low, close, vol, atr_base, log_returns, price_norm, etc.)\n",
    "        # Scaling ·Äú·ÄØ·Äï·Ä∫·Äõ·Äî·Ä∫ features ·Äô·Äª·Ä¨·Ä∏·ÄÖ·Ä¨·Äõ·ÄÑ·Ä∫·Ä∏·ÄÄ·Ä≠·ÄØ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äï·Ä´·Åã\n",
    "        # ·Ä§·Äî·Ä±·Äõ·Ä¨·Äê·ÄΩ·ÄÑ·Ä∫ ·ÄÄ·Äª·ÄΩ·Äî·Ä∫·Äê·Ä±·Ä¨·Ä∫·Äê·Ä≠·ÄØ·Ä∑·Äû·Ää·Ä∫ OHLCV ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ Indicator Features ·Ä°·Ä¨·Ä∏·Äú·ÄØ·Ä∂·Ä∏·ÄÄ·Ä≠·ÄØ Scaling ·Äú·ÄØ·Äï·Ä∫·Äô·Ää·Ä∫·Äü·ÄØ ·Äö·Ä∞·ÄÜ·Äï·Ä´·Äô·Ää·Ä∫·Åã\n",
    "        self._scale_data()\n",
    "\n",
    "        # Action ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ Observation Spaces ·ÄÄ·Ä≠·ÄØ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äû·Ää·Ä∫·Åã\n",
    "        self._initialize_spaces()\n",
    "        # Scaled Data ·ÄÄ·Ä≠·ÄØ Numpy Array ·Ä°·Äñ·Äº·ÄÖ·Ä∫ ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Äº·ÄÆ·Ä∏ ·ÄÖ·ÄÖ·Ä∫·Äï·Ä´·Åã\n",
    "        data_check = self.data[self.features_scaled].values\n",
    "\n",
    "        if np.isnan(data_check).any() or np.isinf(data_check).any():\n",
    "            # ·Ä§·Äî·Ä±·Äõ·Ä¨·Äê·ÄΩ·ÄÑ·Ä∫ Debugging ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ print ·Äë·ÄØ·Äê·Ä∫·Äï·Ä´\n",
    "            print(\"FATAL ERROR: NaN/Inf detected in scaled data!\")\n",
    "            # NaN/Inf ·Äõ·Äæ·Ä≠·Äû·Ä±·Ä¨ Column ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Äû·Ä≠·ÄÅ·Äª·ÄÑ·Ä∫·Äõ·ÄÑ·Ä∫:\n",
    "            # print(self.data[self.features_scaled].isnull().any())\n",
    "            raise ValueError(\"Invalid values detected in environment data (NaN/Inf)!\")\n",
    "\n",
    "\n",
    "        # Environment ·ÄÄ·Ä≠·ÄØ ·Ä°·ÄÖ·Äï·Äº·ÄØ·Ä°·ÄÅ·Äº·Ä±·Ä°·Äî·Ä±·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Äî·Ä∫·Äú·Ää·Ä∫·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äû·Ää·Ä∫·Åã\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def _scale_data(self):\n",
    "        \"\"\"Raw Data (self.data) ·Äô·Äæ features ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ Global Scaler ·Äñ·Äº·ÄÑ·Ä∑·Ä∫ Transform ·Äú·ÄØ·Äï·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\"\"\"\n",
    "        # [self.features] ·Äê·ÄΩ·ÄÑ·Ä∫ OHLCV ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ Indicator Features ·Ä°·Ä¨·Ä∏·Äú·ÄØ·Ä∂·Ä∏ ·Äï·Ä´·Äù·ÄÑ·Ä∫·Äõ·Äô·Ää·Ä∫·Åã\n",
    "        # [NOTE]: 'time' ·ÄÄ·Ä≤·Ä∑·Äû·Ä≠·ÄØ·Ä∑·Äû·Ä±·Ä¨ Non-Numeric features ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ self.features ·Äê·ÄΩ·ÄÑ·Ä∫ ·Äô·Äï·Ä´·Äù·ÄÑ·Ä∫·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏ ·Äû·Ä±·ÄÅ·Äª·Ä¨·Äï·Ä´·ÄÖ·Ä±·Åã\n",
    "        if not self.scaler.scale_.any():\n",
    "             logger.warning(\"Scaler is not properly fitted. Continuing with raw data.\")\n",
    "             return\n",
    "\n",
    "        # Scaled Features ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ Original DataFrame ·Äê·ÄΩ·ÄÑ·Ä∫ ·Ä°·ÄÖ·Ä¨·Ä∏·Äë·Ä≠·ÄØ·Ä∏·Äû·Ää·Ä∫·Åã\n",
    "        # Scaled Data ·Äê·ÄΩ·ÄÑ·Ä∫ NaN/Inf ·Äô·Äñ·Äº·ÄÖ·Ä∫·ÄÖ·Ä±·Äõ·Äî·Ä∫ Data Frame ·ÄÄ·Ä≠·ÄØ ·ÄÄ·Äº·Ä≠·ÄØ·Äê·ÄÑ·Ä∫·ÄÖ·ÄÖ·Ä∫·ÄÜ·Ä±·Ä∏·Äë·Ä¨·Ä∏·Äû·ÄÑ·Ä∑·Ä∫·Äï·Ä´·Äû·Ää·Ä∫·Åã\n",
    "        self.data[self.features_scaled] = self.scaler.transform(self.data[self.features_scaled])\n",
    "        # logger.info(f\"Data scaled successfully using fitted StandardScaler.\")\n",
    "\n",
    "    # ·ÄÄ·Ä≠·Äî·Ä∫·Ä∏·Äõ·Äæ·ÄÑ·Ä∫·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·ÄÖ·Äê·ÄÑ·Ä∫·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äû·Ää·Ä∫·Åã\n",
    "    def _initialize_parameters(self, file, cf, asset, logger_show):\n",
    "        # Params to variables\n",
    "        self.csv_file               =   file\n",
    "        self.cf                     =   cf\n",
    "        self.symbol_col             =   asset\n",
    "        self.features_scaled        =   self.cf.env_parameters('features_scaled') # Time-Series Features List\n",
    "        self.features_unscaled      =   self.cf.env_parameters('features_unscaled')\n",
    "        self.features_filter        =   self.cf.env_parameters('features_filter')\n",
    "        # Scaled Data Frame ·Åè Feature List\n",
    "        self.obs_features           =   self.features_scaled + self.features_unscaled\n",
    "        self.sequence_length        =   self.cf.data_processing_parameters(\"sequence_length\") # Transformer Lookback Window (100)\n",
    "        self.logger_show            =   logger_show\n",
    "\n",
    "        self.data_raw = pd.read_csv(file)\n",
    "        # if 'time' in self.data_raw.columns:\n",
    "        #     self.data_raw = self.data_raw.set_index(pd.to_datetime(self.data_raw['time'], utc=True)).drop(columns=['time'])\n",
    "        # Index ·ÄÄ·Ä≠·ÄØ datetime type ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏\n",
    "        self.data_raw.index = pd.to_datetime(self.data_raw.index)\n",
    "\n",
    "\n",
    "        # self.data ·ÄÄ·Ä≠·ÄØ Scaling ·Äú·ÄØ·Äï·Ä∫·Äõ·Äî·Ä∫ Copy ·Äö·Ä∞·Äï·Ä´·Äô·Ää·Ä∫·Åã\n",
    "        self.data = self.data_raw.copy()\n",
    "\n",
    "        # We use sequence transformer, so max steps will be this\n",
    "        self.max_steps              =   len(self.data) - self.sequence_length - 1\n",
    "\n",
    "        # Configs to variables\n",
    "        # Agent ·ÄÄ Action ·ÄÄ Continuous Action ·ÄÄ·Ä≠·ÄØ Discrete Action ·Äû·Ä≠·ÄØ·Ä∑·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Ä±·Ä∏·Äû·Ä±·Ä¨ threshold\n",
    "        self.action_threshold       =   self.cf.env_parameters('action_threshold')\n",
    "        self.balance_initial        =   self.cf.env_parameters('balance')\n",
    "\n",
    "        # position close ·Äô·Äñ·Äº·ÄÖ·Ä∫·Äû·Ä±·Ä∏·Äõ·ÄÑ·Ä∫\n",
    "        # buy ·Äë·Ä¨·Ä∏·Äï·Äº·ÄÆ·Ä∏ price up ·Äñ·Äº·ÄÖ·Ä∫·Äî·Ä±·Äõ·ÄÑ·Ä∫ reward ·Äï·Ä±·Ä∏·Åã sell ·Äë·Ä¨·Ä∏·Äï·Äº·ÄÆ·Ä∏ price down ·Äñ·Äº·ÄÖ·Ä∫·Äî·Ä±·Äõ·ÄÑ·Ä∫ reward ·Äï·Ä±·Ä∏\n",
    "        # position management ·Äô·Äæ·Ä¨·Äú·Ää·Ä∫·Ä∏ ·Äû·ÄØ·Ä∂·Ä∏·Åã\n",
    "        # buy ·Äô·Äæ·Ä¨ ·Äô·Äº·Äê·Ä∫·Äî·Ä±·Äõ·ÄÑ·Ä∫ tp ·Ä°·Äï·Ä±·Ä´·Ä∫·Äõ·ÄΩ·Ä±·Ä∑ sl ·Ä°·Äï·Ä±·Ä´·Ä∫·Äõ·ÄΩ·Ä±·Ä∑·Åã  ·Äõ·Äæ·ÄØ·Ä∂·Ä∏·Äî·Ä±·Äõ·ÄÑ·Ä∫ tp ·Ä°·Ä±·Ä¨·ÄÄ·Ä∫·Äõ·ÄΩ·Ä±·Ä∑ sl ·Ä°·Äï·Ä±·Ä´·Ä∫·Äê·ÄÑ·Ä∫,\n",
    "        # sell ·Äô·Äæ·Ä¨ ·Äô·Äº·Äê·Ä∫·Äî·Ä±·Äõ·ÄÑ·Ä∫ tp ·Ä°·Ä±·Ä¨·ÄÄ·Ä∫·Äõ·ÄΩ·Ä±·Ä∑ sl ·Ä°·Ä±·Ä¨·ÄÄ·Ä∫·Äõ·ÄΩ·Ä±·Ä∑·Åã ·Äõ·Äæ·ÄØ·Ä∂·Ä∏·Äî·Ä±·Äõ·ÄÑ·Ä∫ tp ·Ä°·Äï·Ä±·Ä´·Ä∫·Äê·ÄÑ·Ä∫ sl ·Ä°·Ä±·Ä¨·ÄÄ·Ä∫·ÄÅ·Äª\n",
    "        self.good_position_reward_scale = self.cf.env_parameters(\"good_position_reward_scale\") # ·Ä•·Äï·Äô·Ä¨: 0.01\n",
    "        # ·Äõ·Ää·Ä∫·Äõ·ÄΩ·Äö·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫ ·ÅÇ: SL/PT Trailing ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Äê·Äî·Ä∫·Äñ·Ä≠·ÄØ·Ä∏ (Move Step Size)\n",
    "        self.trailing_distance = self.cf.env_parameters(\"trailing_stop_distance_points\")\n",
    "\n",
    "        # ·Ä°·Äõ·Äæ·ÄØ·Ä∂·Ä∏·Äî·Ä≤·Ä∑·Ä°·Äô·Äº·Äê·Ä∫ ·Äô·Äª·Äæ·Äê·Äô·Äæ·ÄØ·Äõ·Äæ·Ä≠·Äê·Ä≤·Ä∑ trading performance ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Äï·Ä±·Ä∏·Äê·Ä≤·Ä∑ bonus reward 0.01\n",
    "        # self.consistency_reward = self.cf.env_parameters(\"consistency_reward\")\n",
    "        self.stop_loss = self.cf.symbol(self.symbol_col, \"stop_loss_max\")\n",
    "        self.profit_taken = self.cf.symbol(self.symbol_col, \"profit_taken_max\")\n",
    "        self.point = self.cf.symbol(self.symbol_col, \"point\")\n",
    "        self.transaction_fee = self.cf.symbol(self.symbol_col, \"transaction_fee\")\n",
    "        self.over_night_penalty = self.cf.symbol(self.symbol_col, \"over_night_penalty\")\n",
    "        self.max_current_holding = self.cf.symbol(self.symbol_col, \"max_current_holding\")\n",
    "        # Drawdown Penalty Factor\n",
    "        self.drawdown_penalty_factor = self.cf.env_parameters(\"drawdown_penalty_factor\")\n",
    "        self.margin_requirement = self.cf.env_parameters('margin_requirement')\n",
    "\n",
    "\n",
    "    # Action ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ Observation Spaces ·ÄÄ·Ä≠·ÄØ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äû·Ää·Ä∫·Åã\n",
    "    def _initialize_spaces(self):\n",
    "        # Continuous actions: [1 -> 0.5] LONG | [0.5 -> -0.5] HOLD |[-0.5 -> -1] SHORT\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1,\n",
    "            high=1,\n",
    "            shape=(1,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # [MODIFIED] Transformer Observation Space: Time Series (100) + Context (4)\n",
    "        N_FEATURES_TS = len(self.obs_features)\n",
    "        N_FEATURES_CONTEXT = 4 # [Equity, Drawdown, Open_Pos_Ratio, Time_Context]\n",
    "\n",
    "        # [MODIFIED] Observation Space (Time Series Sequence Only)\n",
    "        # Transformer ·Äû·ÄØ·Ä∂·Ä∏·Äë·Ä¨·Ä∏·Äû·Ä±·Ä¨ features ·Äê·ÄΩ·Ä±·Äõ·Ä≤·Ä∑ previous sequence length candle ·ÄÄ·Ä≠·ÄØ·Äï·Ä´ ·Äê·Äï·Äº·Ä≠·ÄØ·ÄÑ·Ä∫·Äê·Ää·Ä∫·Ä∏·ÄÄ·Äº·Ää·Ä∑·Ä∫\n",
    "        obs_shape = (self.sequence_length, N_FEATURES_TS + N_FEATURES_CONTEXT)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf, # Scaled Data ·Äô·Äª·Ä¨·Ä∏·Äû·Ää·Ä∫ Theoretical Inf/ -Inf ·Äõ·Äæ·Ä≠·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äû·Ä±·Ä¨·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∑·Ä∫ np.inf ·ÄÄ·Ä≠·ÄØ ·Äû·ÄØ·Ä∂·Ä∏·Äï·Ä´\n",
    "            shape=obs_shape,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    # Environment ·ÄÄ·Ä≠·ÄØ ·Ä°·ÄÖ·Äï·Äº·ÄØ·Ä°·ÄÅ·Äº·Ä±·Ä°·Äî·Ä±·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Äî·Ä∫·Äú·Ää·Ä∫·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äû·Ää·Ä∫·Åã\n",
    "    def reset(self, *, seed = None, options = None):\n",
    "        super().reset(seed=seed, options=options)\n",
    "\n",
    "        self.ticket_id          =   0\n",
    "        self.ttl_rewards        =   0 # total rewards\n",
    "\n",
    "        self.balance            =   self.balance_initial\n",
    "        self.positions          =   []\n",
    "\n",
    "        # equity tracking\n",
    "        self.equity_curve       =   [self.balance_initial] # Starting with initial balance\n",
    "        # ·Ä°·Äô·Äº·ÄÑ·Ä∑·Ä∫·ÄÜ·ÄØ·Ä∂·Ä∏·Äõ·Ä±·Ä¨·ÄÄ·Ä∫·Äñ·Ä∞·Ä∏·Äê·Ä≤·Ä∑ eq value\n",
    "        self.peak_equity        =   self.balance_initial # Start with initial balance as peak\n",
    "\n",
    "        self.max_drawdown       =   0.0\n",
    "        self.current_drawdown   =   0.0\n",
    "\n",
    "        # transformer ·Ä°·Äû·ÄØ·Ä∂·Ä∏·Äï·Äº·ÄØ·Äë·Ä¨·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∑\n",
    "        self.current_step       =   self.sequence_length\n",
    "        logger.info(f\"--- Environment reset. Starting at step {self.current_step} --total rewards: {self.ttl_rewards}\")\n",
    "\n",
    "        observation             =   self._next_observation()\n",
    "        info                    =   {}\n",
    "        return  observation, info\n",
    "\n",
    "\n",
    "# AI model ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Äú·ÄÄ·Ä∫·Äõ·Äæ·Ä≠ market condition ·ÄÄ·Ä≠·ÄØ·ÄÄ·Ä≠·ÄØ·Äö·Ä∫·ÄÖ·Ä¨·Ä∏·Äï·Äº·ÄØ·Äê·Ä≤·Ä∑ observation data ·ÄÄ·Ä≠·ÄØ ·Äï·Äº·ÄÑ·Ä∫·ÄÜ·ÄÑ·Ä∫·Äï·Ä±·Ä∏·Äñ·Ä≠·ÄØ·Ä∑·Äñ·Äº·ÄÖ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫·Åã\n",
    "    def _next_observation(self):\n",
    "\n",
    "        # 1. Time Series Observation (Scaled Data)\n",
    "        obs_ts = self.data.iloc[\n",
    "            self.current_step - self.sequence_length: self.current_step\n",
    "        ][self.obs_features].values # Shape: (100, N_Features_TS)\n",
    "\n",
    "        # 2. Account State (Non-Time-Series / Context Vector)\n",
    "        current_equity = self._calculate_current_equity()\n",
    "        open_positions_count = sum(1 for p in self.positions if p['Status'] == 0)\n",
    "\n",
    "        obs_context = np.array([\n",
    "            current_equity / self.balance_initial, # 1. Normalized Equity\n",
    "            self.current_drawdown,                 # 2. Current Drawdown (Percentage)\n",
    "            open_positions_count / self.max_current_holding, # 3. Open Positions Ratio\n",
    "            self.data.iloc[self.current_step]['hour_cos']   # 4. Time Context (Scaled)\n",
    "        ], dtype=np.float32) # Shape: (4,)\n",
    "\n",
    "        # 3. Final Observation Construction (Time Series + Context)\n",
    "\n",
    "        # Context features ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ Sequence Length (100) ·Ä°·Äú·Ä≠·ÄØ·ÄÄ·Ä∫ ·Äñ·Äº·Äî·Ä∑·Ä∫·ÄÄ·Äª·ÄÄ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏ (Broadcasting)\n",
    "        obs_context_expanded = np.tile(obs_context, (self.sequence_length, 1)) # Shape: (100, 4)\n",
    "\n",
    "        # Horizontal Stack (Sequence length, N_Features_TS + N_Features_Context)\n",
    "        obs_final = np.hstack([obs_ts, obs_context_expanded])\n",
    "\n",
    "        # Data Validation ·ÄÖ·ÄÖ·Ä∫·ÄÜ·Ä±·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "        # NaN (Not a Number) values ·Äõ·Äæ·Ä≠·Äô·Äõ·Äæ·Ä≠·ÄÖ·ÄÖ·Ä∫·Äô·Äö·Ä∫\n",
    "        # Infinite values ·Äõ·Äæ·Ä≠·Äô·Äõ·Äæ·Ä≠·ÄÖ·ÄÖ·Ä∫·Äô·Äö·Ä∫\n",
    "        # Invalid data ·Äõ·Äæ·Ä≠·Äõ·ÄÑ·Ä∫ error ·Äï·Äº·Äô·Äö·Ä∫\n",
    "        if np.isnan(obs_final).any() or np.isinf(obs_final).any():\n",
    "            logger.error(f\"Invalid observation at step {self.current_step}\")\n",
    "            raise ValueError(f\"Invalid observation at step {self.current_step}\")\n",
    "\n",
    "\n",
    "        # NumPy Array ·Äï·Äº·Äî·Ä∫·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "        # GPU memory ‚Üí CPU memory ·Äï·Äº·Äî·Ä∫·Äõ·ÄΩ·Äæ·Ä±·Ä∑·Äô·Äö·Ä∫\n",
    "        # PyTorch tensor ‚Üí NumPy array ·Äï·Äº·Äî·Ä∫·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äô·Äö·Ä∫\n",
    "        # Gym environment ·ÄÄ NumPy arrays ·ÄÄ·Ä≠·ÄØ ·Äï·Ä≠·ÄØ·ÄÄ·Äº·Ä≠·ÄØ·ÄÄ·Ä∫·Äê·Äö·Ä∫·Åã Memory management ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äê·Äö·Ä∫\n",
    "        return obs_final\n",
    "\n",
    "\n",
    "        # # 4. PyTorch/Device Conversion and Validation\n",
    "        # try:\n",
    "        #     # NumPy array ‚Üí PyTorch tensor ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äô·Äö·Ä∫\n",
    "        #     obs_tensor = torch.tensor(obs_final, dtype=torch.float32).to(device)\n",
    "        #     # Data Validation ·ÄÖ·ÄÖ·Ä∫·ÄÜ·Ä±·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "        #     if torch.isnan(obs_tensor).any() or torch.isinf(obs_tensor).any():\n",
    "        #         logger.error(f\"Invalid observation (NaN/Inf) at step {self.current_step}\")\n",
    "        #         raise ValueError(f\"Invalid observation (NaN/Inf) at step {self.current_step}\")\n",
    "        #     return obs_tensor.cpu().numpy()\n",
    "\n",
    "        # except NameError:\n",
    "        #     # Torch ·ÄÄ·Ä≠·ÄØ ·Äô·Äû·ÄØ·Ä∂·Ä∏·Äï·Ä´·ÄÄ NumPy ·ÄÄ·Ä≠·ÄØ·Äû·Ä¨ ·Äï·Äº·Äî·Ä∫·Äï·Ä±·Ä∏·Äï·Ä´·Åã\n",
    "        #     if np.isnan(obs_final).any() or np.isinf(obs_final).any():\n",
    "        #         logger.error(f\"Invalid observation (NaN/Inf) at step {self.current_step}\")\n",
    "        #         raise ValueError(f\"Invalid observation (NaN/Inf) at step {self.current_step}\")\n",
    "        #     return obs_final # Final NumPy array\n",
    "\n",
    "\n",
    "    def _ray_mask(self, a, c, bounds):\n",
    "        \"\"\"\n",
    "        Ray Mask ·Äú·ÄØ·Äï·Ä∫·Äï·Ä´·Åã\n",
    "        a: ·Äô·Ä∞·Äõ·ÄÑ·Ä∫·Ä∏ action (np.array)\n",
    "        c: ·Äû·ÄÄ·Ä∫·ÄÜ·Ä≠·ÄØ·ÄÑ·Ä∫·Äõ·Ä¨ ·Ä°·ÄÖ·ÄØ ·Äõ·Ä≤·Ä∑ ·Ä°·Äú·Äö·Ä∫·Äó·Äü·Ä≠·ÄØ\n",
    "        A_r_boundary_func: lambda_A_r ·Äê·ÄΩ·ÄÄ·Ä∫·Äê·Ä≤·Ä∑ func\n",
    "        A_boundary_func: lambda_A ·Äê·ÄΩ·ÄÄ·Ä∫·Äê·Ä≤·Ä∑ func\n",
    "        \"\"\"\n",
    "        if np.allclose(a, c):\n",
    "            return c  # ·Ä°·Äú·Äö·Ä∫·Äô·Äæ·Ä¨ ·ÄÜ·Ä≠·ÄØ ·Äô·Äõ·ÄΩ·Äæ·Ä±·Ä∑\n",
    "\n",
    "        direction = a - c\n",
    "        norm_dir = direction / np.linalg.norm(direction)\n",
    "\n",
    "        lambda_A_r = bounds[1] - c if norm_dir > 0 else c - bounds[0]\n",
    "        lambda_A = 1 - c if norm_dir > 0 else c - (-1)\n",
    "\n",
    "\n",
    "        if lambda_A_r <= 0 or lambda_A <= 0:\n",
    "            return c  # ·Ä°·Äô·Äæ·Ä¨·Ä∏ ·Äõ·Äæ·Ä±·Ä¨·ÄÑ·Ä∫\n",
    "\n",
    "        scale = lambda_A_r / lambda_A\n",
    "        a_r = c + scale * direction\n",
    "        return np.clip(a_r, -1, 1)  # Action space ·ÄÄ·Äî·Ä∫·Ä∑·Äû·Äê·Ä∫\n",
    "\n",
    "\n",
    "    def _get_action_name(self, _action, ma_first, ma_slow):\n",
    "\n",
    "        c = 0.0  # ·Ä°·Äú·Äö·Ä∫·Äó·Äü·Ä≠·ÄØ (hold)\n",
    "        if ma_first > ma_slow:  # Uptrend: buy only [0, 1]\n",
    "            bounds = [0, 1]\n",
    "        else:  # Downtrend: sell only [-1, 0]\n",
    "            bounds = [-1, 0]\n",
    "\n",
    "        a_masked = self._ray_mask(_action, c, bounds)\n",
    "\n",
    "        \"\"\"Convert continuous action to discrete action name\"\"\"\n",
    "        if a_masked >= self.action_threshold:\n",
    "            return \"BUY\"\n",
    "        elif a_masked <= -self.action_threshold:\n",
    "            return \"SELL\"\n",
    "        else:\n",
    "            return \"HOLD\"\n",
    "\n",
    "    def step(self, action):\n",
    "        # self.data ·Äû·Ää·Ä∫ Index ·Äê·ÄΩ·ÄÑ·Ä∫ 'time' ·ÄÄ·Ä≠·ÄØ ·Äë·Ä¨·Ä∏·Äõ·Äæ·Ä≠·Äï·Äº·ÄÆ·Ä∏ drop ·Äú·ÄØ·Äï·Ä∫·Äë·Ä¨·Ä∏·Äû·Ä±·Ä¨·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∑·Ä∫·Åä Index ·Äô·Äæ time ·ÄÄ·Ä≠·ÄØ ·Äö·Ä∞·Äõ·Äî·Ä∫·Äú·Ä≠·ÄØ·Äû·Ää·Ä∫·Åã\n",
    "        current_row_raw = self.data_raw.iloc[self.current_step]\n",
    "\n",
    "        # Unscaled Price Features\n",
    "        _o, _h, _l, _c, ma_fast, ma_slow = current_row_raw[['open', 'high', 'low', 'close', 'ma_fast', 'ma_slow']]\n",
    "\n",
    "        _t = self.data.index[self.current_step] # Get time from index\n",
    "        reward                      =   0 # ·Äí·ÄÆ step ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·ÄÖ·ÄØ·ÄÖ·ÄØ·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏ reward\n",
    "        position_reward             =   0 # Position ·Äï·Ä≠·Äê·Ä∫·Äõ·ÄÑ·Ä∫ ·Äõ·Äê·Ä≤·Ä∑ reward\n",
    "        action_hold_reward          =   0 # Hold action ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ reward/penalty\n",
    "\n",
    "        _msg                        =   []\n",
    "        _action                     =   action[0] # action value eg. [0.75]\n",
    "        open_position               =   0\n",
    "        for position in self.positions:\n",
    "            if position['Status']   ==  0:\n",
    "                position_reward, closed, _msg   =   self._calculate_reward(position)\n",
    "                if not closed: open_position += 1  # Count what we already knew\n",
    "                reward += position_reward\n",
    "\n",
    "        # Continuous actions: [1 -> 0.5] LONG | [0.5 -> -0.5] HOLD |[-0.5 -> -1] SHORT\n",
    "        action_name = self._get_action_name(action, ma_fast, ma_slow)\n",
    "\n",
    "        if open_position < self.max_current_holding and action_name in ['BUY', 'SELL']:\n",
    "            self.ticket_id  +=  1\n",
    "\n",
    "            # Real trading ·Äô·Äæ·Ä¨ margin requirement ·Äõ·Äæ·Ä≠·Äû·Äú·Ä≠·ÄØ·Äô·Äª·Ä≠·ÄØ·Ä∏\n",
    "            # Position ·Äñ·ÄΩ·ÄÑ·Ä∑·Ä∫·Äõ·ÄÑ·Ä∫ capital ·ÄÅ·Äª·ÄØ·Äï·Ä∫·ÄÑ·Äº·Ä¨·Ä∏·Äî·Ä±·Äõ·Äê·Äö·Ä∫\n",
    "            # Position ·Äï·Ä≠·Äê·Ä∫·Äê·Ä≤·Ä∑·Ä°·ÄÅ·Ä´ ·Äï·Äº·Äî·Ä∫·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏·Äë·Ää·Ä∑·Ä∫·Äï·Ä±·Ä∏·Äê·Äö·Ä∫\n",
    "            self.balance -= self.margin_requirement # hold up, this will make sure model can not open a lot of\n",
    "\n",
    "            position        =   {\n",
    "                \"Ticket\"        :   self.ticket_id,\n",
    "                \"Symbol\"        :   self.symbol_col,\n",
    "                \"ActionTime\"    :   _t,\n",
    "                \"Type\"          :   action_name,\n",
    "                \"Lot\"           :   1,\n",
    "                \"ActionPrice\"   :   _c,\n",
    "                \"SL\"            :   self.stop_loss,\n",
    "                \"PT\"            :   self.profit_taken,\n",
    "                \"MaxDD\"         :   0,\n",
    "                \"Swap\"          :   0.0,\n",
    "                \"CloseTime\"     :   \"\",\n",
    "                \"ClosePrice\"    :   0.0,\n",
    "                \"Point\"         :   self.point,\n",
    "                \"Reward\"        :   self.transaction_fee,\n",
    "                \"DateDuration\"  :   _t.date().isoformat(),\n",
    "                \"Status\"        :   0, # 0 is Position is currently OPEN and active\n",
    "                #\"PIPS\"          :   self.transaction_fee, # Price Interest Point (profit/loss ·ÄÄ·Ä≠·ÄØ measure ·Äú·ÄØ·Äï·Ä∫·Äê·Ä≤·Ä∑ unit)\n",
    "                \"PIPS\"          :   0,\n",
    "                \"ActionStep\"    :   self.current_step,\n",
    "                \"CloseStep\"     :   -1, # Step number when position closed, not close yet is -1\n",
    "                \"DeltaStep\"     :   0,\n",
    "                \"OpenBal\"       :   self.balance,\n",
    "                \"CloseBal\"       :   0,\n",
    "                \"HighestPrice\"  :   _c,\n",
    "                \"LowestPrice\"   :   _c,\n",
    "            }\n",
    "\n",
    "            self.positions.append(position)\n",
    "            # do not use transaction_fee penalty\n",
    "            # reward = self.transaction_fee #open cost\n",
    "            # model ·ÄÄ ·Ä°·Äú·ÄΩ·Äî·Ä∫·Ä°·ÄÄ·Äª·ÄΩ·Ä∂ position ·Äê·ÄΩ·Ä± ·Äô·Äñ·ÄΩ·ÄÑ·Ä∑·Ä∫·Äô·Ä≠·Ä°·Ä±·Ä¨·ÄÑ·Ä∫ ·Äë·Ä≠·Äî·Ä∫·Ä∏·ÄÅ·Äª·ÄØ·Äï·Ä∫·Äê·Ä≤·Ä∑ mechanism ·Äñ·Äº·ÄÖ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫·Åã\n",
    "            _msg.append(f'Step:{self.current_step} Tkt:{position[\"Ticket\"]} {position[\"Type\"]} Rwd:{position[\"PIPS\"]} SL:{position[\"SL\"]} PT:{position[\"PT\"]}')\n",
    "\n",
    "        # HOLD Penalty ·ÄÄ·Ä≠·ÄØ ·Ä°·Äú·ÄΩ·Äî·Ä∫·Äû·Ä±·Ä∏·ÄÑ·Äö·Ä∫·Äû·Ä±·Ä¨ ·Äê·Äî·Ä∫·Äñ·Ä≠·ÄØ·Ä∏\n",
    "        # (·Ä•·Äï·Äô·Ä¨: -0.0001) ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Ä´·Åã ·Ä°·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÜ·ÄØ·Ä∂·Ä∏·Äô·Äæ·Ä¨\n",
    "        # Trading ·Äô·Äú·ÄØ·Äï·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ Penalty ·Äô·Äï·Ä±·Ä∏·Äò·Ä≤ action_hold_reward = 0 ·Äë·Ä¨·Ä∏·Äï·Ä´·Åã\n",
    "        elif open_position < self.max_current_holding and action_name == \"HOLD\":\n",
    "            action_hold_reward  =   0  # no open any position, encourage open position\n",
    "        else:\n",
    "            action_hold_reward  =   0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        reward              +=  action_hold_reward\n",
    "\n",
    "        # Move to the next time step\n",
    "        self.current_step   +=  1\n",
    "\n",
    "        # check if episode is done\n",
    "        terminated          =   (self.balance <= 0)\n",
    "        truncated           =   (self.current_step > self.max_steps)\n",
    "\n",
    "        # get next observation\n",
    "        obs                 =   self._next_observation()\n",
    "        _msg.append(f'---idle----step:{self.current_step}, RF:{action_name} Action:{_action} Balance: {self.balance} reward:{reward} total_rewards:{self.ttl_rewards} position_reward:{position_reward} action_hold_reward:{action_hold_reward}')\n",
    "\n",
    "\n",
    "        current_equity = self._calculate_current_equity()\n",
    "        self.equity_curve.append(current_equity)\n",
    "        self._calculate_drawdown()  # This updates peak_equity and drawdowns\n",
    "\n",
    "        # =========================================================================\n",
    "        # START: Drawdown Penalty Logic\n",
    "        # =========================================================================\n",
    "        # self.current_drawdown ·Äû·Ää·Ä∫ Percentage (0.0 ·Äô·Äæ 1.0) ·Äñ·Äº·ÄÖ·Ä∫·Äû·Ää·Ä∫·Åã\n",
    "\n",
    "\n",
    "        drawdown_penalty = self.current_drawdown * self.drawdown_penalty_factor\n",
    "        # Reward ·Äê·ÄΩ·ÄÑ·Ä∫ ·Äî·ÄØ·Äê·Ä∫·Äï·Ä±·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "        reward -= drawdown_penalty\n",
    "\n",
    "        # Log the penalty for debugging\n",
    "        _msg.append(f'Drawdown Penalty: -{drawdown_penalty:.4f} (DD:{self.current_drawdown:.4f})')\n",
    "        # =========================================================================\n",
    "        # END: Drawdown Penalty Logic\n",
    "        # =========================================================================\n",
    "        # Drawdown Penalty ·Äî·ÄØ·Äê·Ä∫·Äï·Äº·ÄÆ·Ä∏·Äô·Äæ·Äû·Ä¨ ·ÄÖ·ÄØ·ÄÖ·ÄØ·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏ Reward ·ÄÄ·Ä≠·ÄØ ·Ä°·Äï·Ä∫·Äí·Ä≠·Äê·Ä∫·Äú·ÄØ·Äï·Ä∫·Äï·Ä´\n",
    "        self.ttl_rewards += reward  # <--- ·Ä§·Äî·Ä±·Äõ·Ä¨·Äê·ÄΩ·ÄÑ·Ä∫ ·Äï·Äº·Äî·Ä∫·Äë·Ää·Ä∑·Ä∫·Äï·Ä´\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        if terminated or truncated:\n",
    "            buy_positions = [p for p in self.positions if p[\"Type\"] == \"BUY\"]\n",
    "            sell_positions = [p for p in self.positions if p[\"Type\"] == \"SELL\"]\n",
    "\n",
    "            buy_count = len(buy_positions)\n",
    "            sell_count = len(sell_positions)\n",
    "            total_positions = len(self.positions)\n",
    "\n",
    "            # Calculate win rates\n",
    "            buy_wins = len([p for p in buy_positions if p[\"PIPS\"] > 0])\n",
    "            sell_wins = len([p for p in sell_positions if p[\"PIPS\"] > 0])\n",
    "\n",
    "            buy_win_rate = buy_wins / buy_count if buy_count > 0 else 0\n",
    "            sell_win_rate = sell_wins / sell_count if sell_count > 0 else 0\n",
    "\n",
    "            _m = f'--- Positions: {total_positions} (Buy:{buy_count}, Sell:{sell_count}) | '\n",
    "            _m += f'WinRates: Buy:{buy_win_rate:.1%}, Sell:{sell_win_rate:.1%} | '\n",
    "            _m += f'TotalRewards: {self.ttl_rewards} Balance: {self.balance}'\n",
    "\n",
    "            logger.info(_m)\n",
    "            _msg.append(_m)\n",
    "\n",
    "            # Additional info\n",
    "            if self.logger_show:\n",
    "                for _m in _msg:\n",
    "                    logger.info(_m)\n",
    "\n",
    "            info[\"info\"]                = _msg\n",
    "            info[\"sharpe\"]              = self._calculate_sharpe()  # ‚úÖ Now works! üí° 'sharpe_ratio' ·Äô·Äæ 'sharpe' ·Äû·Ä≠·ÄØ·Ä∑·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Ä´·Åã\n",
    "            info[\"drawdown\"]            = self.max_drawdown         # ‚úÖ Now accurate!'max_drawdown' ·Äô·Äæ 'drawdown' ·Äû·Ä≠·ÄØ·Ä∑·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Ä´·Åã\n",
    "            info[\"current_equity\"]      = current_equity            # ‚úÖ For debugging\n",
    "            info[\"peak_equity\"]         = self.peak_equity          # ‚úÖ For debugging\n",
    "            info[\"equity_curve_length\"] = len(self.equity_curve)    # ‚úÖ Monitor growth\n",
    "            info[\"episode\"]             = {\n",
    "                \"r\": reward,\n",
    "                \"l\": self.current_step\n",
    "            }\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _calculate_reward(self, position):\n",
    "        _o, _h, _l, _c              = self.data_raw.iloc[self.current_step][['open', 'high', 'low', 'close']]\n",
    "        _t                          = self.data.index[self.current_step]\n",
    "        _msg                        =   []\n",
    "\n",
    "        entry_price                 =   position['ActionPrice']\n",
    "        direction                   =   position['Type']\n",
    "        profit_target_price         =   entry_price + position['PT']/ self.point if direction == 'BUY' else entry_price - position['PT']/self.point\n",
    "        stop_loss_price             =   entry_price + position['SL']/ self.point if direction == 'BUY' else entry_price - position['SL']/self.point\n",
    "        closed                      =   False\n",
    "        close_position_reward       =   0.0\n",
    "        good_position_reward        =   0.0\n",
    "\n",
    "        # Check for stoploss hit\n",
    "        if (direction == 'BUY' and _l <= stop_loss_price) or (direction == 'SELL' and _h >= stop_loss_price):\n",
    "            close_position_reward   =   position['SL'] # position sl ·ÄÄ minus value ·Äñ·Äº·ÄÖ·Ä∫·Äê·Äö·Ä∫\n",
    "\n",
    "            position['CloseTime']   =   _t\n",
    "            position['ClosePrice']  =   stop_loss_price\n",
    "            position['Status']      =   1   # Status ·ÄÄ open ·ÄÜ·Ä≠·ÄØ 0 close ·ÄÜ·Ä≠·ÄØ 1\n",
    "            position['CloseStep']   =   self.current_step\n",
    "            position['PIPS']        =   close_position_reward - self.transaction_fee\n",
    "            position['DeltaStep']   =   self.current_step - position['ActionStep']\n",
    "            position['Info']        =   f'{profit_target_price:.5f} | {stop_loss_price:.5f}'\n",
    "\n",
    "            self.balance            +=  self.margin_requirement + position['PIPS'] # return 100 is margin hold\n",
    "            position['CloseBal']    =   self.balance\n",
    "            closed                  =   True\n",
    "            _msg.append(f'Step:{self.current_step} Tkt:{position[\"Ticket\"]}: Rwd:{position[\"PIPS\"]}, SL:{position[\"SL\"]}, DeltaStep:{position[\"DeltaStep\"]}')\n",
    "\n",
    "        elif (direction == 'BUY' and _h >= profit_target_price) or (direction == 'SELL' and _l <= profit_target_price):\n",
    "            close_position_reward   =    position['PT'] # position tp ·ÄÄ plus value ·Äñ·Äº·ÄÖ·Ä∫·Äê·Äö·Ä∫\n",
    "\n",
    "            position['CloseTime']   =   _t\n",
    "            position['ClosePrice']  =   profit_target_price\n",
    "            position['Status']      =   2   # Status ·ÄÄ open ·ÄÜ·Ä≠·ÄØ 0 close ·ÄÜ·Ä≠·ÄØ 1\n",
    "            position['CloseStep']   =   self.current_step\n",
    "            position['PIPS']        =   close_position_reward - self.transaction_fee\n",
    "            position['DeltaStep']   =   self.current_step - position['ActionStep']\n",
    "            position['Info']        =   f'{profit_target_price:.5f} | {stop_loss_price:.5f}'\n",
    "\n",
    "            self.balance            +=  self.margin_requirement + position['PIPS'] # return 100 is margin hold\n",
    "            position['CloseBal']    =   self.balance\n",
    "            closed                  =   True\n",
    "            _msg.append(f'Step:{self.current_step} Tkt:{position[\"Ticket\"]}: Rwd:{position[\"PIPS\"]}, SL:{position[\"SL\"]}, DeltaStep:{position[\"DeltaStep\"]}')\n",
    "\n",
    "        else:\n",
    "            if self.current_step + 5 + self.sequence_length >= len(self.data):\n",
    "                close_position_reward   =   (_c - position[\"ActionPrice\"] if direction == 'BUY' else position[\"ActionPrice\"] - _c)* self.point\n",
    "\n",
    "                position['CloseTime']   =   _t\n",
    "                position['ClosePrice']  =   _c\n",
    "                position['Status']      =   3   # Status ·ÄÄ open ·ÄÜ·Ä≠·ÄØ 0 close ·ÄÜ·Ä≠·ÄØ 1, force close 2\n",
    "                position['CloseStep']   =   self.current_step\n",
    "                position['PIPS']        =   close_position_reward - self.transaction_fee\n",
    "                position['DeltaStep']   =   self.current_step - position['ActionStep']\n",
    "                position['Info']        =   f'{profit_target_price:.5f} | {stop_loss_price:.5f}'\n",
    "                self.balance            +=  self.margin_requirement + position[\"PIPS\"] # return 100 is margin hold\n",
    "                position['CloseBal']    =   self.balance\n",
    "\n",
    "                closed                  =   True\n",
    "                _msg.append(f'Step:{self.current_step} Tkt:{position[\"Ticket\"]}: Rwd:{position[\"PIPS\"]}, Cls:End, DeltaStep:{position[\"DeltaStep\"]}')\n",
    "\n",
    "            else:\n",
    "                # =========================================================================\n",
    "                # Real Trailing Stop Logic (·Ä°·Äô·Äº·ÄÑ·Ä∑·Ä∫·ÄÜ·ÄØ·Ä∂·Ä∏ ·Äõ·Ä±·Ä¨·ÄÄ·Ä∫·Äñ·Ä∞·Ä∏·Äû·Ä±·Ä¨ ·Äà·Ä±·Ä∏·Äî·Äæ·ÄØ·Äî·Ä∫·Ä∏·ÄÄ·Ä≠·ÄØ ·Äô·Äæ·Äê·Ä∫·Äê·Äô·Ä∫·Ä∏·Äê·ÄÑ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏)\n",
    "                # =========================================================================\n",
    "                # 1. Highest/Lowest Price Update\n",
    "\n",
    "                if direction == \"BUY\":\n",
    "                  # Buy position ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Ä°·Äô·Äº·ÄÑ·Ä∑·Ä∫·ÄÜ·ÄØ·Ä∂·Ä∏ ·Äõ·Ä±·Ä¨·ÄÄ·Ä∫·Äñ·Ä∞·Ä∏·Äû·Ä±·Ä¨ ·Äà·Ä±·Ä∏·Äî·Äæ·ÄØ·Äî·Ä∫·Ä∏·ÄÄ·Ä≠·ÄØ ·Äô·Äæ·Äê·Ä∫·Äê·Äô·Ä∫·Ä∏·Äê·ÄÑ·Ä∫\n",
    "                  if _c > position[\"HighestPrice\"]:\n",
    "                      position[\"HighestPrice\"] = _c\n",
    "\n",
    "                  # 2. New SL Target Price (Trailing Price) ·ÄÄ·Ä≠·ÄØ ·Äê·ÄΩ·ÄÄ·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "                  # New_SL_Price = HighestPrice - (Trailing Distance Pips ·ÄÄ·Ä≠·ÄØ Price Change ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏)\n",
    "                  trailing_price = position[\"HighestPrice\"] - self.trailing_distance / self.point\n",
    "\n",
    "                  # 3. SL ·ÄÄ·Ä≠·ÄØ ·Ä°·ÄÜ·ÄÑ·Ä∑·Ä∫·Äô·Äº·Äæ·ÄÑ·Ä∑·Ä∫·Äê·ÄÑ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "                  # ·Äú·ÄÄ·Ä∫·Äõ·Äæ·Ä≠ SL ·Äë·ÄÄ·Ä∫ ·Äï·Ä≠·ÄØ·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äô·Äæ·Äû·Ä¨ ·Äõ·ÄΩ·Ä±·Ä∑·Äï·Ä´\n",
    "                  if trailing_price > stop_loss_price:\n",
    "\n",
    "                      stop_loss_price = trailing_price\n",
    "                      # SL_Price ·Ä°·Äû·ÄÖ·Ä∫·ÄÄ·Ä≠·ÄØ Points ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Äî·Ä∫·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Äº·ÄÆ·Ä∏ position['SL'] ·ÄÄ·Ä≠·ÄØ ·Ä°·Äï·Ä∫·Äí·Ä≠·Äê·Ä∫·Äú·ÄØ·Äï·Ä∫·Äï·Ä´\n",
    "                      position[\"SL\"] = (stop_loss_price - entry_price) * self.point\n",
    "                    #   if position[\"SL\"] > 0:\n",
    "                    #       position[\"SL\"]    =   -abs(position[\"SL\"])\n",
    "                      trailing_happened = True\n",
    "                  else:\n",
    "                      trailing_happened = False\n",
    "\n",
    "\n",
    "                elif direction == \"SELL\":\n",
    "                  # Sell position ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Ä°·Äî·Ä≠·Äô·Ä∑·Ä∫·ÄÜ·ÄØ·Ä∂·Ä∏ ·Äõ·Ä±·Ä¨·ÄÄ·Ä∫·Äñ·Ä∞·Ä∏·Äû·Ä±·Ä¨ ·Äà·Ä±·Ä∏·Äî·Äæ·ÄØ·Äî·Ä∫·Ä∏·ÄÄ·Ä≠·ÄØ ·Äô·Äæ·Äê·Ä∫·Äê·Äô·Ä∫·Ä∏·Äê·ÄÑ·Ä∫\n",
    "                  if _c < position[\"LowestPrice\"]:\n",
    "                      position[\"LowestPrice\"] = _c\n",
    "\n",
    "                  # New SL Target Price (Trailing Price) ·ÄÄ·Ä≠·ÄØ ·Äê·ÄΩ·ÄÄ·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "                  trailing_price = position[\"LowestPrice\"] + self.trailing_distance / self.point\n",
    "\n",
    "                  # SL ·ÄÄ·Ä≠·ÄØ ·Ä°·ÄÜ·ÄÑ·Ä∑·Ä∫·Äô·Äº·Äæ·ÄÑ·Ä∑·Ä∫·Äê·ÄÑ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "                  if trailing_price < stop_loss_price:\n",
    "                      stop_loss_price = trailing_price\n",
    "                      # SL_Price ·Ä°·Äû·ÄÖ·Ä∫·ÄÄ·Ä≠·ÄØ Points ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Äî·Ä∫·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Äº·ÄÆ·Ä∏ position['SL'] ·ÄÄ·Ä≠·ÄØ ·Ä°·Äï·Ä∫·Äí·Ä≠·Äê·Ä∫·Äú·ÄØ·Äï·Ä∫·Äï·Ä´\n",
    "                      position[\"SL\"] = (entry_price - stop_loss_price) * self.point\n",
    "                    #   if position[\"SL\"] > 0:\n",
    "                    #       position[\"SL\"]    =   -abs(position[\"SL\"])\n",
    "                      trailing_happened = True\n",
    "                  else:\n",
    "                      trailing_happened = False\n",
    "\n",
    "                # =========================================================================\n",
    "                # Reward Logic (Trailing ·Äú·ÄØ·Äï·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ Bonus ·Äï·Ä±·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏)\n",
    "                # =========================================================================\n",
    "                # Reward Sign ·ÄÄ·Ä≠·ÄØ ·Äö·ÄÅ·ÄÑ·Ä∫·Ä°·Äê·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏ ·Äê·ÄΩ·ÄÄ·Ä∫·Äï·Ä´·Åã\n",
    "                delta = _c - entry_price\n",
    "                if direction == \"BUY\":\n",
    "                    reward_sign = 1 if delta >= 0 else -1\n",
    "                elif direction == \"SELL\":\n",
    "                    reward_sign = -1 if delta >= 0 else 1\n",
    "\n",
    "                good_position_reward = reward_sign * self.good_position_reward_scale\n",
    "\n",
    "                # Trailing ·Ä°·Äô·Äæ·Äî·Ä∫·Äê·ÄÄ·Äö·Ä∫ ·Äñ·Äº·ÄÖ·Ä∫·Äû·ÄΩ·Ä¨·Ä∏·Äô·Äæ·Äû·Ä¨ Bonus Reward ·ÄÄ·Ä≠·ÄØ ·Äï·Ä±·Ä∏·Äï·Ä´\n",
    "                if trailing_happened:\n",
    "                    good_position_reward += 0.001\n",
    "\n",
    "                position['Info']        =   f'{profit_target_price:.5f} | {stop_loss_price:.5f}'\n",
    "                position['CloseBal']    =   self.balance\n",
    "                _msg.append(f'Step:{self.current_step} Tkt:{position[\"Ticket\"]}: NO_Close, PT:{position[\"PT\"]}, SL:{position[\"SL\"]}')\n",
    "\n",
    "        return close_position_reward + good_position_reward, closed, _msg\n",
    "\n",
    "\n",
    "    def _calculate_sharpe(self, risk_free_rate=0.0):\n",
    "        \"\"\"Calculate Sharpe ratio for the current episode\"\"\"\n",
    "        if len(self.equity_curve) < 2:\n",
    "            return 0.0\n",
    "\n",
    "        returns = np.diff(self.equity_curve) / self.equity_curve[:-1]\n",
    "\n",
    "        if np.std(returns) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        sharpe = (np.mean(returns) - risk_free_rate) / np.std(returns)\n",
    "        return float(sharpe * np.sqrt(288))  # Annualized (5-min bars ‚Üí 288/day)\n",
    "\n",
    "    def _calculate_drawdown(self):\n",
    "        \"\"\"Update max drawdown during episode\"\"\"\n",
    "        current_equity          =   self.equity_curve[-1]\n",
    "        self.peak_equity        =   max(self.peak_equity, current_equity)\n",
    "        self.current_drawdown   =   (self.peak_equity - current_equity) / self.peak_equity\n",
    "        self.max_drawdown       =   max(self.max_drawdown, self.current_drawdown)\n",
    "\n",
    "\n",
    "    def _calculate_current_equity(self):\n",
    "        \"\"\"Calculate total current equity (balance + unrealized P/L)\"\"\"\n",
    "        total_equity = self.balance  # Start with cash balance\n",
    "\n",
    "        # Add unrealized P/L from open positions\n",
    "        for position in self.positions:\n",
    "            if position['Status'] == 0:  # Only open positions\n",
    "                current_price = self.data.iloc[self.current_step][\"close\"]\n",
    "                entry_price = position['ActionPrice']\n",
    "\n",
    "                if position['Type'] == 'BUY':\n",
    "                    unrealized_pnl = (current_price - entry_price) * self.point\n",
    "                else:  # Sell\n",
    "                    unrealized_pnl = (entry_price - current_price) * self.point\n",
    "\n",
    "                total_equity += unrealized_pnl\n",
    "\n",
    "        return total_equity\n",
    "\n",
    "    def render(self, mode='human', title=None, **kwargs):\n",
    "        # Render the environment to the screen\n",
    "        if mode in ('human', 'file'):\n",
    "            log_header      =   True\n",
    "            printout        =   False\n",
    "            if mode == 'human':\n",
    "                printout    =   True\n",
    "\n",
    "            log_file = self.csv_file.replace(\"split/\", \"log/\")\n",
    "            pm = {\n",
    "                \"log_header\": log_header,\n",
    "                \"log_filename\": log_file,\n",
    "                \"printout\": printout,\n",
    "                \"balance\": self.balance,\n",
    "                \"balance_initial\": self.balance_initial,\n",
    "                \"transaction_close_this_step\": self.positions,\n",
    "                \"done_information\": False\n",
    "            }\n",
    "            render_to_file(**pm)\n",
    "            if log_header:\n",
    "                    log_header = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f63c455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import dump, load # Scaler ·ÄÄ·Ä≠·ÄØ ·Äû·Ä≠·Äô·Ä∫·Ä∏·ÄÜ·Ää·Ä∫·Ä∏/·Äï·Äº·Äî·Ä∫·Äö·Ä∞·Äõ·Äî·Ä∫\n",
    "\n",
    "cf = EnvConfig('./drive/MyDrive/configure.json')\n",
    "features_scaled = cf.env_parameters(item='features_scaled')\n",
    "\n",
    "SCALER_PATH = '/content/drive/MyDrive/data/model/EURUSD/scalar_2020_2021.joblib'\n",
    "DATA_PATH = '/content/drive/MyDrive/data/raw/final.csv'\n",
    "train_scaler = StandardScaler()\n",
    "\n",
    "train_data_df = pd.read_csv(DATA_PATH)\n",
    "data_to_fit = train_data_df[features_scaled]\n",
    "data_to_fit = data_to_fit.dropna()\n",
    "train_scaler.fit(data_to_fit)\n",
    "\n",
    "# 3. Scaler Object ·ÄÄ·Ä≠·ÄØ File ·Ä°·Äñ·Äº·ÄÖ·Ä∫ ·Äû·Ä≠·Äô·Ä∫·Ä∏·ÄÜ·Ää·Ä∫·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏ (Production/Test ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫)\n",
    "dump(train_scaler, SCALER_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a52685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "# Total Data: ·ÅÇ·ÅÄ·ÅÇ·ÅÄ ·Äá·Äî·Ä∫·Äî·Äù·Ä´·Äõ·ÄÆ·Äô·Äæ ·ÅÇ·ÅÄ·ÅÇ·ÅÖ (·ÅÖ ·Äî·Äæ·ÄÖ·Ä∫·ÄÖ·Ä¨)\n",
    "# Total Steps (·ÄÅ·Äî·Ä∑·Ä∫·Äô·Äæ·Äî·Ä∫·Ä∏): 72,000 Steps/·Äî·Äæ·ÄÖ·Ä∫ 5·Äî·Äæ·ÄÖ·Ä∫ === 360,000 Steps\n",
    "# Total Training Steps (Decay ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫): Continuous Fine-tuning Steps ·Äô·Äª·Ä¨·Ä∏·Äï·Ä´ ·Äë·Ää·Ä∑·Ä∫·Äû·ÄΩ·ÄÑ·Ä∫·Ä∏·Äê·ÄΩ·ÄÄ·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫·Äï·Ä´·ÄÄ·Åä\n",
    "# ·ÄÖ·ÄØ·ÄÖ·ÄØ·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏ 1,000,000 (1M) Steps ·ÄÄ·Ä≠·ÄØ Total Decay Length ·Ä°·Äñ·Äº·ÄÖ·Ä∫ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·ÄÄ·Äº·Ää·Ä∑·Ä∫·Äï·Ä´·Åã\n",
    "# Global Variables ·ÄÄ·Ä≠·ÄØ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "\n",
    "GLOBAL_TOTAL_STEPS = 1_000_000 # ·Ä•·Äï·Äô·Ä¨- 1 Million Steps\n",
    "START_LR = 1e-4\n",
    "END_LR = 5e-6\n",
    "\n",
    "class ContinuousLRScheduler(BaseCallback):\n",
    "    def __init__(self, total_global_steps, verbose=0):\n",
    "        super(ContinuousLRScheduler, self).__init__(verbose)\n",
    "        self.total_global_steps = total_global_steps\n",
    "        # 'num_timesteps' ·Äû·Ää·Ä∫ SB3 ·ÄÄ Model ·Äê·ÄÖ·Ä∫·ÄÅ·ÄØ Training ·ÄÖ·Äê·ÄÑ·Ä∫·ÄÅ·Äª·Ä≠·Äî·Ä∫·Äô·Äæ ·ÄÖ·ÄØ·ÄÖ·ÄØ·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏ Step ·ÄÄ·Ä≠·ÄØ ·Äô·Äæ·Äê·Ä∫·Äë·Ä¨·Ä∏·Äû·Ää·Ä∫·Åã\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # ·Äú·ÄÄ·Ä∫·Äõ·Äæ·Ä≠ ·ÄÖ·ÄØ·ÄÖ·ÄØ·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏ Global Step ·ÄÄ·Ä≠·ÄØ ·Äõ·Äö·Ä∞·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "        current_global_step = self.num_timesteps\n",
    "\n",
    "        # Global Progress (0.0 ·Äô·Äæ 1.0) ·ÄÄ·Ä≠·ÄØ ·Äê·ÄΩ·ÄÄ·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "        progress = current_global_step / self.total_global_steps\n",
    "        progress_remaining = 1.0 - progress\n",
    "\n",
    "        # LR ·Ä°·Äû·ÄÖ·Ä∫·ÄÄ·Ä≠·ÄØ ·Äê·ÄΩ·ÄÄ·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏ (·Äû·ÄÑ·Ä∫·Åè Decay Logic ·Ä°·Äê·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏)\n",
    "        new_lr = END_LR + (START_LR - END_LR) * progress_remaining\n",
    "\n",
    "        # Optimizer ·Åè LR ·ÄÄ·Ä≠·ÄØ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "        self.model.policy.optimizer.param_groups[0]['lr'] = new_lr\n",
    "\n",
    "        return True\n",
    "\n",
    "# ·Ä§ Callback ·ÄÄ·Ä≠·ÄØ Initial Training ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ Continuous Fine-tuning ·Äî·Äæ·ÄÖ·Ä∫·ÄÅ·ÄØ·ÄÖ·Äú·ÄØ·Ä∂·Ä∏·Äê·ÄΩ·ÄÑ·Ä∫ ·Äû·ÄØ·Ä∂·Ä∏·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äï·Ä´·Äû·Ää·Ä∫·Åã\n",
    "# ·Åé·ÄÑ·Ä∫·Ä∏·Äû·Ää·Ä∫ Agent ·Åè ·Äú·ÄÄ·Ä∫·Äõ·Äæ·Ä≠ num_timesteps ·Äï·Ä±·Ä´·Ä∫·Äô·Ä∞·Äê·Ää·Ä∫·Åç LR ·ÄÄ·Ä≠·ÄØ ·Ä°·ÄÜ·ÄÄ·Ä∫·Äô·Äï·Äº·Äê·Ä∫ ·Äú·Äª·Ä±·Ä¨·Ä∑·ÄÅ·Äª·Äï·Ä±·Ä∏·Äû·ÄΩ·Ä¨·Ä∏·Äï·Ä´·Äô·Ää·Ä∫·Åã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dc5c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3 import PPO\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "# Assume logger is defined elsewhere, e.g., import logging; logger = logging.getLogger(__name__)\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "\n",
    "BASE_SEED = 42\n",
    "number_envs = 1\n",
    "# Stable-Baselines3 ·Äõ·Ä≤·Ä∑ Global Seed ·ÄÄ·Ä≠·ÄØ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äï·Ä´\n",
    "set_random_seed(BASE_SEED)\n",
    "\n",
    "\n",
    "def single_csv_training(csv_file, env_config_file, asset, model_name='', cf=None, number_envs=1, week_num=0):  # Added week_num for varying seed\n",
    "    # 1. Log Root Directory ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ Run Name ·ÄÄ·Ä≠·ÄØ ·Äê·ÄΩ·ÄÄ·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "    BASE_LOG_DIR = \"/content/drive/MyDrive/data/log\"\n",
    "    RUN_NAME = f\"{asset}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "    # 2. Log Root Directory ·Äõ·Äæ·Ä≠·Äô·Äõ·Äæ·Ä≠ ·ÄÖ·ÄÖ·Ä∫·ÄÜ·Ä±·Ä∏·Äï·Äº·ÄÆ·Ä∏ ·Äñ·Äî·Ä∫·Äê·ÄÆ·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "    os.makedirs(BASE_LOG_DIR, exist_ok=True)\n",
    "\n",
    "    #sequence_length = cf.env_parameters(\"backward_window\")\n",
    "    sequence_length = cf.data_processing_parameters(\"sequence_length\")\n",
    "    # lr_schedule = linear_schedule(1e-4, 5e-6)\n",
    "    policy_kwargs = dict(\n",
    "        # Repo ·Äõ·Ä≤·Ä∑ custom feature extractor (Transformer + MLP ·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏·Äë·Ä¨·Ä∏·Äê·Ä¨·Åä time series data ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Äû·ÄÑ·Ä∑·Ä∫·Äê·Ä±·Ä¨·Ä∫·Äê·Äö·Ä∫)·Åã\n",
    "        features_extractor_class=CustomCombinedExtractor,\n",
    "        # features_extractor_kwargs: Sequence length ·ÄÄ·Ä≠·ÄØ ·Äë·Ää·Ä∑·Ä∫·Åã\n",
    "        features_extractor_kwargs=dict(sequence_length=sequence_length),\n",
    "        # net_arch: Actor (pi - policy network) ·Äî·Ä≤·Ä∑ Critic (vf - value function) ·Äî·Äæ·ÄÖ·Ä∫·ÄÅ·ÄØ ·Äú·ÄØ·Ä∂·Ä∏ ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ hidden layers [256, 256] ·Äû·ÄØ·Ä∂·Ä∏·Åã (Updated vf to [512,256] for better explained variance)\n",
    "        net_arch=dict(pi=[256, 256], vf=[512, 256]),  # Increased vf capacity\n",
    "        # Activation function ·Ä°·Äî·Ä±·Äî·Ä≤·Ä∑ ReLU ·Äû·ÄØ·Ä∂·Ä∏ (non-linear ·Äñ·Äº·ÄÖ·Ä∫·Ä°·Ä±·Ä¨·ÄÑ·Ä∫)·Åã\n",
    "        activation_fn=nn.ReLU,\n",
    "        # Orthogonal initialization ·Äô·Äû·ÄØ·Ä∂·Ä∏ (financial data ·Äô·Äæ·Ä¨ ·Äï·Ä≠·ÄØ·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏ ·Äê·Äö·Ä∫·Äú·Ä≠·ÄØ·Ä∑ comment ·Äô·Äæ·Ä¨ ·Äõ·Ä±·Ä∏ ·Äë·Ä¨·Ä∏·Äê·Äö·Ä∫·Åä ·Äí·Ä´·ÄÄ weights ·ÄÄ·Ä≠·ÄØ ·Äï·Ä≠·ÄØ·Äõ·Ä≠·ÄØ·Ä∏·Äõ·Äæ·ÄÑ·Ä∫·Ä∏ ·ÄÖ ·Äú·ÄØ·Äï·Ä∫·Äê·Äö·Ä∫)·Åã\n",
    "        ortho_init=False  # better for financial data\n",
    "    )\n",
    "\n",
    "    # Environment Factories ·Äô·Äª·Ä¨·Ä∏ ·Äñ·Äî·Ä∫·Äê·ÄÆ·Ä∏·Äï·Ä´\n",
    "    env_fns = [\n",
    "        lambda: ForexTradingEnv(\n",
    "            csv_file,\n",
    "            cf,\n",
    "            asset,\n",
    "            logger_show=True,\n",
    "            scaler=train_scaler\n",
    "        )\n",
    "        for _ in range(number_envs)\n",
    "    ]\n",
    "    # DummyVecEnv ·ÄÄ·Ä≠·ÄØ ·Äê·Ää·Ä∫·ÄÜ·Ä±·Ä¨·ÄÄ·Ä∫·Äï·Ä´ (SubprocVecEnv)\n",
    "    env = DummyVecEnv(env_fns)\n",
    "    # ·Ä§·Äî·Ä±·Äõ·Ä¨·Äû·Ää·Ä∫ ·Ä°·Äì·Ä≠·ÄÄ·ÄÄ·Äª·Äû·Ää·Ä∫·Åã ·Åé·ÄÑ·Ä∫·Ä∏·ÄÄ Environment ·Äê·ÄÖ·Ä∫·ÄÅ·ÄØ·ÄÅ·Äª·ÄÑ·Ä∫·Ä∏·ÄÖ·ÄÆ·ÄÄ·Ä≠·ÄØ\n",
    "    # BASE_SEED, BASE_SEED+1, BASE_SEED+2... ·ÄÖ·Äû·Ää·Ä∫·Äñ·Äº·ÄÑ·Ä∑·Ä∫ Seed ·Äô·Äª·Ä¨·Ä∏ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äï·Ä±·Ä∏·Äï·Äº·ÄÆ·Ä∏\n",
    "    # ·Åé·ÄÑ·Ä∫·Ä∏·Äê·Ä≠·ÄØ·Ä∑·Åè reset() ·ÄÄ·Ä≠·ÄØ ·Äï·Äº·Äî·Ä∫·Äú·Ää·Ä∫·ÄÅ·Ä±·Ä´·Ä∫·Äï·Ä±·Ä∏·Äú·Ä≠·Äô·Ä∑·Ä∫·Äô·Ää·Ä∫·Åã\n",
    "    # Vary seed per week to avoid overfitting in incremental training\n",
    "    varied_seed = BASE_SEED + week_num  # Example: Pass week_num=1 for week 2, etc.\n",
    "    env.seed(varied_seed)\n",
    "\n",
    "    if model_name:\n",
    "        model = PPO.load(\n",
    "            model_name,\n",
    "            env=env,\n",
    "            learning_rate=START_LR # override by learning_rate callback\n",
    "        )\n",
    "    else:\n",
    "        model = PPO(\n",
    "            'MlpPolicy',\n",
    "            env,\n",
    "            device='cuda', # üí• ·Ä§·Äî·Ä±·Äõ·Ä¨·Äê·ÄΩ·ÄÑ·Ä∫ 'cpu' ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÄ·Äº·Ää·Ä∑·Ä∫·Äï·Ä´\n",
    "            verbose=1,\n",
    "            tensorboard_log=BASE_LOG_DIR,\n",
    "            normalize_advantage=True,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            learning_rate=START_LR,\n",
    "            seed=varied_seed,\n",
    "\n",
    "            # ·Ä°·Äõ·Ä±·Ä∏·ÄÄ·Äº·ÄÆ·Ä∏\n",
    "            n_steps= 2048,\n",
    "            batch_size= 256,\n",
    "            n_epochs= 10,\n",
    "            max_grad_norm=0.5  # Tighter gradient control\n",
    "\n",
    "\n",
    "            # vf_coef=0.5,  # Reduced from 0.7 to balance policy vs value\n",
    "            # target_kl=0.02,  # Increased from 0.005 for better updates\n",
    "            # ent_coef=0.005,  # Reduced from 0.01 for controlled exploration\n",
    "            # clip_range=0.01,  # Increased from 0.002 to reduce high clip_fraction\n",
    "            # gamma=0.99,\n",
    "        )\n",
    "\n",
    "    # Train the agent\n",
    "    logger.info(\"Starting model training...\")\n",
    "    metrics_callback = TrainingMetricsCallback()\n",
    "\n",
    "    lr_callback = ContinuousLRScheduler(GLOBAL_TOTAL_STEPS)\n",
    "    # Callback ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ List ·Ä°·Äî·Ä±·Äñ·Äº·ÄÑ·Ä∑·Ä∫ ·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏·ÄÖ·Äï·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "    callback_list = CallbackList([lr_callback, metrics_callback])\n",
    "\n",
    "    model.learn(\n",
    "        total_timesteps=1000000, # retain 200,000\n",
    "        callback=callback_list,\n",
    "\n",
    "        # üö® ·Äï·Äº·ÄÑ·Ä∫·ÄÜ·ÄÑ·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫ ·ÅÇ: tb_log_name ·Äî·Ä±·Äõ·Ä¨·Äô·Äæ·Ä¨ Run Folder Name ·ÄÄ·Ä≠·ÄØ·Äï·Ä≤ ·Äï·Ä±·Ä∏·Äï·Ä´·Åã\n",
    "        tb_log_name=RUN_NAME,\n",
    "        reset_num_timesteps=False if model_name else True  # üîÑ Existing model ·ÄÜ·Ä≠·ÄØ·Äõ·ÄÑ·Ä∫ timesteps ·ÄÜ·ÄÄ·Ä∫·Äô·Äæ·Äê·Ä∫\n",
    "    )\n",
    "    logger.info(\"Model training complete\")\n",
    "    model_filename = csv_file.replace(\"split/\", \"model/\").replace(\".csv\", \"_single_test.zip\")\n",
    "    model.save(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0cfdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "log_dir = '/content/drive/MyDrive/data/log'\n",
    "\n",
    "%tensorboard --logdir $log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77b994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv('/content/drive/MyDrive/data/raw/final.csv')\n",
    "data_raw.head(3)\n",
    "    # if 'time' in self.data_raw.columns:\n",
    "    #     self.data_raw = self.data_raw.set_index(pd.to_datetime(self.data_raw['time'], utc=True)).drop(columns=['time'])\n",
    "\n",
    "    # # self.data ·ÄÄ·Ä≠·ÄØ Scaling ·Äú·ÄØ·Äï·Ä∫·Äõ·Äî·Ä∫ Copy ·Äö·Ä∞·Äï·Ä´·Äô·Ää·Ä∫·Åã\n",
    "    # self.data = self.data_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index ·ÄÄ·Ä≠·ÄØ datetime type ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏\n",
    "data_raw.index = pd.to_datetime(data_raw.index)\n",
    "\n",
    "# ·ÄÖ·ÄÖ·Ä∫·ÄÜ·Ä±·Ä∏\n",
    "print(data_raw.head())\n",
    "print(data_raw.index)  # datetime index ·Äñ·Äº·ÄÖ·Ä∫·Äî·Ä±·Äô·Äö·Ä∫\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da262914",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset = \"EURUSD\"\n",
    "env_config_file = '/content/drive/MyDrive/configure.json'\n",
    "cf = EnvConfig(env_config_file)\n",
    "split_cfg = cf.data_processing_parameters(\"train_eval_split\")\n",
    "base_path = split_cfg[\"base_path\"].format(symbol=asset)\n",
    "csv_file = '/content/drive/MyDrive/data/raw/final.csv'\n",
    "model_name = ''\n",
    "single_csv_training(csv_file=csv_file, env_config_file =env_config_file, asset= asset, model_name=model_name, cf=cf, number_envs=1, week_num=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ForexTradingEnv('/content/drive/MyDrive/data/raw/final.csv', cf, 'EURUSD', logger_show=False, scaler=train_scaler)\n",
    "obs, info = env.reset()\n",
    "print(\"Observation shape:\", np.shape(obs))\n",
    "\n",
    "obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "done = terminated or truncated\n",
    "print(\"Step OK:\", reward, done)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09bb9bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def convert_1m_to_5m_df(file_path: str) -> pd.DataFrame:\n",
    "    # Column Names (·Äû·ÄÑ·Ä∑·Ä∫·Äõ·Ä≤·Ä∑ Data ·Ä°·ÄÖ·ÄÆ·Ä°·ÄÖ·Äâ·Ä∫·Ä°·Äê·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏)\n",
    "    COLUMN_NAMES = ['Date', 'Time', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    DATETIME_FORMAT = '%Y.%m.%d %H:%M'\n",
    "\n",
    "    \"\"\" CSV File ·Äô·Äæ 1-Minute Data ·ÄÄ·Ä≠·ÄØ Load ·Äï·Äº·ÄÆ·Ä∏ 5-Minute Candle ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äú·Ä≤·Äû·Ää·Ä∫·Åã \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"üö® Error: File not found at path: {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=',', header=None, names=COLUMN_NAMES,\n",
    "                         dtype={'Open': np.float64, 'High': np.float64, 'Low': np.float64, 'Close': np.float64})\n",
    "        \n",
    "        # Volume column ·ÄÄ·Ä≠·ÄØ ·Äö·Ä¨·Äö·ÄÆ float ·Ä°·Äî·Ä±·Äñ·Äº·ÄÑ·Ä∑·Ä∫ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äï·Äº·ÄÆ·Ä∏ NA ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ 0 ·Äñ·Äº·ÄÑ·Ä∑·Ä∫ ·Ä°·ÄÖ·Ä¨·Ä∏·Äë·Ä≠·ÄØ·Ä∏·Äû·Ää·Ä∫·Åã\n",
    "        df['Volume'] = pd.to_numeric(df['Volume'], errors='coerce').fillna(0)\n",
    "        df['Volume'] = df['Volume'].astype(np.int64) \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"üö® Error loading CSV file: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Datetime Index ·ÄÄ·Ä≠·ÄØ ·Äê·Ää·Ä∫·ÄÜ·Ä±·Ä¨·ÄÄ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "    df['Datetime'] = df['Date'].astype(str) + ' ' + df['Time'].astype(str)\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'], format=DATETIME_FORMAT, errors='coerce')\n",
    "    df.set_index('Datetime', inplace=True)\n",
    "    df.drop(columns=['Date', 'Time'], inplace=True)\n",
    "    #df.dropna(subset=[df.index.name], inplace=True) # Invalid Datetime ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Äñ·Äö·Ä∫·Äõ·Äæ·Ä¨·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "\n",
    "    ohlcv_aggregation_rules: Dict[str, Any] = {\n",
    "        'Open': 'first', 'High': 'max', 'Low': 'min', 'Close': 'last', 'Volume': 'sum'\n",
    "    }\n",
    "    df = df.resample('5Min').agg(ohlcv_aggregation_rules)\n",
    "    df.dropna(inplace=True)\n",
    "    df = df[df['Volume'] > 0]\n",
    "    \n",
    "    print(f\"‚úÖ Conversion successful! 5-Min rows: {len(df)}\")\n",
    "    return df.reset_index().rename(columns={'index': 'Datetime'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "058ad0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conversion successful! 5-Min rows: 149715\n"
     ]
    }
   ],
   "source": [
    "file_path_from_data = \"./content/drive/MyDrive/data/raw/EURUSD_2020_all.csv\"\n",
    "file_path_to_data = \"./content/drive/MyDrive/data/raw/EURUSD_2020_all_5.csv\"\n",
    "\n",
    "raw = convert_1m_to_5m_df(file_path_from_data)\n",
    "raw.to_csv(file_path_to_data, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0be379fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class EnvConfig():\n",
    "    \"\"\"environment configuration from json file\n",
    "       tgym requires you configure your own parameters in json file.\n",
    "        Args:\n",
    "            config_file path/file.json\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,config_file):\n",
    "        self.config = {}\n",
    "        with open(config_file) as j:\n",
    "            self.config = json.load(j)\n",
    "\n",
    "    def env_parameters(self,item=''):\n",
    "        \"\"\"environment variables\n",
    "        \"\"\"\n",
    "        if item:\n",
    "            return self.config[\"env\"][item]\n",
    "        else:\n",
    "            return self.config[\"env\"]\n",
    "\n",
    "    def symbol(self, asset=\"GBPUSD\", item='') :\n",
    "        \"\"\"get trading pair (symbol) information\n",
    "\n",
    "        Args:\n",
    "            asset (str, optional): symbol in config. Defaults to \"GBPUSD\".\n",
    "            item (str, optional): name of item, if '' return dict, else return item value. Defaults to ''.\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        if item:\n",
    "            return self.config[\"symbol\"][asset][item]\n",
    "        else:\n",
    "            return self.config[\"symbol\"][asset]\n",
    "\n",
    "    def data_processing_parameters(self, item=''):\n",
    "        \"\"\"Get data processing config\"\"\"\n",
    "        if item:\n",
    "            return self.config[\"data_processing\"][item]\n",
    "        return self.config[\"data_processing\"]\n",
    "\n",
    "    def trading_hour(self,place=\"NewYork\"):\n",
    "        \"\"\"forex trading hour from different markets\n",
    "\n",
    "        Args:\n",
    "            place (str, optional): [Sydney,Tokyo,London] Defaults to \"New York\".\n",
    "\n",
    "        Returns:\n",
    "            [dict]: from time, to time\n",
    "        \"\"\"\n",
    "        if place:\n",
    "            return self.config[\"trading_hour\"][place]\n",
    "        else:\n",
    "            return self.config[\"trading_hour\"]\n",
    "\n",
    "    def indicator(self,place=\"sma_fast_period\"):\n",
    "        \"\"\"forex trading hour from different markets\n",
    "\n",
    "        Args:\n",
    "            place (str, optional): [Sydney,Tokyo,London] Defaults to \"New York\".\n",
    "\n",
    "        Returns:\n",
    "            [dict]: from time, to time\n",
    "        \"\"\"\n",
    "        if place:\n",
    "            return self.config[\"data_processing\"][\"indicator\"][place]\n",
    "        else:\n",
    "            return self.config[\"data_processing\"][\"indicator\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b69f8c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "import logging\n",
    "# Configure logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def patch_missing_data(df, dt_col_name='time', cf=None):\n",
    "    min_bars = cf.data_processing_parameters(\"min_bars_per_week\")\n",
    "\n",
    "    # [\"time\",\"open\", \"high\", \"low\", \"close\"]\n",
    "    required_cols = cf.data_processing_parameters(\"required_cols\")\n",
    "\n",
    "    # df ·Äô·Äæ·Ä¨ 6 columns ·Äõ·Äæ·Ä≠·Äõ·ÄÑ·Ä∫ vol ·Äï·Ä´·Äë·Ää·Ä∑·Ä∫·Äô·Äö·Ä∫\n",
    "    if df.shape[1] == 6:\n",
    "        df.columns = required_cols + ['Volume']\n",
    "    elif df.shape[1] == 5:\n",
    "        df.columns = required_cols\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid number of columns: {df.shape[1]} =>{required_cols}\")\n",
    "\n",
    "    logger.warning(f\"shape of  column: {df.shape[1]}\")\n",
    "    # 1. Column validation\n",
    "    if missing := set(required_cols) - set(df.columns):\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "    # 2. Auto-detect datetime column\n",
    "    dt_candidates = {'time', 'timestamp', 'date', 'datetime', 'Datetime'}\n",
    "    if dt_col_name not in df.columns:\n",
    "        found = list(dt_candidates & set(df.columns))\n",
    "        if not found:\n",
    "            raise KeyError(f\"No datetime column found. Tried: {dt_candidates}\")\n",
    "        dt_col_name = found[0]\n",
    "        logger.info(f\"Using datetime column: {dt_col_name}\")\n",
    "\n",
    "    # 3. Convert to datetime index\n",
    "    df[dt_col_name] = pd.to_datetime(df[dt_col_name], utc=True)\n",
    "    df = df.set_index(dt_col_name).sort_index()\n",
    "    groups = df.groupby(pd.Grouper(freq='W-SUN'))\n",
    "\n",
    "    patched_weeks = []  # patched weekly df storage\n",
    "\n",
    "    for w, week_df in groups:\n",
    "        if week_df.empty:\n",
    "            continue\n",
    "\n",
    "        if len(week_df) != min_bars:\n",
    "            logger.warning(f\"Week {w} has {len(week_df)}/{min_bars} bars\")\n",
    "\n",
    "        # Create 5-minute frequency index\n",
    "        new_index = pd.date_range(\n",
    "            start=week_df.index.min(),\n",
    "            end=week_df.index.max(),\n",
    "            freq='5min',\n",
    "            tz='UTC'\n",
    "        )\n",
    "\n",
    "        # Reindex + forward fill\n",
    "        week_df = week_df.reindex(new_index)\n",
    "        week_df.index = week_df.index.tz_localize(None)\n",
    "        fill_limit = 12 # ·Ä•·Äï·Äô·Ä¨: 1 ·Äî·Ä¨·Äõ·ÄÆ (12 bars) ·Äë·ÄÄ·Ä∫·Äï·Ä≠·ÄØ·Äê·Ä≤·Ä∑ ·ÄÄ·ÄΩ·ÄÄ·Ä∫·Äú·Äï·Ä∫·ÄÄ·Ä≠·ÄØ ·Äô·Äñ·Äº·Ää·Ä∑·Ä∫·Äï·Ä´\n",
    "        fill_cols = ['open', 'high', 'low', 'close', 'vol'] if 'vol' in df.columns else ['open', 'high', 'low', 'close']\n",
    "        # FFill: ·Äõ·Äæ·Ä±·Ä∑·ÄÄ data ·Äñ·Äº·ÄÑ·Ä∑·Ä∫ ·Äñ·Äº·Ää·Ä∑·Ä∫·Äï·Ä´\n",
    "        week_df[fill_cols] = week_df[fill_cols].ffill(limit=fill_limit)\n",
    "        patched_weeks.append(week_df)\n",
    "\n",
    "    # Merge back all weeks\n",
    "    if patched_weeks:\n",
    "        all_df = pd.concat(patched_weeks)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "\n",
    "    return all_df.reset_index().rename(columns={'index': 'Datetime'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7607077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "shape of  column: 6\n",
      "Week 2020-01-05 00:00:00+00:00 has 576/1440 bars\n",
      "Week 2020-01-12 00:00:00+00:00 has 1439/1440 bars\n",
      "Week 2020-03-29 00:00:00+00:00 has 1433/1440 bars\n",
      "Week 2020-09-20 00:00:00+00:00 has 1439/1440 bars\n",
      "Week 2020-10-04 00:00:00+00:00 has 1436/1440 bars\n",
      "Week 2020-12-27 00:00:00+00:00 has 1152/1440 bars\n",
      "Week 2021-01-03 00:00:00+00:00 has 1142/1440 bars\n",
      "Week 2021-05-23 00:00:00+00:00 has 1439/1440 bars\n",
      "Week 2021-05-30 00:00:00+00:00 has 1437/1440 bars\n",
      "Week 2021-06-06 00:00:00+00:00 has 1439/1440 bars\n",
      "Week 2021-09-19 00:00:00+00:00 has 1439/1440 bars\n",
      "Week 2021-10-10 00:00:00+00:00 has 1439/1440 bars\n",
      "Week 2021-12-12 00:00:00+00:00 has 1439/1440 bars\n",
      "Week 2021-12-26 00:00:00+00:00 has 1438/1440 bars\n",
      "Week 2022-01-02 00:00:00+00:00 has 1428/1440 bars\n"
     ]
    }
   ],
   "source": [
    "cf = EnvConfig('./content/drive/MyDrive/configure.json')\n",
    "raw = pd.read_csv(file_path_to_data)\n",
    "df = patch_missing_data(raw,cf=cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e0f30d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149734</th>\n",
       "      <td>2021-12-31 23:45:00</td>\n",
       "      <td>1.13780</td>\n",
       "      <td>1.13801</td>\n",
       "      <td>1.13764</td>\n",
       "      <td>1.13795</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149735</th>\n",
       "      <td>2021-12-31 23:50:00</td>\n",
       "      <td>1.13796</td>\n",
       "      <td>1.13823</td>\n",
       "      <td>1.13778</td>\n",
       "      <td>1.13780</td>\n",
       "      <td>216.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149736</th>\n",
       "      <td>2021-12-31 23:55:00</td>\n",
       "      <td>1.13780</td>\n",
       "      <td>1.13781</td>\n",
       "      <td>1.13650</td>\n",
       "      <td>1.13660</td>\n",
       "      <td>210.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Datetime     open     high      low    close  Volume\n",
       "149734 2021-12-31 23:45:00  1.13780  1.13801  1.13764  1.13795   172.0\n",
       "149735 2021-12-31 23:50:00  1.13796  1.13823  1.13778  1.13780   216.0\n",
       "149736 2021-12-31 23:55:00  1.13780  1.13781  1.13650  1.13660   210.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be70eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_feature(df_5m: pd.DataFrame, cf=None, source_tz='UTC') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    5M Data Frame (DatetimeIndex ·Äï·Ä´·Äù·ÄÑ·Ä∫·Äû·Ää·Ä∫·Äü·ÄØ ·Äö·Ä∞·ÄÜ·Äï·Ä´) ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ Temporal features ·Äë·Ää·Ä∑·Ä∫·Äû·ÄΩ·ÄÑ·Ä∫·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏·Åã\n",
    "    \"\"\"\n",
    "    # üß≠ Ensure datetime index\n",
    "    if not isinstance(df_5m.index, pd.DatetimeIndex):\n",
    "        if 'Datetime' in df_5m.columns:\n",
    "            df_5m['Datetime'] = pd.to_datetime(df_5m['Datetime'])\n",
    "            df_5m = df_5m.set_index('Datetime')\n",
    "        else:\n",
    "            raise ValueError(\"DataFrame must have datetime index or 'time' column\")\n",
    "\n",
    "    # DataFrame ·Åè Index ·ÄÄ·Ä≠·ÄØ DatetimeIndex ·Ä°·Äñ·Äº·ÄÖ·Ä∫ ·ÄÖ·ÄÖ·Ä∫·ÄÜ·Ä±·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "    if not isinstance(df_5m.index, pd.DatetimeIndex):\n",
    "         raise TypeError(\"DataFrame ·Åè Index ·Äû·Ää·Ä∫ DatetimeIndex ·Äñ·Äº·ÄÖ·Ä∫·Äõ·Äï·Ä´·Äô·Ää·Ä∫·Åã\")\n",
    "\n",
    "    df_5m.index = df_5m.index.tz_localize(None)\n",
    "    # Index ·ÄÄ·Ä≠·ÄØ Timezone aware (UTC) ·Ä°·Äñ·Äº·ÄÖ·Ä∫ ·Äû·Ä±·ÄÅ·Äª·Ä¨·Ä°·Ä±·Ä¨·ÄÑ·Ä∫·Äú·ÄØ·Äï·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "    if df_5m.index.tz is None:\n",
    "        # Timezone-Naive data ·ÄÄ·Ä≠·ÄØ ·Äô·Ä∞·Äõ·ÄÑ·Ä∫·Ä∏ Source Timezone ·Äñ·Äº·ÄÑ·Ä∑·Ä∫ localize\n",
    "        # Dukascopy data ·ÄÜ·Ä≠·ÄØ·Äõ·ÄÑ·Ä∫ 'UTC' ·Äû·ÄØ·Ä∂·Ä∏·Äï·Äº·ÄÆ·Ä∏·Åä Broker data ·ÄÜ·Ä≠·ÄØ·Äõ·ÄÑ·Ä∫ 'GMT+3' ·Äú·Ä≠·ÄØ·Äô·Äª·Ä≠·ÄØ·Ä∏ ·Äû·ÄØ·Ä∂·Ä∏·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫\n",
    "        df = df_5m.tz_localize(source_tz, ambiguous='NaT', nonexistent='NaT')\n",
    "        df = df.tz_convert('UTC')\n",
    "    else:\n",
    "        df = df_5m.copy()\n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    # I. ·Ä°·ÄÅ·Äº·Ä±·ÄÅ·Ä∂ features ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ Cyclical Encoding ·Äô·Äª·Ä¨·Ä∏ (Hour ·ÄÄ·Ä≠·ÄØ Index ·Äô·Äæ ·Äê·Ä≠·ÄØ·ÄÄ·Ä∫·Äõ·Ä≠·ÄØ·ÄÄ·Ä∫·Äö·Ä∞·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏)\n",
    "    # ----------------------------------------------------\n",
    "\n",
    "    # df['weekday'] = df.index.dayofweek \n",
    "    # df['day'] = df.index.day\n",
    "    # df['week'] = df.index.isocalendar().week.astype(int)\n",
    "    # df['month'] = df.index.month\n",
    "    # df['year'] = df.index.year\n",
    "    df['hour'] = df.index.hour\n",
    "    \n",
    "    # ·Äî·Ä¨·Äõ·ÄÆ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ Cyclical features\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24).round(6)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24).round(6)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # III. DST-Aware Market Sessions (Timezone Handling)\n",
    "    # ----------------------------------------------------\n",
    "    \n",
    "    # ·Äî·Ä¨·Äõ·ÄÆ·ÄÄ·Ä≠·ÄØ local time zone ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äú·Ä≤ (Timezone Aware Index ·Äô·Äæ·Äû·Ä¨ tz_convert ·Äú·ÄØ·Äï·Ä∫·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äû·Ää·Ä∫)\n",
    "    london_time = df.index.tz_convert('Europe/London')\n",
    "    ny_time = df.index.tz_convert('America/New_York')\n",
    "\n",
    "    # Session Hours (cf ·Äô·Äæ Local Time ·Äî·Ä¨·Äõ·ÄÆ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Äï·Ä±·Ä∏·Äï·Ä≠·ÄØ·Ä∑·Äõ·Äï·Ä´·Äô·Ää·Ä∫·Åã)\n",
    "    ny = cf.trading_hour('NewYork')\n",
    "    ldn = cf.trading_hour('London')\n",
    "\n",
    "    # London Session (Local Time: 08:00 - 16:00)\n",
    "    df['london_session'] = ((london_time.hour >= ldn['from']) & (london_time.hour < ldn['to'])).astype(int)\n",
    "    \n",
    "    # NY Session (Local Time: 13:00 - 21:00 UTC/GMT) -> (9:00 - 17:00 EST/EDT)\n",
    "    # cf ·Äô·Äæ Local NY Time (·Ä•·Äï·Äô·Ä¨: 9, 17) ·ÄÄ·Ä≠·ÄØ ·Äï·Ä±·Ä∏·Äï·Ä≠·ÄØ·Ä∑·Äõ·Äô·Ää·Ä∫\n",
    "    df['ny_session'] = ((ny_time.hour >= ny['from']) & (ny_time.hour < ny['to'])).astype(int)\n",
    "\n",
    "    df['overlap_session'] = (df['london_session'] & df['ny_session']).astype(int)\n",
    "\n",
    "    # ... (IV. Holiday features ·ÄÄ·Ä≠·ÄØ ·ÄÜ·ÄÄ·Ä∫·Äú·ÄÄ·Ä∫·Äë·Ää·Ä∑·Ä∫·Äû·ÄΩ·ÄÑ·Ä∫·Ä∏·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äû·Ää·Ä∫) ...\n",
    "    \n",
    "    #df['symbol'] = symbol\n",
    "    \n",
    "    # ·Äö·Ä¨·Äö·ÄÆ columns ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Äñ·Äö·Ä∫·Äõ·Äæ·Ä¨·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "    df = df.drop(columns=['hour'], errors='ignore') # minute_block_15 ·Äû·Ää·Ä∫ 1M data ·Äô·Äæ ·Äú·Ä¨·Äú·Äª·Äæ·ÄÑ·Ä∫·Äû·Ä¨ ·Äú·Ä≠·ÄØ·Ä°·Äï·Ä∫·Äû·Ää·Ä∫·Åã 5M ·Äê·ÄΩ·ÄÑ·Ä∫ ·Äô·Äú·Ä≠·ÄØ·Ä°·Äï·Ä∫·Äï·Ä´·Åã\n",
    "    \n",
    "    # Index ·ÄÄ·Ä≠·ÄØ reset ·Äô·Äú·ÄØ·Äï·Ä∫·Äò·Ä≤ ·Äï·Äº·Äî·Ä∫·Äï·Ä≠·ÄØ·Ä∑·Äï·Ä´ (Env ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ Datetime Index ·Äú·Ä≠·ÄØ·Ä°·Äï·Ä∫·Äû·Ää·Ä∫)\n",
    "    return df.reset_index().rename(columns={'index': 'Datetime'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "131d2c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broker Data (00:00 ·Äô·Äæ ·ÄÖ·Äû·Ä±·Ä¨) ·ÄÄ·Ä≠·ÄØ ·ÄÅ·Ä±·Ä´·Ä∫·ÄÜ·Ä≠·ÄØ·Äû·Ää·Ä∑·Ä∫·Ä°·ÄÅ·Ä´\n",
    "# GMT+2/GMT+3 ·ÄÄ·Ä≠·ÄØ ·Ä°·Äú·Ä≠·ÄØ·Ä°·Äú·Äª·Ä±·Ä¨·ÄÄ·Ä∫ ·ÄÄ·Ä≠·ÄØ·ÄÑ·Ä∫·Äê·ÄΩ·Äö·Ä∫·Äï·Ä±·Ä∏·Äõ·Äî·Ä∫\n",
    "axiory_tz = 'Europe/Kiev'  \n",
    "\n",
    "dft = add_time_feature(df, cf=cf, source_tz=axiory_tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "001bb550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>london_session</th>\n",
       "      <th>ny_session</th>\n",
       "      <th>overlap_session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 22:00:00+00:00</td>\n",
       "      <td>1.12117</td>\n",
       "      <td>1.12128</td>\n",
       "      <td>1.12087</td>\n",
       "      <td>1.12114</td>\n",
       "      <td>31.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 22:05:00+00:00</td>\n",
       "      <td>1.12117</td>\n",
       "      <td>1.12124</td>\n",
       "      <td>1.12103</td>\n",
       "      <td>1.12103</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Datetime     open     high      low    close  Volume  \\\n",
       "0 2020-01-01 22:00:00+00:00  1.12117  1.12128  1.12087  1.12114    31.0   \n",
       "1 2020-01-01 22:05:00+00:00  1.12117  1.12124  1.12103  1.12103    44.0   \n",
       "\n",
       "   hour_sin  hour_cos  london_session  ny_session  overlap_session  \n",
       "0      -0.5  0.866025               0           0                0  \n",
       "1      -0.5  0.866025               0           0                0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "623a2a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.to_csv('dft.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c67d003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finta import TA\n",
    "\n",
    "def tech_indicators(df, cf=None):\n",
    "    \"\"\"\n",
    "    Forex RL ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ Price Action·Åä Momentum ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ Long-Term Trend Features ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Äë·Ää·Ä∑·Ä∫·Äû·ÄΩ·ÄÑ·Ä∫·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏·Åã\n",
    "    \"\"\"\n",
    "    price   =   df['close']\n",
    "    sma_fast_period = cf.indicator('sma_fast_period')\n",
    "    sma_mid_period = cf.indicator('sma_mid_period')\n",
    "    sma_slow_period = cf.indicator('sma_slow_period')\n",
    "    atr_period = cf.indicator('atr_period')\n",
    "    rsi_period = cf.indicator('rsi_period')\n",
    "    \n",
    "    \n",
    "    df['return_5'] = df['close'].pct_change(5)  # 5-step return\n",
    "\n",
    "    \n",
    "    df['fast_ma'] = TA.SMA(df, period=sma_fast_period)\n",
    "    df['mid_ma'] = TA.SMA(df, period=sma_mid_period)\n",
    "    df['slow_ma'] = TA.SMA(df, period=sma_slow_period)\n",
    "    df['rsi'] = TA.RSI(df, period=rsi_period).ffill().round(6)\n",
    "    \n",
    "    # trend strength\n",
    "    df['fast_ts']  =   (price - df['fast_ma']) / df['fast_ma']\n",
    "    df['mid_ts']  =   (price - df['mid_ma']) / df['mid_ma']\n",
    "    df['slow_ts']  =   (price - df['slow_ma']) / df['slow_ma']\n",
    "\n",
    "    df['fast_td'] = np.sign(df['fast_ts'])\n",
    "    df['mid_td'] = np.sign(df['mid_ts'])\n",
    "    df['slow_td'] = np.sign(df['slow_ts'])\n",
    "\n",
    "    df['fast_mid_gap'] = (df['fast_ma'] - df['mid_ma']) / df['mid_ma']\n",
    "    df['mid_slow_gap'] = (df['mid_ma'] - df['slow_ma']) / df['slow_ma']\n",
    "    df['fast_slow_gap'] = (df['fast_ma'] - df['slow_ma']) / df['slow_ma']\n",
    "\n",
    "    # --- ·ÅÅ·Åã Volatility Measure (ATR ·ÄÄ·Ä≠·ÄØ Base ·Ä°·Äñ·Äº·ÄÖ·Ä∫ ·Ä°·Äû·ÄØ·Ä∂·Ä∏·Äï·Äº·ÄØ·Äõ·Äî·Ä∫) ---\n",
    "    df['atr_base'] = TA.ATR(df, period=atr_period).ffill()\n",
    "    df['atr_norm'] = df['atr_base'] / price\n",
    "\n",
    "    window=100\n",
    "    df['low_thr'] = df['atr_norm'].rolling(window).quantile(0.33)\n",
    "    df['high_thr'] = df['atr_norm'].rolling(window).quantile(0.66)\n",
    "\n",
    "    # Volatility categories (one-hot)\n",
    "    df['vol_low'] = (df['atr_norm'] < df['low_thr']).astype(int)\n",
    "    df['vol_med'] = ((df['atr_norm'] >= df['low_thr']) &\n",
    "                    (df['atr_norm'] < df['high_thr'])).astype(int)\n",
    "    df['vol_high'] = (df['atr_norm'] >= df['high_thr']).astype(int)\n",
    "\n",
    "    # Momentum\n",
    "    df['momentum_score'] = (df['rsi'] - 50) / 50\n",
    "    # RSI categories one-hot\n",
    "    df['mom_bearish'] = (df['rsi'] < 45).astype(int)\n",
    "    df['mom_neutral'] = ((df['rsi'] >= 45) & (df['rsi'] <= 55)).astype(int)\n",
    "    df['mom_bullish'] = (df['rsi'] > 55).astype(int)\n",
    "\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6d6c5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pd.read_csv('dft.csv')\n",
    "dfi = tech_indicators(dft, cf=cf)\n",
    "dfi.to_csv('dfi.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d96e18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>london_session</th>\n",
       "      <th>ny_session</th>\n",
       "      <th>...</th>\n",
       "      <th>atr_norm</th>\n",
       "      <th>low_thr</th>\n",
       "      <th>high_thr</th>\n",
       "      <th>vol_low</th>\n",
       "      <th>vol_med</th>\n",
       "      <th>vol_high</th>\n",
       "      <th>momentum_score</th>\n",
       "      <th>mom_bearish</th>\n",
       "      <th>mom_neutral</th>\n",
       "      <th>mom_bullish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2020-01-02 14:35:00+00:00</td>\n",
       "      <td>1.11767</td>\n",
       "      <td>1.11802</td>\n",
       "      <td>1.11767</td>\n",
       "      <td>1.11780</td>\n",
       "      <td>518.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.434099</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>2020-01-02 14:40:00+00:00</td>\n",
       "      <td>1.11780</td>\n",
       "      <td>1.11781</td>\n",
       "      <td>1.11712</td>\n",
       "      <td>1.11725</td>\n",
       "      <td>496.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.536659</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>2020-01-02 14:45:00+00:00</td>\n",
       "      <td>1.11727</td>\n",
       "      <td>1.11746</td>\n",
       "      <td>1.11712</td>\n",
       "      <td>1.11727</td>\n",
       "      <td>506.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.525830</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Datetime     open     high      low    close  Volume  \\\n",
       "199  2020-01-02 14:35:00+00:00  1.11767  1.11802  1.11767  1.11780   518.0   \n",
       "200  2020-01-02 14:40:00+00:00  1.11780  1.11781  1.11712  1.11725   496.0   \n",
       "201  2020-01-02 14:45:00+00:00  1.11727  1.11746  1.11712  1.11727   506.0   \n",
       "\n",
       "     hour_sin  hour_cos  london_session  ny_session  ...  atr_norm   low_thr  \\\n",
       "199      -0.5 -0.866025               1           1  ...  0.000370  0.000224   \n",
       "200      -0.5 -0.866025               1           1  ...  0.000393  0.000229   \n",
       "201      -0.5 -0.866025               1           1  ...  0.000393  0.000232   \n",
       "\n",
       "     high_thr  vol_low  vol_med  vol_high  momentum_score  mom_bearish  \\\n",
       "199  0.000272        0        0         1       -0.434099            1   \n",
       "200  0.000273        0        0         1       -0.536659            1   \n",
       "201  0.000276        0        0         1       -0.525830            1   \n",
       "\n",
       "     mom_neutral  mom_bullish  \n",
       "199            0            0  \n",
       "200            0            0  \n",
       "201            0            0  \n",
       "\n",
       "[3 rows x 36 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ca80191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_news_features(df: pd.DataFrame, news_df: pd.DataFrame, window_pre=30, window_post=30):\n",
    "    # üß≠ Ensure datetime index\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        if 'Datetime' in df.columns:\n",
    "            df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "            df = df.set_index('Datetime')\n",
    "        else:\n",
    "            raise ValueError(\"DataFrame must have datetime index or 'time' column\")\n",
    "\n",
    "    \n",
    "    df['pre_news'] = 0.0\n",
    "    df['post_news'] = 0.0\n",
    "    news_df['Start'] = pd.to_datetime(news_df['Start'], utc=True)\n",
    "\n",
    "    for _, row in news_df.iterrows():\n",
    "        news_time  = row['Start']\n",
    "        pre_mask = (df.index >= news_time - pd.Timedelta(minutes=window_pre)) & (df.index < news_time)\n",
    "\n",
    "        if pre_mask.any():\n",
    "            minutes_to_news = (news_time - df.index[pre_mask]).total_seconds() / 60\n",
    "            df.loc[pre_mask, 'pre_news'] = 1 - (minutes_to_news / window_pre)\n",
    "\n",
    "        # --- Post-news: 1 ‚Üí 0 decay ---\n",
    "        post_mask = (df.index > news_time) & (df.index <= news_time + pd.Timedelta(minutes=window_post))\n",
    "        if post_mask.any():\n",
    "            minutes_after_news = (df.index[post_mask] - news_time).total_seconds() / 60\n",
    "            df.loc[post_mask, 'post_news'] = 1 - (minutes_after_news / window_post)\n",
    "\n",
    "\n",
    "        # --- News candle itself ---\n",
    "        exact_mask = (df.index == news_time)\n",
    "        if exact_mask.any():\n",
    "            df.loc[exact_mask, ['pre_news', 'post_news']] = 1.0\n",
    "            \n",
    "    df['pre_news'] = df['pre_news'].clip(0, 1)\n",
    "    df['post_news'] = df['post_news'].clip(0, 1)\n",
    "    # Index ·ÄÄ·Ä≠·ÄØ reset ·Äô·Äú·ÄØ·Äï·Ä∫·Äò·Ä≤ ·Äï·Äº·Äî·Ä∫·Äï·Ä≠·ÄØ·Ä∑·Äï·Ä´ (Env ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ Datetime Index ·Äú·Ä≠·ÄØ·Ä°·Äï·Ä∫·Äû·Ää·Ä∫)\n",
    "    return df.reset_index().rename(columns={'index': 'Datetime'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4331cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"calendar-event-list.csv\")\n",
    "dfi = pd.read_csv(\"dfi.csv\")\n",
    "\n",
    "dfn = add_news_features(dfi, news, window_pre=30, window_post=30)\n",
    "dfn.to_csv('dfn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a51585ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import gymnasium as gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "920424cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer-based model for time series data.\n",
    "    This class projects input features to an embedding, adds positional\n",
    "    encodings, and then processes the inputs using a Transformer encoder.\n",
    "    Finally, a decoder layer is used to produce the output.\n",
    "    Args:\n",
    "        input_size (int): Number of features in the input time series data.\n",
    "        embed_dim (int): Dimensionality of the learned embedding space.\n",
    "        num_heads (int): Number of attention heads in each Transformer layer.\n",
    "        num_layers (int): Number of Transformer encoder layers.\n",
    "        sequence_length (int): Length of the input sequences (time steps).\n",
    "        dropout (float, optional): Dropout probability to apply in the\n",
    "            Transformer encoder layers. Defaults to 0.1.\n",
    "    Attributes:\n",
    "        model_type (str): Identifier for the model type ('Transformer').\n",
    "        embedding (nn.Linear): Linear layer for input feature embedding.\n",
    "        positional_encoding (torch.nn.Parameter): Parameter storing the\n",
    "            positional encodings used to retain temporal information.\n",
    "        transformer_encoder (nn.TransformerEncoder): Stack of Transformer\n",
    "            encoder layers with optional final LayerNorm.\n",
    "        decoder (nn.Linear): Linear layer used to produce the final output\n",
    "            dimensions.\n",
    "    Forward Inputs:\n",
    "        src (torch.Tensor): Input tensor of shape (batch_size, sequence_length,\n",
    "            input_size).\n",
    "    Forward Returns:\n",
    "        torch.Tensor: Output tensor of shape (batch_size, embed_dim) from the\n",
    "            last time step.\n",
    "    Raises:\n",
    "        ValueError: If the model output contains NaN or Inf values, indicating\n",
    "            numerical instability.\n",
    "    \"\"\"\n",
    "    # input_size: Input features ·Ä°·Äõ·Ä±·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ (·Ä•·Äï·Äô·Ä¨ 10·Åä price + SMA/RSI indicators ·ÄÖ·Äê·Ä¨)·Åã\n",
    "    # embed_dim: Internal embedding ·Ä°·Äê·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏·Ä°·Äê·Ä¨ (·Ä•·Äï·Äô·Ä¨ 64·Åä data ·ÄÄ·Ä≠·ÄØ ·Äï·Ä≠·ÄØ·Äî·ÄÄ·Ä∫·Äõ·Äæ·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏ ·Ä°·Ä±·Ä¨·ÄÑ·Ä∫ ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏)·Åã\n",
    "    # num_heads: Attention heads ·Ä°·Äõ·Ä±·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ (multi-head attention ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫·Åä ·Äô·Äê·Ä∞·Ää·ÄÆ ·Ä°·Äî·Ä±·Äî·Ä≤·Ä∑ ·Ä°·Ä¨·Äõ·ÄØ·Ä∂ ·ÄÖ·Ä≠·ÄØ·ÄÄ·Ä∫)·Åã\n",
    "    # num_layers: Encoder layers ·Ä°·Äõ·Ä±·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ (·Ä•·Äï·Äô·Ä¨ 2·Åä ·Äõ·Ä≠·ÄØ·Ä∏·Äõ·Äæ·ÄÑ·Ä∫·Ä∏ ·Äë·Ä¨·Ä∏·Äê·Ä¨)·Åã\n",
    "    # sequence_length: Input sequence ·Ä°·Äõ·Äæ·Ää·Ä∫ (·Ä•·Äï·Äô·Ä¨ 20 timesteps)·Åã\n",
    "    # dropout=0.1: Overfitting ·ÄÄ·Äî·Ä± ·ÄÄ·Ä¨·ÄÄ·ÄΩ·Äö·Ä∫ ·Äê·Ä≤·Ä∑ dropout rate·Åã\n",
    "    def __init__(self, input_size, embed_dim, num_heads, num_layers, sequence_length, dropout=0.1):\n",
    "\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        # Input embedding\n",
    "        self.embedding = nn.Linear(input_size, embed_dim)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, sequence_length, embed_dim))\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dropout=dropout,\n",
    "            norm_first=True\n",
    "        )\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "            norm=nn.LayerNorm(embed_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "        # Decoder (optional feature projection)\n",
    "        self.decoder = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, src, aggregate_last_n: int = 1):\n",
    "        \"\"\"\n",
    "        src: (batch_size, sequence_length, input_size)\n",
    "        aggregate_last_n: int, number of last timesteps to aggregate\n",
    "        \"\"\"\n",
    "        device = src.device  # Use input device dynamically\n",
    "        \n",
    "        # Embedding + positional encoding\n",
    "        x = self.embedding(src) + self.positional_encoding.to(device)\n",
    "        # Transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        # Decoder projection\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        # Check for NaN/Inf\n",
    "        if torch.isnan(x).any() or torch.isinf(x).any():\n",
    "            raise ValueError(\"Transformer output contains NaN or Inf values\")\n",
    "\n",
    "        # Aggregate last N timesteps (default = last timestep)\n",
    "        if aggregate_last_n == 1:\n",
    "            return x[:, -1, :]\n",
    "        else:\n",
    "            return x[:, -aggregate_last_n:, :].mean(dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "43ac009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCombinedExtractor(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    Custom feature extractor for time series data using a Transformer.\n",
    "    - LayerNorm input normalization for stability\n",
    "    - Device-agnostic (works CPU/GPU)\n",
    "    - Supports optional aggregation of last N timesteps\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Box, sequence_length: int, embed_dim: int = 64, num_heads: int = 2, num_layers: int = 2):\n",
    "        super(CustomCombinedExtractor, self).__init__(observation_space, features_dim=embed_dim)\n",
    "        num_features = observation_space.shape[1]  # input feature count\n",
    "\n",
    "        # Layer normalization before Transformer\n",
    "        self.layernorm_before = nn.LayerNorm(num_features)\n",
    "\n",
    "        # TimeSeriesTransformer\n",
    "        self.transformer = TimeSeriesTransformer(\n",
    "            input_size=num_features,\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            num_layers=num_layers,\n",
    "            sequence_length=sequence_length\n",
    "        )\n",
    "\n",
    "    def forward(self, observations, aggregate_last_n: int = 1):\n",
    "        \"\"\"\n",
    "        observations: (batch_size, sequence_length, num_features)\n",
    "        aggregate_last_n: number of last timesteps to aggregate (default 1)\n",
    "        \"\"\"\n",
    "        device = observations.device\n",
    "        # Normalize input\n",
    "        x = self.layernorm_before(observations.float().to(device))\n",
    "\n",
    "        # Pass through Transformer\n",
    "        x = self.transformer(x, aggregate_last_n=aggregate_last_n)\n",
    "\n",
    "        # Check for NaN / Inf\n",
    "        if torch.isnan(x).any() or torch.isinf(x).any():\n",
    "            raise ValueError(\"Transformer output contains NaN or Inf values\")\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f3d41374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import numpy as np\n",
    "\n",
    "class TrainingMetricsCallback(BaseCallback):\n",
    "    def __init__(self, check_freq=1000, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.sharpe_ratios = []\n",
    "        self.drawdowns = []\n",
    "        self.episode_count = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Track metrics only when episodes complete\n",
    "        if \"sharpe\" in self.locals['infos'][0] and \"drawdown\" in self.locals['infos'][0]:\n",
    "            self.episode_count += 1\n",
    "            self.sharpe_ratios.append(self.locals['infos'][0]['sharpe'])\n",
    "            self.drawdowns.append(self.locals['infos'][0]['drawdown'])\n",
    "\n",
    "            # Log to tensorboard every N episodes\n",
    "            if self.episode_count % 10 == 0:\n",
    "                self.logger.record('train/mean_sharpe', np.mean(self.sharpe_ratios[-10:]))\n",
    "                self.logger.record('train/max_drawdown', np.mean(self.drawdowns[-10:]))\n",
    "                self.logger.record('train/episodes', self.episode_count)\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ae615ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_to_file(**kwargs):\n",
    "    log_header                  =   kwargs.get(\"log_header\",False)\n",
    "    log_filename                =   kwargs.get(\"log_filename\",\"\")\n",
    "    printout                    =   kwargs.get(\"printout\",False)\n",
    "    balance                     =   kwargs.get(\"balance\")\n",
    "    balance_initial             =   kwargs.get(\"balance_initial\")\n",
    "    transaction_close_this_step =   kwargs.get(\"transaction_close_this_step\",[])\n",
    "    done_information            =   kwargs.get(\"done_information\",\"\")\n",
    "    profit                      =   balance - balance_initial\n",
    "\n",
    "    tr_lines                    =   \"\"\n",
    "    tr_lines_comma              =   \"\"\n",
    "    _header                     =   \"\"\n",
    "    _header_comma               =   \"\"\n",
    "    if log_header:\n",
    "        _header = f'{\"Ticket\":>8} {\"Type\":>4} {\"ActionStep\":16} \\\n",
    "                    {\"ActionPrice\":>12} {\"CloseStep\":8} {\"ClosePrice\":>12} \\\n",
    "                    {\"OpenBal\":>12} {\"CloseBal\":>12} {\"Status\":8} {\"Info\":>8} {\"PIPS\":>6} {\"SL\":>6} {\"PT\":>6} {\"DeltaStep\":8}\\n'\n",
    "\n",
    "\n",
    "        _header_comma = f'{\"Ticket,Type,ActionTime,ActionStep,ActionPrice,CloseTime,ClosePrice, OpenBal, CloseBal, Status, Info, PIPS,SL,PT,CloseStep,DeltaStep\"}\\n'\n",
    "    if transaction_close_this_step:\n",
    "        for _tr in transaction_close_this_step:\n",
    "            if _tr[\"CloseStep\"] >=0:\n",
    "                tr_lines += f'{_tr[\"Ticket\"]:>8} {_tr[\"Type\"]:>4} {_tr[\"ActionStep\"]:16} \\\n",
    "                    {_tr[\"ActionPrice\"]:.5f} {_tr[\"CloseStep\"]:8} {_tr[\"ClosePrice\"]:.5f} \\\n",
    "                    {_tr[\"OpenBal\"]:.2f} {_tr[\"CloseBal\"]:.2f} {_tr[\"Status\"]:8}  {_tr[\"Info\"]:>8}  {_tr[\"PIPS\"]:4.0f} {_tr[\"SL\"]:4.0f} {_tr[\"PT\"]:4.0f} {_tr[\"DeltaStep\"]:8}\\n'\n",
    "\n",
    "                tr_lines_comma += f'{_tr[\"Ticket\"]},{_tr[\"Type\"]},{_tr[\"ActionTime\"]},{_tr[\"ActionStep\"]}, \\\n",
    "                    {_tr[\"ActionPrice\"]},{_tr[\"CloseTime\"]},{_tr[\"ClosePrice\"]}, \\\n",
    "                    {_tr[\"OpenBal\"]},{_tr[\"CloseBal\"]}, {_tr[\"Status\"]},{_tr[\"Info\"]},{_tr[\"PIPS\"]},{_tr[\"SL\"]},{_tr[\"PT\"]},{_tr[\"CloseStep\"]},{_tr[\"DeltaStep\"]}\\n'\n",
    "\n",
    "    log = _header_comma + tr_lines_comma\n",
    "    # log = f\"Step: {current_step}   Balance: {balance}, Profit: {profit} \\\n",
    "    #     MDD: {max_draw_down_pct}\\n{tr_lines_comma}\\n\"\n",
    "    if done_information:\n",
    "        log += done_information\n",
    "    if log:\n",
    "        # os.makedirs(log_filename, exist_ok=True)\n",
    "        dir_path = os.path.dirname(log_filename)\n",
    "        if dir_path and not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "        with open(log_filename, 'a+') as _f:\n",
    "            _f.write(log)\n",
    "            _f.close()\n",
    "\n",
    "    tr_lines = _header + tr_lines\n",
    "    if printout and tr_lines:\n",
    "        print(tr_lines)\n",
    "        if done_information:\n",
    "            print(done_information)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b020729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForexTradingEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, file, cf, asset, logger_show=False, scaler=None):\n",
    "        # scaler parameter ·ÄÄ·Ä≠·ÄØ ·Äë·Äï·Ä∫·Äë·Ää·Ä∑·Ä∫·Äõ·Äï·Ä´·Äô·Ää·Ä∫·Åã ·Åé·ÄÑ·Ä∫·Ä∏·Äû·Ää·Ä∫ Global Train Set ·Äê·ÄΩ·ÄÑ·Ä∫ Fit ·Äú·ÄØ·Äï·Ä∫·Äë·Ä¨·Ä∏·Äû·Ä±·Ä¨ Scaler ·Äñ·Äº·ÄÖ·Ä∫·Äõ·Äô·Ää·Ä∫·Åã\n",
    "        # 'scaler' ·Äû·Ää·Ä∫ Global Train Set ·Äê·ÄΩ·ÄÑ·Ä∫ Fit ·Äú·ÄØ·Äï·Ä∫·Äë·Ä¨·Ä∏·Äû·Ä±·Ä¨ StandardScaler instance ·Äñ·Äº·ÄÖ·Ä∫·Äõ·Äô·Ää·Ä∫·Åã\n",
    "        self.scaler = scaler\n",
    "        if self.scaler is None:\n",
    "             raise ValueError(\"A fitted StandardScaler instance must be provided to the Environment.\")\n",
    "        super(ForexTradingEnv, self).__init__()\n",
    "        # ·ÄÄ·Ä≠·Äî·Ä∫·Ä∏·Äõ·Äæ·ÄÑ·Ä∫·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·ÄÖ·Äê·ÄÑ·Ä∫·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äû·Ää·Ä∫·Åã\n",
    "        self._initialize_parameters(file, cf, asset, logger_show)\n",
    "        # [NEW ACTION] Raw Data ·ÄÄ·Ä≠·ÄØ Scaler ·Äñ·Äº·ÄÑ·Ä∑·Ä∫ Transform ·Äú·ÄØ·Äï·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "        # OHLCV features ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ Scaling ·Äú·ÄØ·Äï·Ä∫·Äõ·Äï·Ä´·Äô·Äö·Ä∫ (·Ä•·Äï·Äô·Ä¨: open, high, low, close, vol, atr_base, log_returns, price_norm, etc.)\n",
    "        # Scaling ·Äú·ÄØ·Äï·Ä∫·Äõ·Äî·Ä∫ features ·Äô·Äª·Ä¨·Ä∏·ÄÖ·Ä¨·Äõ·ÄÑ·Ä∫·Ä∏·ÄÄ·Ä≠·ÄØ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äï·Ä´·Åã\n",
    "        # ·Ä§·Äî·Ä±·Äõ·Ä¨·Äê·ÄΩ·ÄÑ·Ä∫ ·ÄÄ·Äª·ÄΩ·Äî·Ä∫·Äê·Ä±·Ä¨·Ä∫·Äê·Ä≠·ÄØ·Ä∑·Äû·Ää·Ä∫ OHLCV ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ Indicator Features ·Ä°·Ä¨·Ä∏·Äú·ÄØ·Ä∂·Ä∏·ÄÄ·Ä≠·ÄØ Scaling ·Äú·ÄØ·Äï·Ä∫·Äô·Ää·Ä∫·Äü·ÄØ ·Äö·Ä∞·ÄÜ·Äï·Ä´·Äô·Ää·Ä∫·Åã\n",
    "        self._scale_data()\n",
    "\n",
    "        # Action ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ Observation Spaces ·ÄÄ·Ä≠·ÄØ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äû·Ää·Ä∫·Åã\n",
    "        self._initialize_spaces()\n",
    "        # Scaled Data ·ÄÄ·Ä≠·ÄØ Numpy Array ·Ä°·Äñ·Äº·ÄÖ·Ä∫ ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Äº·ÄÆ·Ä∏ ·ÄÖ·ÄÖ·Ä∫·Äï·Ä´·Åã\n",
    "        data_check = self.data[self.features_scaled].values\n",
    "\n",
    "        if np.isnan(data_check).any() or np.isinf(data_check).any():\n",
    "            # ·Ä§·Äî·Ä±·Äõ·Ä¨·Äê·ÄΩ·ÄÑ·Ä∫ Debugging ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ print ·Äë·ÄØ·Äê·Ä∫·Äï·Ä´\n",
    "            print(\"FATAL ERROR: NaN/Inf detected in scaled data!\")\n",
    "            # NaN/Inf ·Äõ·Äæ·Ä≠·Äû·Ä±·Ä¨ Column ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·Äû·Ä≠·ÄÅ·Äª·ÄÑ·Ä∫·Äõ·ÄÑ·Ä∫:\n",
    "            # print(self.data[self.features_scaled].isnull().any())\n",
    "            raise ValueError(\"Invalid values detected in environment data (NaN/Inf)!\")\n",
    "\n",
    "\n",
    "        # Environment ·ÄÄ·Ä≠·ÄØ ·Ä°·ÄÖ·Äï·Äº·ÄØ·Ä°·ÄÅ·Äº·Ä±·Ä°·Äî·Ä±·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Äî·Ä∫·Äú·Ää·Ä∫·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äû·Ää·Ä∫·Åã\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def _scale_data(self):\n",
    "        \"\"\"Raw Data (self.data) ·Äô·Äæ features ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ Global Scaler ·Äñ·Äº·ÄÑ·Ä∑·Ä∫ Transform ·Äú·ÄØ·Äï·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\"\"\"\n",
    "        # [self.features] ·Äê·ÄΩ·ÄÑ·Ä∫ OHLCV ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ Indicator Features ·Ä°·Ä¨·Ä∏·Äú·ÄØ·Ä∂·Ä∏ ·Äï·Ä´·Äù·ÄÑ·Ä∫·Äõ·Äô·Ää·Ä∫·Åã\n",
    "        # [NOTE]: 'time' ·ÄÄ·Ä≤·Ä∑·Äû·Ä≠·ÄØ·Ä∑·Äû·Ä±·Ä¨ Non-Numeric features ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ self.features ·Äê·ÄΩ·ÄÑ·Ä∫ ·Äô·Äï·Ä´·Äù·ÄÑ·Ä∫·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏ ·Äû·Ä±·ÄÅ·Äª·Ä¨·Äï·Ä´·ÄÖ·Ä±·Åã\n",
    "        if not self.scaler.scale_.any():\n",
    "             logger.warning(\"Scaler is not properly fitted. Continuing with raw data.\")\n",
    "             return\n",
    "\n",
    "        # Scaled Features ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ Original DataFrame ·Äê·ÄΩ·ÄÑ·Ä∫ ·Ä°·ÄÖ·Ä¨·Ä∏·Äë·Ä≠·ÄØ·Ä∏·Äû·Ää·Ä∫·Åã\n",
    "        # Scaled Data ·Äê·ÄΩ·ÄÑ·Ä∫ NaN/Inf ·Äô·Äñ·Äº·ÄÖ·Ä∫·ÄÖ·Ä±·Äõ·Äî·Ä∫ Data Frame ·ÄÄ·Ä≠·ÄØ ·ÄÄ·Äº·Ä≠·ÄØ·Äê·ÄÑ·Ä∫·ÄÖ·ÄÖ·Ä∫·ÄÜ·Ä±·Ä∏·Äë·Ä¨·Ä∏·Äû·ÄÑ·Ä∑·Ä∫·Äï·Ä´·Äû·Ää·Ä∫·Åã\n",
    "        self.data[self.features_scaled] = self.scaler.transform(self.data[self.features_scaled])\n",
    "        # logger.info(f\"Data scaled successfully using fitted StandardScaler.\")\n",
    "\n",
    "    # ·ÄÄ·Ä≠·Äî·Ä∫·Ä∏·Äõ·Äæ·ÄÑ·Ä∫·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ ·ÄÖ·Äê·ÄÑ·Ä∫·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äû·Ää·Ä∫·Åã\n",
    "    def _initialize_parameters(self, file, cf, asset, logger_show):\n",
    "        # Params to variables\n",
    "        self.csv_file               =   file\n",
    "        self.cf                     =   cf\n",
    "        self.symbol_col             =   asset\n",
    "        self.features_scaled        =   self.cf.env_parameters('features_scaled') # Time-Series Features List\n",
    "        self.features_unscaled      =   self.cf.env_parameters('features_unscaled')\n",
    "        self.features_filter        =   self.cf.env_parameters('features_filter')\n",
    "        # Scaled Data Frame ·Åè Feature List\n",
    "        self.obs_features           =   self.features_scaled + self.features_unscaled\n",
    "        self.sequence_length        =   self.cf.data_processing_parameters(\"sequence_length\") # Transformer Lookback Window (100)\n",
    "        self.logger_show            =   logger_show\n",
    "\n",
    "        self.data_raw = pd.read_csv(file)\n",
    "        # if 'time' in self.data_raw.columns:\n",
    "        #     self.data_raw = self.data_raw.set_index(pd.to_datetime(self.data_raw['time'], utc=True)).drop(columns=['time'])\n",
    "        # Index ·ÄÄ·Ä≠·ÄØ datetime type ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏\n",
    "        self.data_raw.index = pd.to_datetime(self.data_raw.index)\n",
    "\n",
    "\n",
    "        # self.data ·ÄÄ·Ä≠·ÄØ Scaling ·Äú·ÄØ·Äï·Ä∫·Äõ·Äî·Ä∫ Copy ·Äö·Ä∞·Äï·Ä´·Äô·Ää·Ä∫·Åã\n",
    "        self.data = self.data_raw.copy()\n",
    "\n",
    "        # We use sequence transformer, so max steps will be this\n",
    "        self.max_steps              =   len(self.data) - self.sequence_length - 1\n",
    "\n",
    "        # Configs to variables\n",
    "        # Agent ·ÄÄ Action ·ÄÄ Continuous Action ·ÄÄ·Ä≠·ÄØ Discrete Action ·Äû·Ä≠·ÄØ·Ä∑·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Ä±·Ä∏·Äû·Ä±·Ä¨ threshold\n",
    "        self.action_threshold       =   self.cf.env_parameters('action_threshold')\n",
    "        self.balance_initial        =   self.cf.env_parameters('balance')\n",
    "\n",
    "        # position close ·Äô·Äñ·Äº·ÄÖ·Ä∫·Äû·Ä±·Ä∏·Äõ·ÄÑ·Ä∫\n",
    "        # buy ·Äë·Ä¨·Ä∏·Äï·Äº·ÄÆ·Ä∏ price up ·Äñ·Äº·ÄÖ·Ä∫·Äî·Ä±·Äõ·ÄÑ·Ä∫ reward ·Äï·Ä±·Ä∏·Åã sell ·Äë·Ä¨·Ä∏·Äï·Äº·ÄÆ·Ä∏ price down ·Äñ·Äº·ÄÖ·Ä∫·Äî·Ä±·Äõ·ÄÑ·Ä∫ reward ·Äï·Ä±·Ä∏\n",
    "        # position management ·Äô·Äæ·Ä¨·Äú·Ää·Ä∫·Ä∏ ·Äû·ÄØ·Ä∂·Ä∏·Åã\n",
    "        # buy ·Äô·Äæ·Ä¨ ·Äô·Äº·Äê·Ä∫·Äî·Ä±·Äõ·ÄÑ·Ä∫ tp ·Ä°·Äï·Ä±·Ä´·Ä∫·Äõ·ÄΩ·Ä±·Ä∑ sl ·Ä°·Äï·Ä±·Ä´·Ä∫·Äõ·ÄΩ·Ä±·Ä∑·Åã  ·Äõ·Äæ·ÄØ·Ä∂·Ä∏·Äî·Ä±·Äõ·ÄÑ·Ä∫ tp ·Ä°·Ä±·Ä¨·ÄÄ·Ä∫·Äõ·ÄΩ·Ä±·Ä∑ sl ·Ä°·Äï·Ä±·Ä´·Ä∫·Äê·ÄÑ·Ä∫,\n",
    "        # sell ·Äô·Äæ·Ä¨ ·Äô·Äº·Äê·Ä∫·Äî·Ä±·Äõ·ÄÑ·Ä∫ tp ·Ä°·Ä±·Ä¨·ÄÄ·Ä∫·Äõ·ÄΩ·Ä±·Ä∑ sl ·Ä°·Ä±·Ä¨·ÄÄ·Ä∫·Äõ·ÄΩ·Ä±·Ä∑·Åã ·Äõ·Äæ·ÄØ·Ä∂·Ä∏·Äî·Ä±·Äõ·ÄÑ·Ä∫ tp ·Ä°·Äï·Ä±·Ä´·Ä∫·Äê·ÄÑ·Ä∫ sl ·Ä°·Ä±·Ä¨·ÄÄ·Ä∫·ÄÅ·Äª\n",
    "        self.good_position_reward_scale = self.cf.env_parameters(\"good_position_reward_scale\") # ·Ä•·Äï·Äô·Ä¨: 0.01\n",
    "        # ·Äõ·Ää·Ä∫·Äõ·ÄΩ·Äö·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫ ·ÅÇ: SL/PT Trailing ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Äê·Äî·Ä∫·Äñ·Ä≠·ÄØ·Ä∏ (Move Step Size)\n",
    "        self.trailing_distance = self.cf.env_parameters(\"trailing_stop_distance_points\")\n",
    "\n",
    "        # ·Ä°·Äõ·Äæ·ÄØ·Ä∂·Ä∏·Äî·Ä≤·Ä∑·Ä°·Äô·Äº·Äê·Ä∫ ·Äô·Äª·Äæ·Äê·Äô·Äæ·ÄØ·Äõ·Äæ·Ä≠·Äê·Ä≤·Ä∑ trading performance ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Äï·Ä±·Ä∏·Äê·Ä≤·Ä∑ bonus reward 0.01\n",
    "        # self.consistency_reward = self.cf.env_parameters(\"consistency_reward\")\n",
    "        self.stop_loss = self.cf.symbol(self.symbol_col, \"stop_loss_max\")\n",
    "        self.profit_taken = self.cf.symbol(self.symbol_col, \"profit_taken_max\")\n",
    "        self.point = self.cf.symbol(self.symbol_col, \"point\")\n",
    "        self.transaction_fee = self.cf.symbol(self.symbol_col, \"transaction_fee\")\n",
    "        self.over_night_penalty = self.cf.symbol(self.symbol_col, \"over_night_penalty\")\n",
    "        self.max_current_holding = self.cf.symbol(self.symbol_col, \"max_current_holding\")\n",
    "        # Drawdown Penalty Factor\n",
    "        self.drawdown_penalty_factor = self.cf.env_parameters(\"drawdown_penalty_factor\")\n",
    "        self.margin_requirement = self.cf.env_parameters('margin_requirement')\n",
    "\n",
    "\n",
    "    # Action ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ Observation Spaces ·ÄÄ·Ä≠·ÄØ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äû·Ää·Ä∫·Åã\n",
    "    def _initialize_spaces(self):\n",
    "        # Continuous actions: [1 -> 0.5] LONG | [0.5 -> -0.5] HOLD |[-0.5 -> -1] SHORT\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1,\n",
    "            high=1,\n",
    "            shape=(1,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # [MODIFIED] Transformer Observation Space: Time Series (100) + Context (4)\n",
    "        N_FEATURES_TS = len(self.obs_features)\n",
    "        N_FEATURES_CONTEXT = 4 # [Equity, Drawdown, Open_Pos_Ratio, Time_Context]\n",
    "\n",
    "        # [MODIFIED] Observation Space (Time Series Sequence Only)\n",
    "        # Transformer ·Äû·ÄØ·Ä∂·Ä∏·Äë·Ä¨·Ä∏·Äû·Ä±·Ä¨ features ·Äê·ÄΩ·Ä±·Äõ·Ä≤·Ä∑ previous sequence length candle ·ÄÄ·Ä≠·ÄØ·Äï·Ä´ ·Äê·Äï·Äº·Ä≠·ÄØ·ÄÑ·Ä∫·Äê·Ää·Ä∫·Ä∏·ÄÄ·Äº·Ää·Ä∑·Ä∫\n",
    "        obs_shape = (self.sequence_length, N_FEATURES_TS + N_FEATURES_CONTEXT)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf, # Scaled Data ·Äô·Äª·Ä¨·Ä∏·Äû·Ää·Ä∫ Theoretical Inf/ -Inf ·Äõ·Äæ·Ä≠·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äû·Ä±·Ä¨·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∑·Ä∫ np.inf ·ÄÄ·Ä≠·ÄØ ·Äû·ÄØ·Ä∂·Ä∏·Äï·Ä´\n",
    "            shape=obs_shape,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    # Environment ·ÄÄ·Ä≠·ÄØ ·Ä°·ÄÖ·Äï·Äº·ÄØ·Ä°·ÄÅ·Äº·Ä±·Ä°·Äî·Ä±·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Äî·Ä∫·Äú·Ää·Ä∫·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äû·Ää·Ä∫·Åã\n",
    "    def reset(self, *, seed = None, options = None):\n",
    "        super().reset(seed=seed, options=options)\n",
    "\n",
    "        self.ticket_id          =   0\n",
    "        self.ttl_rewards        =   0 # total rewards\n",
    "\n",
    "        self.balance            =   self.balance_initial\n",
    "        self.positions          =   []\n",
    "\n",
    "        # equity tracking\n",
    "        self.equity_curve       =   [self.balance_initial] # Starting with initial balance\n",
    "        # ·Ä°·Äô·Äº·ÄÑ·Ä∑·Ä∫·ÄÜ·ÄØ·Ä∂·Ä∏·Äõ·Ä±·Ä¨·ÄÄ·Ä∫·Äñ·Ä∞·Ä∏·Äê·Ä≤·Ä∑ eq value\n",
    "        self.peak_equity        =   self.balance_initial # Start with initial balance as peak\n",
    "\n",
    "        self.max_drawdown       =   0.0\n",
    "        self.current_drawdown   =   0.0\n",
    "\n",
    "        # transformer ·Ä°·Äû·ÄØ·Ä∂·Ä∏·Äï·Äº·ÄØ·Äë·Ä¨·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∑\n",
    "        self.current_step       =   self.sequence_length\n",
    "        logger.info(f\"--- Environment reset. Starting at step {self.current_step} --total rewards: {self.ttl_rewards}\")\n",
    "\n",
    "        observation             =   self._next_observation()\n",
    "        info                    =   {}\n",
    "        return  observation, info\n",
    "\n",
    "\n",
    "# AI model ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Äú·ÄÄ·Ä∫·Äõ·Äæ·Ä≠ market condition ·ÄÄ·Ä≠·ÄØ·ÄÄ·Ä≠·ÄØ·Äö·Ä∫·ÄÖ·Ä¨·Ä∏·Äï·Äº·ÄØ·Äê·Ä≤·Ä∑ observation data ·ÄÄ·Ä≠·ÄØ ·Äï·Äº·ÄÑ·Ä∫·ÄÜ·ÄÑ·Ä∫·Äï·Ä±·Ä∏·Äñ·Ä≠·ÄØ·Ä∑·Äñ·Äº·ÄÖ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫·Åã\n",
    "    def _next_observation(self):\n",
    "\n",
    "        # 1. Time Series Observation (Scaled Data)\n",
    "        obs_ts = self.data.iloc[\n",
    "            self.current_step - self.sequence_length: self.current_step\n",
    "        ][self.obs_features].values # Shape: (100, N_Features_TS)\n",
    "\n",
    "        # 2. Account State (Non-Time-Series / Context Vector)\n",
    "        current_equity = self._calculate_current_equity()\n",
    "        open_positions_count = sum(1 for p in self.positions if p['Status'] == 0)\n",
    "\n",
    "        obs_context = np.array([\n",
    "            current_equity / self.balance_initial, # 1. Normalized Equity\n",
    "            self.current_drawdown,                 # 2. Current Drawdown (Percentage)\n",
    "            open_positions_count / self.max_current_holding, # 3. Open Positions Ratio\n",
    "            self.data.iloc[self.current_step]['hour_cos']   # 4. Time Context (Scaled)\n",
    "        ], dtype=np.float32) # Shape: (4,)\n",
    "\n",
    "        # 3. Final Observation Construction (Time Series + Context)\n",
    "\n",
    "        # Context features ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ Sequence Length (100) ·Ä°·Äú·Ä≠·ÄØ·ÄÄ·Ä∫ ·Äñ·Äº·Äî·Ä∑·Ä∫·ÄÄ·Äª·ÄÄ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏ (Broadcasting)\n",
    "        obs_context_expanded = np.tile(obs_context, (self.sequence_length, 1)) # Shape: (100, 4)\n",
    "\n",
    "        # Horizontal Stack (Sequence length, N_Features_TS + N_Features_Context)\n",
    "        obs_final = np.hstack([obs_ts, obs_context_expanded])\n",
    "\n",
    "        # Data Validation ·ÄÖ·ÄÖ·Ä∫·ÄÜ·Ä±·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "        # NaN (Not a Number) values ·Äõ·Äæ·Ä≠·Äô·Äõ·Äæ·Ä≠·ÄÖ·ÄÖ·Ä∫·Äô·Äö·Ä∫\n",
    "        # Infinite values ·Äõ·Äæ·Ä≠·Äô·Äõ·Äæ·Ä≠·ÄÖ·ÄÖ·Ä∫·Äô·Äö·Ä∫\n",
    "        # Invalid data ·Äõ·Äæ·Ä≠·Äõ·ÄÑ·Ä∫ error ·Äï·Äº·Äô·Äö·Ä∫\n",
    "        if np.isnan(obs_final).any() or np.isinf(obs_final).any():\n",
    "            logger.error(f\"Invalid observation at step {self.current_step}\")\n",
    "            raise ValueError(f\"Invalid observation at step {self.current_step}\")\n",
    "\n",
    "\n",
    "        # NumPy Array ·Äï·Äº·Äî·Ä∫·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "        # GPU memory ‚Üí CPU memory ·Äï·Äº·Äî·Ä∫·Äõ·ÄΩ·Äæ·Ä±·Ä∑·Äô·Äö·Ä∫\n",
    "        # PyTorch tensor ‚Üí NumPy array ·Äï·Äº·Äî·Ä∫·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äô·Äö·Ä∫\n",
    "        # Gym environment ·ÄÄ NumPy arrays ·ÄÄ·Ä≠·ÄØ ·Äï·Ä≠·ÄØ·ÄÄ·Äº·Ä≠·ÄØ·ÄÄ·Ä∫·Äê·Äö·Ä∫·Åã Memory management ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äê·Äö·Ä∫\n",
    "        return obs_final\n",
    "\n",
    "\n",
    "        # # 4. PyTorch/Device Conversion and Validation\n",
    "        # try:\n",
    "        #     # NumPy array ‚Üí PyTorch tensor ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äô·Äö·Ä∫\n",
    "        #     obs_tensor = torch.tensor(obs_final, dtype=torch.float32).to(device)\n",
    "        #     # Data Validation ·ÄÖ·ÄÖ·Ä∫·ÄÜ·Ä±·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "        #     if torch.isnan(obs_tensor).any() or torch.isinf(obs_tensor).any():\n",
    "        #         logger.error(f\"Invalid observation (NaN/Inf) at step {self.current_step}\")\n",
    "        #         raise ValueError(f\"Invalid observation (NaN/Inf) at step {self.current_step}\")\n",
    "        #     return obs_tensor.cpu().numpy()\n",
    "\n",
    "        # except NameError:\n",
    "        #     # Torch ·ÄÄ·Ä≠·ÄØ ·Äô·Äû·ÄØ·Ä∂·Ä∏·Äï·Ä´·ÄÄ NumPy ·ÄÄ·Ä≠·ÄØ·Äû·Ä¨ ·Äï·Äº·Äî·Ä∫·Äï·Ä±·Ä∏·Äï·Ä´·Åã\n",
    "        #     if np.isnan(obs_final).any() or np.isinf(obs_final).any():\n",
    "        #         logger.error(f\"Invalid observation (NaN/Inf) at step {self.current_step}\")\n",
    "        #         raise ValueError(f\"Invalid observation (NaN/Inf) at step {self.current_step}\")\n",
    "        #     return obs_final # Final NumPy array\n",
    "\n",
    "\n",
    "    def _ray_mask(self, a, c, bounds):\n",
    "        \"\"\"\n",
    "        Ray Mask ·Äú·ÄØ·Äï·Ä∫·Äï·Ä´·Åã\n",
    "        a: ·Äô·Ä∞·Äõ·ÄÑ·Ä∫·Ä∏ action (np.array)\n",
    "        c: ·Äû·ÄÄ·Ä∫·ÄÜ·Ä≠·ÄØ·ÄÑ·Ä∫·Äõ·Ä¨ ·Ä°·ÄÖ·ÄØ ·Äõ·Ä≤·Ä∑ ·Ä°·Äú·Äö·Ä∫·Äó·Äü·Ä≠·ÄØ\n",
    "        A_r_boundary_func: lambda_A_r ·Äê·ÄΩ·ÄÄ·Ä∫·Äê·Ä≤·Ä∑ func\n",
    "        A_boundary_func: lambda_A ·Äê·ÄΩ·ÄÄ·Ä∫·Äê·Ä≤·Ä∑ func\n",
    "        \"\"\"\n",
    "        if np.allclose(a, c):\n",
    "            return c  # ·Ä°·Äú·Äö·Ä∫·Äô·Äæ·Ä¨ ·ÄÜ·Ä≠·ÄØ ·Äô·Äõ·ÄΩ·Äæ·Ä±·Ä∑\n",
    "\n",
    "        direction = a - c\n",
    "        norm_dir = direction / np.linalg.norm(direction)\n",
    "\n",
    "        lambda_A_r = bounds[1] - c if norm_dir > 0 else c - bounds[0]\n",
    "        lambda_A = 1 - c if norm_dir > 0 else c - (-1)\n",
    "\n",
    "\n",
    "        if lambda_A_r <= 0 or lambda_A <= 0:\n",
    "            return c  # ·Ä°·Äô·Äæ·Ä¨·Ä∏ ·Äõ·Äæ·Ä±·Ä¨·ÄÑ·Ä∫\n",
    "\n",
    "        scale = lambda_A_r / lambda_A\n",
    "        a_r = c + scale * direction\n",
    "        return np.clip(a_r, -1, 1)  # Action space ·ÄÄ·Äî·Ä∫·Ä∑·Äû·Äê·Ä∫\n",
    "\n",
    "\n",
    "    def _get_action_name(self, _action, ma_first, ma_slow):\n",
    "\n",
    "        c = 0.0  # ·Ä°·Äú·Äö·Ä∫·Äó·Äü·Ä≠·ÄØ (hold)\n",
    "        if ma_first > ma_slow:  # Uptrend: buy only [0, 1]\n",
    "            bounds = [0, 1]\n",
    "        else:  # Downtrend: sell only [-1, 0]\n",
    "            bounds = [-1, 0]\n",
    "\n",
    "        a_masked = self._ray_mask(_action, c, bounds)\n",
    "\n",
    "        \"\"\"Convert continuous action to discrete action name\"\"\"\n",
    "        if a_masked >= self.action_threshold:\n",
    "            return \"BUY\"\n",
    "        elif a_masked <= -self.action_threshold:\n",
    "            return \"SELL\"\n",
    "        else:\n",
    "            return \"HOLD\"\n",
    "\n",
    "    def step(self, action):\n",
    "        # self.data ·Äû·Ää·Ä∫ Index ·Äê·ÄΩ·ÄÑ·Ä∫ 'time' ·ÄÄ·Ä≠·ÄØ ·Äë·Ä¨·Ä∏·Äõ·Äæ·Ä≠·Äï·Äº·ÄÆ·Ä∏ drop ·Äú·ÄØ·Äï·Ä∫·Äë·Ä¨·Ä∏·Äû·Ä±·Ä¨·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∑·Ä∫·Åä Index ·Äô·Äæ time ·ÄÄ·Ä≠·ÄØ ·Äö·Ä∞·Äõ·Äî·Ä∫·Äú·Ä≠·ÄØ·Äû·Ää·Ä∫·Åã\n",
    "        current_row_raw = self.data_raw.iloc[self.current_step]\n",
    "\n",
    "        # Unscaled Price Features\n",
    "        _o, _h, _l, _c, ma_fast, ma_slow = current_row_raw[['open', 'high', 'low', 'close', 'ma_fast', 'ma_slow']]\n",
    "\n",
    "        _t = self.data.index[self.current_step] # Get time from index\n",
    "        reward                      =   0 # ·Äí·ÄÆ step ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·ÄÖ·ÄØ·ÄÖ·ÄØ·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏ reward\n",
    "        position_reward             =   0 # Position ·Äï·Ä≠·Äê·Ä∫·Äõ·ÄÑ·Ä∫ ·Äõ·Äê·Ä≤·Ä∑ reward\n",
    "        action_hold_reward          =   0 # Hold action ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ reward/penalty\n",
    "\n",
    "        _msg                        =   []\n",
    "        _action                     =   action[0] # action value eg. [0.75]\n",
    "        open_position               =   0\n",
    "        for position in self.positions:\n",
    "            if position['Status']   ==  0:\n",
    "                position_reward, closed, _msg   =   self._calculate_reward(position)\n",
    "                if not closed: open_position += 1  # Count what we already knew\n",
    "                reward += position_reward\n",
    "\n",
    "        # Continuous actions: [1 -> 0.5] LONG | [0.5 -> -0.5] HOLD |[-0.5 -> -1] SHORT\n",
    "        action_name = self._get_action_name(action, ma_fast, ma_slow)\n",
    "\n",
    "        if open_position < self.max_current_holding and action_name in ['BUY', 'SELL']:\n",
    "            self.ticket_id  +=  1\n",
    "\n",
    "            # Real trading ·Äô·Äæ·Ä¨ margin requirement ·Äõ·Äæ·Ä≠·Äû·Äú·Ä≠·ÄØ·Äô·Äª·Ä≠·ÄØ·Ä∏\n",
    "            # Position ·Äñ·ÄΩ·ÄÑ·Ä∑·Ä∫·Äõ·ÄÑ·Ä∫ capital ·ÄÅ·Äª·ÄØ·Äï·Ä∫·ÄÑ·Äº·Ä¨·Ä∏·Äî·Ä±·Äõ·Äê·Äö·Ä∫\n",
    "            # Position ·Äï·Ä≠·Äê·Ä∫·Äê·Ä≤·Ä∑·Ä°·ÄÅ·Ä´ ·Äï·Äº·Äî·Ä∫·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏·Äë·Ää·Ä∑·Ä∫·Äï·Ä±·Ä∏·Äê·Äö·Ä∫\n",
    "            self.balance -= self.margin_requirement # hold up, this will make sure model can not open a lot of\n",
    "\n",
    "            position        =   {\n",
    "                \"Ticket\"        :   self.ticket_id,\n",
    "                \"Symbol\"        :   self.symbol_col,\n",
    "                \"ActionTime\"    :   _t,\n",
    "                \"Type\"          :   action_name,\n",
    "                \"Lot\"           :   1,\n",
    "                \"ActionPrice\"   :   _c,\n",
    "                \"SL\"            :   self.stop_loss,\n",
    "                \"PT\"            :   self.profit_taken,\n",
    "                \"MaxDD\"         :   0,\n",
    "                \"Swap\"          :   0.0,\n",
    "                \"CloseTime\"     :   \"\",\n",
    "                \"ClosePrice\"    :   0.0,\n",
    "                \"Point\"         :   self.point,\n",
    "                \"Reward\"        :   self.transaction_fee,\n",
    "                \"DateDuration\"  :   _t.date().isoformat(),\n",
    "                \"Status\"        :   0, # 0 is Position is currently OPEN and active\n",
    "                #\"PIPS\"          :   self.transaction_fee, # Price Interest Point (profit/loss ·ÄÄ·Ä≠·ÄØ measure ·Äú·ÄØ·Äï·Ä∫·Äê·Ä≤·Ä∑ unit)\n",
    "                \"PIPS\"          :   0,\n",
    "                \"ActionStep\"    :   self.current_step,\n",
    "                \"CloseStep\"     :   -1, # Step number when position closed, not close yet is -1\n",
    "                \"DeltaStep\"     :   0,\n",
    "                \"OpenBal\"       :   self.balance,\n",
    "                \"CloseBal\"       :   0,\n",
    "                \"HighestPrice\"  :   _c,\n",
    "                \"LowestPrice\"   :   _c,\n",
    "            }\n",
    "\n",
    "            self.positions.append(position)\n",
    "            # do not use transaction_fee penalty\n",
    "            # reward = self.transaction_fee #open cost\n",
    "            # model ·ÄÄ ·Ä°·Äú·ÄΩ·Äî·Ä∫·Ä°·ÄÄ·Äª·ÄΩ·Ä∂ position ·Äê·ÄΩ·Ä± ·Äô·Äñ·ÄΩ·ÄÑ·Ä∑·Ä∫·Äô·Ä≠·Ä°·Ä±·Ä¨·ÄÑ·Ä∫ ·Äë·Ä≠·Äî·Ä∫·Ä∏·ÄÅ·Äª·ÄØ·Äï·Ä∫·Äê·Ä≤·Ä∑ mechanism ·Äñ·Äº·ÄÖ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫·Åã\n",
    "            _msg.append(f'Step:{self.current_step} Tkt:{position[\"Ticket\"]} {position[\"Type\"]} Rwd:{position[\"PIPS\"]} SL:{position[\"SL\"]} PT:{position[\"PT\"]}')\n",
    "\n",
    "        # HOLD Penalty ·ÄÄ·Ä≠·ÄØ ·Ä°·Äú·ÄΩ·Äî·Ä∫·Äû·Ä±·Ä∏·ÄÑ·Äö·Ä∫·Äû·Ä±·Ä¨ ·Äê·Äî·Ä∫·Äñ·Ä≠·ÄØ·Ä∏\n",
    "        # (·Ä•·Äï·Äô·Ä¨: -0.0001) ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Ä´·Åã ·Ä°·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÜ·ÄØ·Ä∂·Ä∏·Äô·Äæ·Ä¨\n",
    "        # Trading ·Äô·Äú·ÄØ·Äï·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ Penalty ·Äô·Äï·Ä±·Ä∏·Äò·Ä≤ action_hold_reward = 0 ·Äë·Ä¨·Ä∏·Äï·Ä´·Åã\n",
    "        elif open_position < self.max_current_holding and action_name == \"HOLD\":\n",
    "            action_hold_reward  =   0  # no open any position, encourage open position\n",
    "        else:\n",
    "            action_hold_reward  =   0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        reward              +=  action_hold_reward\n",
    "\n",
    "        # Move to the next time step\n",
    "        self.current_step   +=  1\n",
    "\n",
    "        # check if episode is done\n",
    "        terminated          =   (self.balance <= 0)\n",
    "        truncated           =   (self.current_step > self.max_steps)\n",
    "\n",
    "        # get next observation\n",
    "        obs                 =   self._next_observation()\n",
    "        _msg.append(f'---idle----step:{self.current_step}, RF:{action_name} Action:{_action} Balance: {self.balance} reward:{reward} total_rewards:{self.ttl_rewards} position_reward:{position_reward} action_hold_reward:{action_hold_reward}')\n",
    "\n",
    "\n",
    "        current_equity = self._calculate_current_equity()\n",
    "        self.equity_curve.append(current_equity)\n",
    "        self._calculate_drawdown()  # This updates peak_equity and drawdowns\n",
    "\n",
    "        # =========================================================================\n",
    "        # START: Drawdown Penalty Logic\n",
    "        # =========================================================================\n",
    "        # self.current_drawdown ·Äû·Ää·Ä∫ Percentage (0.0 ·Äô·Äæ 1.0) ·Äñ·Äº·ÄÖ·Ä∫·Äû·Ää·Ä∫·Åã\n",
    "\n",
    "\n",
    "        drawdown_penalty = self.current_drawdown * self.drawdown_penalty_factor\n",
    "        # Reward ·Äê·ÄΩ·ÄÑ·Ä∫ ·Äî·ÄØ·Äê·Ä∫·Äï·Ä±·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "        reward -= drawdown_penalty\n",
    "\n",
    "        # Log the penalty for debugging\n",
    "        _msg.append(f'Drawdown Penalty: -{drawdown_penalty:.4f} (DD:{self.current_drawdown:.4f})')\n",
    "        # =========================================================================\n",
    "        # END: Drawdown Penalty Logic\n",
    "        # =========================================================================\n",
    "        # Drawdown Penalty ·Äî·ÄØ·Äê·Ä∫·Äï·Äº·ÄÆ·Ä∏·Äô·Äæ·Äû·Ä¨ ·ÄÖ·ÄØ·ÄÖ·ÄØ·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏ Reward ·ÄÄ·Ä≠·ÄØ ·Ä°·Äï·Ä∫·Äí·Ä≠·Äê·Ä∫·Äú·ÄØ·Äï·Ä∫·Äï·Ä´\n",
    "        self.ttl_rewards += reward  # <--- ·Ä§·Äî·Ä±·Äõ·Ä¨·Äê·ÄΩ·ÄÑ·Ä∫ ·Äï·Äº·Äî·Ä∫·Äë·Ää·Ä∑·Ä∫·Äï·Ä´\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        if terminated or truncated:\n",
    "            buy_positions = [p for p in self.positions if p[\"Type\"] == \"BUY\"]\n",
    "            sell_positions = [p for p in self.positions if p[\"Type\"] == \"SELL\"]\n",
    "\n",
    "            buy_count = len(buy_positions)\n",
    "            sell_count = len(sell_positions)\n",
    "            total_positions = len(self.positions)\n",
    "\n",
    "            # Calculate win rates\n",
    "            buy_wins = len([p for p in buy_positions if p[\"PIPS\"] > 0])\n",
    "            sell_wins = len([p for p in sell_positions if p[\"PIPS\"] > 0])\n",
    "\n",
    "            buy_win_rate = buy_wins / buy_count if buy_count > 0 else 0\n",
    "            sell_win_rate = sell_wins / sell_count if sell_count > 0 else 0\n",
    "\n",
    "            _m = f'--- Positions: {total_positions} (Buy:{buy_count}, Sell:{sell_count}) | '\n",
    "            _m += f'WinRates: Buy:{buy_win_rate:.1%}, Sell:{sell_win_rate:.1%} | '\n",
    "            _m += f'TotalRewards: {self.ttl_rewards} Balance: {self.balance}'\n",
    "\n",
    "            logger.info(_m)\n",
    "            _msg.append(_m)\n",
    "\n",
    "            # Additional info\n",
    "            if self.logger_show:\n",
    "                for _m in _msg:\n",
    "                    logger.info(_m)\n",
    "\n",
    "            info[\"info\"]                = _msg\n",
    "            info[\"sharpe\"]              = self._calculate_sharpe()  # ‚úÖ Now works! üí° 'sharpe_ratio' ·Äô·Äæ 'sharpe' ·Äû·Ä≠·ÄØ·Ä∑·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Ä´·Åã\n",
    "            info[\"drawdown\"]            = self.max_drawdown         # ‚úÖ Now accurate!'max_drawdown' ·Äô·Äæ 'drawdown' ·Äû·Ä≠·ÄØ·Ä∑·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Ä´·Åã\n",
    "            info[\"current_equity\"]      = current_equity            # ‚úÖ For debugging\n",
    "            info[\"peak_equity\"]         = self.peak_equity          # ‚úÖ For debugging\n",
    "            info[\"equity_curve_length\"] = len(self.equity_curve)    # ‚úÖ Monitor growth\n",
    "            info[\"episode\"]             = {\n",
    "                \"r\": reward,\n",
    "                \"l\": self.current_step\n",
    "            }\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _calculate_reward(self, position):\n",
    "        _o, _h, _l, _c              = self.data_raw.iloc[self.current_step][['open', 'high', 'low', 'close']]\n",
    "        _t                          = self.data.index[self.current_step]\n",
    "        _msg                        =   []\n",
    "\n",
    "        entry_price                 =   position['ActionPrice']\n",
    "        direction                   =   position['Type']\n",
    "        profit_target_price         =   entry_price + position['PT']/ self.point if direction == 'BUY' else entry_price - position['PT']/self.point\n",
    "        stop_loss_price             =   entry_price + position['SL']/ self.point if direction == 'BUY' else entry_price - position['SL']/self.point\n",
    "        closed                      =   False\n",
    "        close_position_reward       =   0.0\n",
    "        good_position_reward        =   0.0\n",
    "\n",
    "        # Check for stoploss hit\n",
    "        if (direction == 'BUY' and _l <= stop_loss_price) or (direction == 'SELL' and _h >= stop_loss_price):\n",
    "            close_position_reward   =   position['SL'] # position sl ·ÄÄ minus value ·Äñ·Äº·ÄÖ·Ä∫·Äê·Äö·Ä∫\n",
    "\n",
    "            position['CloseTime']   =   _t\n",
    "            position['ClosePrice']  =   stop_loss_price\n",
    "            position['Status']      =   1   # Status ·ÄÄ open ·ÄÜ·Ä≠·ÄØ 0 close ·ÄÜ·Ä≠·ÄØ 1\n",
    "            position['CloseStep']   =   self.current_step\n",
    "            position['PIPS']        =   close_position_reward - self.transaction_fee\n",
    "            position['DeltaStep']   =   self.current_step - position['ActionStep']\n",
    "            position['Info']        =   f'{profit_target_price:.5f} | {stop_loss_price:.5f}'\n",
    "\n",
    "            self.balance            +=  self.margin_requirement + position['PIPS'] # return 100 is margin hold\n",
    "            position['CloseBal']    =   self.balance\n",
    "            closed                  =   True\n",
    "            _msg.append(f'Step:{self.current_step} Tkt:{position[\"Ticket\"]}: Rwd:{position[\"PIPS\"]}, SL:{position[\"SL\"]}, DeltaStep:{position[\"DeltaStep\"]}')\n",
    "\n",
    "        elif (direction == 'BUY' and _h >= profit_target_price) or (direction == 'SELL' and _l <= profit_target_price):\n",
    "            close_position_reward   =    position['PT'] # position tp ·ÄÄ plus value ·Äñ·Äº·ÄÖ·Ä∫·Äê·Äö·Ä∫\n",
    "\n",
    "            position['CloseTime']   =   _t\n",
    "            position['ClosePrice']  =   profit_target_price\n",
    "            position['Status']      =   2   # Status ·ÄÄ open ·ÄÜ·Ä≠·ÄØ 0 close ·ÄÜ·Ä≠·ÄØ 1\n",
    "            position['CloseStep']   =   self.current_step\n",
    "            position['PIPS']        =   close_position_reward - self.transaction_fee\n",
    "            position['DeltaStep']   =   self.current_step - position['ActionStep']\n",
    "            position['Info']        =   f'{profit_target_price:.5f} | {stop_loss_price:.5f}'\n",
    "\n",
    "            self.balance            +=  self.margin_requirement + position['PIPS'] # return 100 is margin hold\n",
    "            position['CloseBal']    =   self.balance\n",
    "            closed                  =   True\n",
    "            _msg.append(f'Step:{self.current_step} Tkt:{position[\"Ticket\"]}: Rwd:{position[\"PIPS\"]}, SL:{position[\"SL\"]}, DeltaStep:{position[\"DeltaStep\"]}')\n",
    "\n",
    "        else:\n",
    "            if self.current_step + 5 + self.sequence_length >= len(self.data):\n",
    "                close_position_reward   =   (_c - position[\"ActionPrice\"] if direction == 'BUY' else position[\"ActionPrice\"] - _c)* self.point\n",
    "\n",
    "                position['CloseTime']   =   _t\n",
    "                position['ClosePrice']  =   _c\n",
    "                position['Status']      =   3   # Status ·ÄÄ open ·ÄÜ·Ä≠·ÄØ 0 close ·ÄÜ·Ä≠·ÄØ 1, force close 2\n",
    "                position['CloseStep']   =   self.current_step\n",
    "                position['PIPS']        =   close_position_reward - self.transaction_fee\n",
    "                position['DeltaStep']   =   self.current_step - position['ActionStep']\n",
    "                position['Info']        =   f'{profit_target_price:.5f} | {stop_loss_price:.5f}'\n",
    "                self.balance            +=  self.margin_requirement + position[\"PIPS\"] # return 100 is margin hold\n",
    "                position['CloseBal']    =   self.balance\n",
    "\n",
    "                closed                  =   True\n",
    "                _msg.append(f'Step:{self.current_step} Tkt:{position[\"Ticket\"]}: Rwd:{position[\"PIPS\"]}, Cls:End, DeltaStep:{position[\"DeltaStep\"]}')\n",
    "\n",
    "            else:\n",
    "                # =========================================================================\n",
    "                # Real Trailing Stop Logic (·Ä°·Äô·Äº·ÄÑ·Ä∑·Ä∫·ÄÜ·ÄØ·Ä∂·Ä∏ ·Äõ·Ä±·Ä¨·ÄÄ·Ä∫·Äñ·Ä∞·Ä∏·Äû·Ä±·Ä¨ ·Äà·Ä±·Ä∏·Äî·Äæ·ÄØ·Äî·Ä∫·Ä∏·ÄÄ·Ä≠·ÄØ ·Äô·Äæ·Äê·Ä∫·Äê·Äô·Ä∫·Ä∏·Äê·ÄÑ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏)\n",
    "                # =========================================================================\n",
    "                # 1. Highest/Lowest Price Update\n",
    "\n",
    "                if direction == \"BUY\":\n",
    "                  # Buy position ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Ä°·Äô·Äº·ÄÑ·Ä∑·Ä∫·ÄÜ·ÄØ·Ä∂·Ä∏ ·Äõ·Ä±·Ä¨·ÄÄ·Ä∫·Äñ·Ä∞·Ä∏·Äû·Ä±·Ä¨ ·Äà·Ä±·Ä∏·Äî·Äæ·ÄØ·Äî·Ä∫·Ä∏·ÄÄ·Ä≠·ÄØ ·Äô·Äæ·Äê·Ä∫·Äê·Äô·Ä∫·Ä∏·Äê·ÄÑ·Ä∫\n",
    "                  if _c > position[\"HighestPrice\"]:\n",
    "                      position[\"HighestPrice\"] = _c\n",
    "\n",
    "                  # 2. New SL Target Price (Trailing Price) ·ÄÄ·Ä≠·ÄØ ·Äê·ÄΩ·ÄÄ·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "                  # New_SL_Price = HighestPrice - (Trailing Distance Pips ·ÄÄ·Ä≠·ÄØ Price Change ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏)\n",
    "                  trailing_price = position[\"HighestPrice\"] - self.trailing_distance / self.point\n",
    "\n",
    "                  # 3. SL ·ÄÄ·Ä≠·ÄØ ·Ä°·ÄÜ·ÄÑ·Ä∑·Ä∫·Äô·Äº·Äæ·ÄÑ·Ä∑·Ä∫·Äê·ÄÑ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "                  # ·Äú·ÄÄ·Ä∫·Äõ·Äæ·Ä≠ SL ·Äë·ÄÄ·Ä∫ ·Äï·Ä≠·ÄØ·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äô·Äæ·Äû·Ä¨ ·Äõ·ÄΩ·Ä±·Ä∑·Äï·Ä´\n",
    "                  if trailing_price > stop_loss_price:\n",
    "\n",
    "                      stop_loss_price = trailing_price\n",
    "                      # SL_Price ·Ä°·Äû·ÄÖ·Ä∫·ÄÄ·Ä≠·ÄØ Points ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Äî·Ä∫·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Äº·ÄÆ·Ä∏ position['SL'] ·ÄÄ·Ä≠·ÄØ ·Ä°·Äï·Ä∫·Äí·Ä≠·Äê·Ä∫·Äú·ÄØ·Äï·Ä∫·Äï·Ä´\n",
    "                      position[\"SL\"] = (stop_loss_price - entry_price) * self.point\n",
    "                    #   if position[\"SL\"] > 0:\n",
    "                    #       position[\"SL\"]    =   -abs(position[\"SL\"])\n",
    "                      trailing_happened = True\n",
    "                  else:\n",
    "                      trailing_happened = False\n",
    "\n",
    "\n",
    "                elif direction == \"SELL\":\n",
    "                  # Sell position ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Ä°·Äî·Ä≠·Äô·Ä∑·Ä∫·ÄÜ·ÄØ·Ä∂·Ä∏ ·Äõ·Ä±·Ä¨·ÄÄ·Ä∫·Äñ·Ä∞·Ä∏·Äû·Ä±·Ä¨ ·Äà·Ä±·Ä∏·Äî·Äæ·ÄØ·Äî·Ä∫·Ä∏·ÄÄ·Ä≠·ÄØ ·Äô·Äæ·Äê·Ä∫·Äê·Äô·Ä∫·Ä∏·Äê·ÄÑ·Ä∫\n",
    "                  if _c < position[\"LowestPrice\"]:\n",
    "                      position[\"LowestPrice\"] = _c\n",
    "\n",
    "                  # New SL Target Price (Trailing Price) ·ÄÄ·Ä≠·ÄØ ·Äê·ÄΩ·ÄÄ·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "                  trailing_price = position[\"LowestPrice\"] + self.trailing_distance / self.point\n",
    "\n",
    "                  # SL ·ÄÄ·Ä≠·ÄØ ·Ä°·ÄÜ·ÄÑ·Ä∑·Ä∫·Äô·Äº·Äæ·ÄÑ·Ä∑·Ä∫·Äê·ÄÑ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "                  if trailing_price < stop_loss_price:\n",
    "                      stop_loss_price = trailing_price\n",
    "                      # SL_Price ·Ä°·Äû·ÄÖ·Ä∫·ÄÄ·Ä≠·ÄØ Points ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Äî·Ä∫·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Äº·ÄÆ·Ä∏ position['SL'] ·ÄÄ·Ä≠·ÄØ ·Ä°·Äï·Ä∫·Äí·Ä≠·Äê·Ä∫·Äú·ÄØ·Äï·Ä∫·Äï·Ä´\n",
    "                      position[\"SL\"] = (entry_price - stop_loss_price) * self.point\n",
    "                    #   if position[\"SL\"] > 0:\n",
    "                    #       position[\"SL\"]    =   -abs(position[\"SL\"])\n",
    "                      trailing_happened = True\n",
    "                  else:\n",
    "                      trailing_happened = False\n",
    "\n",
    "                # =========================================================================\n",
    "                # Reward Logic (Trailing ·Äú·ÄØ·Äï·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ Bonus ·Äï·Ä±·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏)\n",
    "                # =========================================================================\n",
    "                # Reward Sign ·ÄÄ·Ä≠·ÄØ ·Äö·ÄÅ·ÄÑ·Ä∫·Ä°·Äê·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏ ·Äê·ÄΩ·ÄÄ·Ä∫·Äï·Ä´·Åã\n",
    "                delta = _c - entry_price\n",
    "                if direction == \"BUY\":\n",
    "                    reward_sign = 1 if delta >= 0 else -1\n",
    "                elif direction == \"SELL\":\n",
    "                    reward_sign = -1 if delta >= 0 else 1\n",
    "\n",
    "                good_position_reward = reward_sign * self.good_position_reward_scale\n",
    "\n",
    "                # Trailing ·Ä°·Äô·Äæ·Äî·Ä∫·Äê·ÄÄ·Äö·Ä∫ ·Äñ·Äº·ÄÖ·Ä∫·Äû·ÄΩ·Ä¨·Ä∏·Äô·Äæ·Äû·Ä¨ Bonus Reward ·ÄÄ·Ä≠·ÄØ ·Äï·Ä±·Ä∏·Äï·Ä´\n",
    "                if trailing_happened:\n",
    "                    good_position_reward += 0.001\n",
    "\n",
    "                position['Info']        =   f'{profit_target_price:.5f} | {stop_loss_price:.5f}'\n",
    "                position['CloseBal']    =   self.balance\n",
    "                _msg.append(f'Step:{self.current_step} Tkt:{position[\"Ticket\"]}: NO_Close, PT:{position[\"PT\"]}, SL:{position[\"SL\"]}')\n",
    "\n",
    "        return close_position_reward + good_position_reward, closed, _msg\n",
    "\n",
    "\n",
    "    def _calculate_sharpe(self, risk_free_rate=0.0):\n",
    "        \"\"\"Calculate Sharpe ratio for the current episode\"\"\"\n",
    "        if len(self.equity_curve) < 2:\n",
    "            return 0.0\n",
    "\n",
    "        returns = np.diff(self.equity_curve) / self.equity_curve[:-1]\n",
    "\n",
    "        if np.std(returns) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        sharpe = (np.mean(returns) - risk_free_rate) / np.std(returns)\n",
    "        return float(sharpe * np.sqrt(288))  # Annualized (5-min bars ‚Üí 288/day)\n",
    "\n",
    "    def _calculate_drawdown(self):\n",
    "        \"\"\"Update max drawdown during episode\"\"\"\n",
    "        current_equity          =   self.equity_curve[-1]\n",
    "        self.peak_equity        =   max(self.peak_equity, current_equity)\n",
    "        self.current_drawdown   =   (self.peak_equity - current_equity) / self.peak_equity\n",
    "        self.max_drawdown       =   max(self.max_drawdown, self.current_drawdown)\n",
    "\n",
    "\n",
    "    def _calculate_current_equity(self):\n",
    "        \"\"\"Calculate total current equity (balance + unrealized P/L)\"\"\"\n",
    "        total_equity = self.balance  # Start with cash balance\n",
    "\n",
    "        # Add unrealized P/L from open positions\n",
    "        for position in self.positions:\n",
    "            if position['Status'] == 0:  # Only open positions\n",
    "                current_price = self.data.iloc[self.current_step][\"close\"]\n",
    "                entry_price = position['ActionPrice']\n",
    "\n",
    "                if position['Type'] == 'BUY':\n",
    "                    unrealized_pnl = (current_price - entry_price) * self.point\n",
    "                else:  # Sell\n",
    "                    unrealized_pnl = (entry_price - current_price) * self.point\n",
    "\n",
    "                total_equity += unrealized_pnl\n",
    "\n",
    "        return total_equity\n",
    "\n",
    "    def render(self, mode='human', title=None, **kwargs):\n",
    "        # Render the environment to the screen\n",
    "        if mode in ('human', 'file'):\n",
    "            log_header      =   True\n",
    "            printout        =   False\n",
    "            if mode == 'human':\n",
    "                printout    =   True\n",
    "\n",
    "            log_file = self.csv_file.replace(\"split/\", \"log/\")\n",
    "            pm = {\n",
    "                \"log_header\": log_header,\n",
    "                \"log_filename\": log_file,\n",
    "                \"printout\": printout,\n",
    "                \"balance\": self.balance,\n",
    "                \"balance_initial\": self.balance_initial,\n",
    "                \"transaction_close_this_step\": self.positions,\n",
    "                \"done_information\": False\n",
    "            }\n",
    "            render_to_file(**pm)\n",
    "            if log_header:\n",
    "                    log_header = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f1ff4815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Data: ·ÅÇ·ÅÄ·ÅÇ·ÅÄ ·Äá·Äî·Ä∫·Äî·Äù·Ä´·Äõ·ÄÆ·Äô·Äæ ·ÅÇ·ÅÄ·ÅÇ·ÅÖ (·ÅÖ ·Äî·Äæ·ÄÖ·Ä∫·ÄÖ·Ä¨)\n",
    "# Total Steps (·ÄÅ·Äî·Ä∑·Ä∫·Äô·Äæ·Äî·Ä∫·Ä∏): 72,000 Steps/·Äî·Äæ·ÄÖ·Ä∫ 5·Äî·Äæ·ÄÖ·Ä∫ === 360,000 Steps\n",
    "# Total Training Steps (Decay ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫): Continuous Fine-tuning Steps ·Äô·Äª·Ä¨·Ä∏·Äï·Ä´ ·Äë·Ää·Ä∑·Ä∫·Äû·ÄΩ·ÄÑ·Ä∫·Ä∏·Äê·ÄΩ·ÄÄ·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫·Äï·Ä´·ÄÄ·Åä\n",
    "# ·ÄÖ·ÄØ·ÄÖ·ÄØ·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏ 1,000,000 (1M) Steps ·ÄÄ·Ä≠·ÄØ Total Decay Length ·Ä°·Äñ·Äº·ÄÖ·Ä∫ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·ÄÄ·Äº·Ää·Ä∑·Ä∫·Äï·Ä´·Åã\n",
    "# Global Variables ·ÄÄ·Ä≠·ÄØ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "\n",
    "GLOBAL_TOTAL_STEPS = 1_000_000 # ·Ä•·Äï·Äô·Ä¨- 1 Million Steps\n",
    "START_LR = 1e-4\n",
    "END_LR = 5e-6\n",
    "\n",
    "class ContinuousLRScheduler(BaseCallback):\n",
    "    def __init__(self, total_global_steps, verbose=0):\n",
    "        super(ContinuousLRScheduler, self).__init__(verbose)\n",
    "        self.total_global_steps = total_global_steps\n",
    "        # 'num_timesteps' ·Äû·Ää·Ä∫ SB3 ·ÄÄ Model ·Äê·ÄÖ·Ä∫·ÄÅ·ÄØ Training ·ÄÖ·Äê·ÄÑ·Ä∫·ÄÅ·Äª·Ä≠·Äî·Ä∫·Äô·Äæ ·ÄÖ·ÄØ·ÄÖ·ÄØ·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏ Step ·ÄÄ·Ä≠·ÄØ ·Äô·Äæ·Äê·Ä∫·Äë·Ä¨·Ä∏·Äû·Ää·Ä∫·Åã\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # ·Äú·ÄÄ·Ä∫·Äõ·Äæ·Ä≠ ·ÄÖ·ÄØ·ÄÖ·ÄØ·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏ Global Step ·ÄÄ·Ä≠·ÄØ ·Äõ·Äö·Ä∞·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "        current_global_step = self.num_timesteps\n",
    "\n",
    "        # Global Progress (0.0 ·Äô·Äæ 1.0) ·ÄÄ·Ä≠·ÄØ ·Äê·ÄΩ·ÄÄ·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "        progress = current_global_step / self.total_global_steps\n",
    "        progress_remaining = 1.0 - progress\n",
    "\n",
    "        # LR ·Ä°·Äû·ÄÖ·Ä∫·ÄÄ·Ä≠·ÄØ ·Äê·ÄΩ·ÄÄ·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏ (·Äû·ÄÑ·Ä∫·Åè Decay Logic ·Ä°·Äê·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏)\n",
    "        new_lr = END_LR + (START_LR - END_LR) * progress_remaining\n",
    "\n",
    "        # Optimizer ·Åè LR ·ÄÄ·Ä≠·ÄØ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "        self.model.policy.optimizer.param_groups[0]['lr'] = new_lr\n",
    "\n",
    "        return True\n",
    "\n",
    "# ·Ä§ Callback ·ÄÄ·Ä≠·ÄØ Initial Training ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ Continuous Fine-tuning ·Äî·Äæ·ÄÖ·Ä∫·ÄÅ·ÄØ·ÄÖ·Äú·ÄØ·Ä∂·Ä∏·Äê·ÄΩ·ÄÑ·Ä∫ ·Äû·ÄØ·Ä∂·Ä∏·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äï·Ä´·Äû·Ää·Ä∫·Åã\n",
    "# ·Åé·ÄÑ·Ä∫·Ä∏·Äû·Ää·Ä∫ Agent ·Åè ·Äú·ÄÄ·Ä∫·Äõ·Äæ·Ä≠ num_timesteps ·Äï·Ä±·Ä´·Ä∫·Äô·Ä∞·Äê·Ää·Ä∫·Åç LR ·ÄÄ·Ä≠·ÄØ ·Ä°·ÄÜ·ÄÄ·Ä∫·Äô·Äï·Äº·Äê·Ä∫ ·Äú·Äª·Ä±·Ä¨·Ä∑·ÄÅ·Äª·Äï·Ä±·Ä∏·Äû·ÄΩ·Ä¨·Ä∏·Äï·Ä´·Äô·Ää·Ä∫·Åã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a0ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import dump, load # Scaler ·ÄÄ·Ä≠·ÄØ ·Äû·Ä≠·Äô·Ä∫·Ä∏·ÄÜ·Ää·Ä∫·Ä∏/·Äï·Äº·Äî·Ä∫·Äö·Ä∞·Äõ·Äî·Ä∫\n",
    "\n",
    "cf = EnvConfig('./drive/MyDrive/configure.json')\n",
    "features_scaled = cf.env_parameters(item='features_scaled')\n",
    "\n",
    "SCALER_PATH = '/content/drive/MyDrive/data/model/EURUSD/scalar_2020_2021.joblib'\n",
    "DATA_PATH = '/content/drive/MyDrive/data/raw/final.csv'\n",
    "train_scaler = StandardScaler()\n",
    "\n",
    "train_data_df = pd.read_csv(DATA_PATH)\n",
    "data_to_fit = train_data_df[features_scaled]\n",
    "data_to_fit = data_to_fit.dropna()\n",
    "train_scaler.fit(data_to_fit)\n",
    "\n",
    "# 3. Scaler Object ·ÄÄ·Ä≠·ÄØ File ·Ä°·Äñ·Äº·ÄÖ·Ä∫ ·Äû·Ä≠·Äô·Ä∫·Ä∏·ÄÜ·Ää·Ä∫·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏ (Production/Test ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫)\n",
    "dump(train_scaler, SCALER_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1609b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3 import PPO\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "# Assume logger is defined elsewhere, e.g., import logging; logger = logging.getLogger(__name__)\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "\n",
    "BASE_SEED = 42\n",
    "number_envs = 1\n",
    "# Stable-Baselines3 ·Äõ·Ä≤·Ä∑ Global Seed ·ÄÄ·Ä≠·ÄØ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äï·Ä´\n",
    "set_random_seed(BASE_SEED)\n",
    "\n",
    "\n",
    "def single_csv_training(csv_file, env_config_file, asset, model_name='', cf=None, number_envs=1, week_num=0):  # Added week_num for varying seed\n",
    "    # 1. Log Root Directory ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ Run Name ·ÄÄ·Ä≠·ÄØ ·Äê·ÄΩ·ÄÄ·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "    BASE_LOG_DIR = \"/content/drive/MyDrive/data/log\"\n",
    "    RUN_NAME = f\"{asset}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "    # 2. Log Root Directory ·Äõ·Äæ·Ä≠·Äô·Äõ·Äæ·Ä≠ ·ÄÖ·ÄÖ·Ä∫·ÄÜ·Ä±·Ä∏·Äï·Äº·ÄÆ·Ä∏ ·Äñ·Äî·Ä∫·Äê·ÄÆ·Ä∏·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "    os.makedirs(BASE_LOG_DIR, exist_ok=True)\n",
    "\n",
    "    #sequence_length = cf.env_parameters(\"backward_window\")\n",
    "    sequence_length = cf.data_processing_parameters(\"sequence_length\")\n",
    "    # lr_schedule = linear_schedule(1e-4, 5e-6)\n",
    "    policy_kwargs = dict(\n",
    "        # Repo ·Äõ·Ä≤·Ä∑ custom feature extractor (Transformer + MLP ·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏·Äë·Ä¨·Ä∏·Äê·Ä¨·Åä time series data ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Äû·ÄÑ·Ä∑·Ä∫·Äê·Ä±·Ä¨·Ä∫·Äê·Äö·Ä∫)·Åã\n",
    "        features_extractor_class=CustomCombinedExtractor,\n",
    "        # features_extractor_kwargs: Sequence length ·ÄÄ·Ä≠·ÄØ ·Äë·Ää·Ä∑·Ä∫·Åã\n",
    "        features_extractor_kwargs=dict(\n",
    "            sequence_length=sequence_length,   # input sequence length\n",
    "            embed_dim=64,\n",
    "            num_heads=2,\n",
    "            num_layers=2\n",
    "        ),\n",
    "        # net_arch: Actor (pi - policy network) ·Äî·Ä≤·Ä∑ Critic (vf - value function) ·Äî·Äæ·ÄÖ·Ä∫·ÄÅ·ÄØ ·Äú·ÄØ·Ä∂·Ä∏ ·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ hidden layers [256, 256] ·Äû·ÄØ·Ä∂·Ä∏·Åã (Updated vf to [512,256] for better explained variance)\n",
    "        net_arch=dict(pi=[256, 256], vf=[512, 256]),  # Increased vf capacity\n",
    "        # Activation function ·Ä°·Äî·Ä±·Äî·Ä≤·Ä∑ ReLU ·Äû·ÄØ·Ä∂·Ä∏ (non-linear ·Äñ·Äº·ÄÖ·Ä∫·Ä°·Ä±·Ä¨·ÄÑ·Ä∫)·Åã\n",
    "        activation_fn=nn.ReLU,\n",
    "        # Orthogonal initialization ·Äô·Äû·ÄØ·Ä∂·Ä∏ (financial data ·Äô·Äæ·Ä¨ ·Äï·Ä≠·ÄØ·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏ ·Äê·Äö·Ä∫·Äú·Ä≠·ÄØ·Ä∑ comment ·Äô·Äæ·Ä¨ ·Äõ·Ä±·Ä∏ ·Äë·Ä¨·Ä∏·Äê·Äö·Ä∫·Åä ·Äí·Ä´·ÄÄ weights ·ÄÄ·Ä≠·ÄØ ·Äï·Ä≠·ÄØ·Äõ·Ä≠·ÄØ·Ä∏·Äõ·Äæ·ÄÑ·Ä∫·Ä∏ ·ÄÖ ·Äú·ÄØ·Äï·Ä∫·Äê·Äö·Ä∫)·Åã\n",
    "        ortho_init=False  # better for financial data\n",
    "    )\n",
    "\n",
    "    # Environment Factories ·Äô·Äª·Ä¨·Ä∏ ·Äñ·Äî·Ä∫·Äê·ÄÆ·Ä∏·Äï·Ä´\n",
    "    env_fns = [\n",
    "        lambda: ForexTradingEnv(\n",
    "            csv_file,\n",
    "            cf,\n",
    "            asset,\n",
    "            logger_show=True,\n",
    "            scaler=train_scaler\n",
    "        )\n",
    "        for _ in range(number_envs)\n",
    "    ]\n",
    "    # DummyVecEnv ·ÄÄ·Ä≠·ÄØ ·Äê·Ää·Ä∫·ÄÜ·Ä±·Ä¨·ÄÄ·Ä∫·Äï·Ä´ (SubprocVecEnv)\n",
    "    env = DummyVecEnv(env_fns)\n",
    "    # ·Ä§·Äî·Ä±·Äõ·Ä¨·Äû·Ää·Ä∫ ·Ä°·Äì·Ä≠·ÄÄ·ÄÄ·Äª·Äû·Ää·Ä∫·Åã ·Åé·ÄÑ·Ä∫·Ä∏·ÄÄ Environment ·Äê·ÄÖ·Ä∫·ÄÅ·ÄØ·ÄÅ·Äª·ÄÑ·Ä∫·Ä∏·ÄÖ·ÄÆ·ÄÄ·Ä≠·ÄØ\n",
    "    # BASE_SEED, BASE_SEED+1, BASE_SEED+2... ·ÄÖ·Äû·Ää·Ä∫·Äñ·Äº·ÄÑ·Ä∑·Ä∫ Seed ·Äô·Äª·Ä¨·Ä∏ ·Äû·Äê·Ä∫·Äô·Äæ·Äê·Ä∫·Äï·Ä±·Ä∏·Äï·Äº·ÄÆ·Ä∏\n",
    "    # ·Åé·ÄÑ·Ä∫·Ä∏·Äê·Ä≠·ÄØ·Ä∑·Åè reset() ·ÄÄ·Ä≠·ÄØ ·Äï·Äº·Äî·Ä∫·Äú·Ää·Ä∫·ÄÅ·Ä±·Ä´·Ä∫·Äï·Ä±·Ä∏·Äú·Ä≠·Äô·Ä∑·Ä∫·Äô·Ää·Ä∫·Åã\n",
    "    # Vary seed per week to avoid overfitting in incremental training\n",
    "    varied_seed = BASE_SEED + week_num  # Example: Pass week_num=1 for week 2, etc.\n",
    "    env.seed(varied_seed)\n",
    "\n",
    "    if model_name:\n",
    "        model = PPO.load(\n",
    "            model_name,\n",
    "            env=env,\n",
    "            learning_rate=START_LR # override by learning_rate callback\n",
    "        )\n",
    "    else:\n",
    "        model = PPO(\n",
    "            'MlpPolicy',\n",
    "            env,\n",
    "            device='cuda', # üí• ·Ä§·Äî·Ä±·Äõ·Ä¨·Äê·ÄΩ·ÄÑ·Ä∫ 'cpu' ·Äû·Ä≠·ÄØ·Ä∑ ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÄ·Äº·Ää·Ä∑·Ä∫·Äï·Ä´\n",
    "            verbose=1,\n",
    "            tensorboard_log=BASE_LOG_DIR,\n",
    "            normalize_advantage=True,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            learning_rate=START_LR,\n",
    "            seed=varied_seed,\n",
    "\n",
    "            # ·Ä°·Äõ·Ä±·Ä∏·ÄÄ·Äº·ÄÆ·Ä∏\n",
    "            n_steps= 2048,\n",
    "            batch_size= 256,\n",
    "            n_epochs= 7,\n",
    "            max_grad_norm=0.5,  # Tighter gradient control\n",
    "\n",
    "            ent_coef=0.01,  # Reduced from 0.01 for controlled exploration\n",
    "            vf_coef=0.5,\n",
    "            target_kl=0.02,\n",
    "            clip_range=0.02,\n",
    "            \n",
    "            # vf_coef=0.5,  # Reduced from 0.7 to balance policy vs value\n",
    "            # target_kl=0.02,  # Increased from 0.005 for better updates\n",
    "            # ent_coef=0.005,  # Reduced from 0.01 for controlled exploration\n",
    "            # clip_range=0.01,  # Increased from 0.002 to reduce high clip_fraction\n",
    "            # gamma=0.99,\n",
    "        )\n",
    "\n",
    "    # Train the agent\n",
    "    logger.info(\"Starting model training...\")\n",
    "    metrics_callback = TrainingMetricsCallback()\n",
    "\n",
    "    lr_callback = ContinuousLRScheduler(GLOBAL_TOTAL_STEPS)\n",
    "    # Callback ·Äô·Äª·Ä¨·Ä∏·ÄÄ·Ä≠·ÄØ List ·Ä°·Äî·Ä±·Äñ·Äº·ÄÑ·Ä∑·Ä∫ ·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏·ÄÖ·Äï·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏\n",
    "    callback_list = CallbackList([lr_callback, metrics_callback])\n",
    "\n",
    "    model.learn(\n",
    "        total_timesteps=3_000_000, # retain 200,000\n",
    "        callback=callback_list,\n",
    "\n",
    "        # üö® ·Äï·Äº·ÄÑ·Ä∫·ÄÜ·ÄÑ·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫ ·ÅÇ: tb_log_name ·Äî·Ä±·Äõ·Ä¨·Äô·Äæ·Ä¨ Run Folder Name ·ÄÄ·Ä≠·ÄØ·Äï·Ä≤ ·Äï·Ä±·Ä∏·Äï·Ä´·Åã\n",
    "        tb_log_name=RUN_NAME,\n",
    "        reset_num_timesteps=False if model_name else True  # üîÑ Existing model ·ÄÜ·Ä≠·ÄØ·Äõ·ÄÑ·Ä∫ timesteps ·ÄÜ·ÄÄ·Ä∫·Äô·Äæ·Äê·Ä∫\n",
    "    )\n",
    "    logger.info(\"Model training complete\")\n",
    "    model_filename = csv_file.replace(\"split/\", \"model/\").replace(\".csv\", \"_single_test.zip\")\n",
    "    model.save(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d878f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a08faba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gymnasium as gym\n",
    "import pandas as pd\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "import mplfinance as mpf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Configure logging\n",
    "logger = logging.getLogger(__name__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55fc827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_to_file(**kwargs):\n",
    "    log_header                  =   kwargs.get(\"log_header\",False)\n",
    "    log_filename                =   kwargs.get(\"log_filename\",\"\")\n",
    "    printout                    =   kwargs.get(\"printout\",False)\n",
    "    balance                     =   kwargs.get(\"balance\")\n",
    "    balance_initial             =   kwargs.get(\"balance_initial\")\n",
    "    transaction_close_this_step =   kwargs.get(\"transaction_close_this_step\",[])\n",
    "    done_information            =   kwargs.get(\"done_information\",\"\")\n",
    "    profit                      =   balance - balance_initial\n",
    "\n",
    "    tr_lines                    =   \"\"\n",
    "    tr_lines_comma              =   \"\"\n",
    "    _header                     =   \"\"\n",
    "    _header_comma               =   \"\"\n",
    "    if log_header:\n",
    "        _header = f'{\"Ticket\":>8} {\"Type\":>4} {\"ActionStep\":16} \\\n",
    "                    {\"ActionPrice\":>12} {\"CloseStep\":8} {\"ClosePrice\":>12} \\\n",
    "                    {\"OpenBal\":>12} {\"CloseBal\":>12} {\"Status\":8} {\"Info\":>8} {\"PIPS\":>6} {\"SL\":>6} {\"PT\":>6} {\"DeltaStep\":8}\\n'\n",
    "\n",
    "\n",
    "        _header_comma = f'{\"Ticket,Type,ActionTime,ActionStep,ActionPrice,CloseTime,ClosePrice, OpenBal, CloseBal, Status, Info, PIPS,SL,PT,CloseStep,DeltaStep\"}\\n'\n",
    "    if transaction_close_this_step:\n",
    "        for _tr in transaction_close_this_step:\n",
    "            if _tr[\"CloseStep\"] >=0:\n",
    "                tr_lines += f'{_tr[\"Ticket\"]:>8} {_tr[\"Type\"]:>4} {_tr[\"ActionStep\"]:16} \\\n",
    "                    {_tr[\"ActionPrice\"]:.5f} {_tr[\"CloseStep\"]:8} {_tr[\"ClosePrice\"]:.5f} \\\n",
    "                    {_tr[\"OpenBal\"]:.2f} {_tr[\"CloseBal\"]:.2f} {_tr[\"Status\"]:8}  {_tr[\"Info\"]:>8}  {_tr[\"PIPS\"]:4.0f} {_tr[\"SL\"]:4.0f} {_tr[\"PT\"]:4.0f} {_tr[\"DeltaStep\"]:8}\\n'\n",
    "\n",
    "                tr_lines_comma += f'{_tr[\"Ticket\"]},{_tr[\"Type\"]},{_tr[\"ActionTime\"]},{_tr[\"ActionStep\"]}, \\\n",
    "                    {_tr[\"ActionPrice\"]},{_tr[\"CloseTime\"]},{_tr[\"ClosePrice\"]}, \\\n",
    "                    {_tr[\"OpenBal\"]},{_tr[\"CloseBal\"]}, {_tr[\"Status\"]},{_tr[\"Info\"]},{_tr[\"PIPS\"]},{_tr[\"SL\"]},{_tr[\"PT\"]},{_tr[\"CloseStep\"]},{_tr[\"DeltaStep\"]}\\n'\n",
    "\n",
    "    log = _header_comma + tr_lines_comma\n",
    "    # log = f\"Step: {current_step}   Balance: {balance}, Profit: {profit} \\\n",
    "    #     MDD: {max_draw_down_pct}\\n{tr_lines_comma}\\n\"\n",
    "    if done_information:\n",
    "        log += done_information\n",
    "    if log:\n",
    "        # os.makedirs(log_filename, exist_ok=True)\n",
    "        dir_path = os.path.dirname(log_filename)\n",
    "        if dir_path and not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "        with open(log_filename, 'a+') as _f:\n",
    "            _f.write(log)\n",
    "            _f.close()\n",
    "\n",
    "    tr_lines = _header + tr_lines\n",
    "    if printout and tr_lines:\n",
    "        print(tr_lines)\n",
    "        if done_information:\n",
    "            print(done_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2b2c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForexTradingEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, file, cf, asset, features, sequence_length=24, logger_show=False, save_plot=False):\n",
    "        super(ForexTradingEnv, self).__init__()\n",
    "        # ကိန်းရှင်များကို စတင်သတ်မှတ်သည်။\n",
    "        self._initialize_parameters(file, cf, asset, features, sequence_length, logger_show, save_plot)\n",
    "        # Action နှင့် Observation Spaces ကို သတ်မှတ်သည်။\n",
    "        self._initialize_spaces()\n",
    "        # Environment ကို အစပြုအခြေအနေသို့ ပြန်လည်သတ်မှတ်သည်။\n",
    "        self.reset()\n",
    "\n",
    "    # ကိန်းရှင်များကို စတင်သတ်မှတ်သည်။\n",
    "    def _initialize_parameters(self, file, cf, asset, features, sequence_length, logger_show, save_plot):\n",
    "        # Params to variables\n",
    "        self.csv_file               =   file\n",
    "        self.cf                     =   cf\n",
    "        self.symbol_col             =   asset\n",
    "        self.features               =   features\n",
    "        self.sequence_length        =   sequence_length\n",
    "        self.logger_show            =   logger_show\n",
    "        self.save_plot              =   save_plot\n",
    "\n",
    "        self.data                   =   pd.read_csv(file)\n",
    "        # We use sequence transformer, so max steps will be this\n",
    "        self.max_steps              =   len(self.data) - self.sequence_length - 1\n",
    "\n",
    "        # Configs to variables\n",
    "        # Agent က Action က Continuous Action ကို Discrete Action သို့ပြောင်းပေးသော threshold\n",
    "        self.action_threshold       =   self.cf.env_parameters('action_threshold')\n",
    "        self.balance_initial        =   self.cf.env_parameters('balance')\n",
    "\n",
    "        # position close မဖြစ်သေးရင်\n",
    "        # buy ထားပြီး price up ဖြစ်နေရင် reward ပေး။ sell ထားပြီး price down ဖြစ်နေရင် reward ပေး\n",
    "        # position management မှာလည်း သုံး။\n",
    "        # buy မှာ မြတ်နေရင် tp အပေါ်ရွေ့ sl အပေါ်ရွေ့။  ရှုံးနေရင် tp အောက်ရွေ့ sl အပေါ်တင်,\n",
    "        # sell မှာ မြတ်နေရင် tp အောက်ရွေ့ sl အောက်ရွေ့။ ရှုံးနေရင် tp အပေါ်တင် sl အောက်ချ\n",
    "        self.good_position_reward_scale = self.cf.env_parameters(\"good_position_reward_scale\") # ဥပမာ: 0.01        \n",
    "        # ရည်ရွယ်ချက် ၂: SL/PT Trailing အတွက် တန်ဖိုး (Move Step Size)\n",
    "        self.trailing_distance = self.cf.env_parameters(\"trailing_stop_distance_points\")\n",
    "\n",
    "        # အရှုံးနဲ့အမြတ် မျှတမှုရှိတဲ့ trading performance အတွက် ပေးတဲ့ bonus reward 0.01\n",
    "        # self.consistency_reward = self.cf.env_parameters(\"consistency_reward\")\n",
    "        self.stop_loss = self.cf.symbol(self.symbol_col, \"stop_loss_max\")\n",
    "        self.profit_taken = self.cf.symbol(self.symbol_col, \"profit_taken_max\")\n",
    "        self.point = self.cf.symbol(self.symbol_col, \"point\")\n",
    "        self.transaction_fee = self.cf.symbol(self.symbol_col, \"transaction_fee\")\n",
    "        self.over_night_penalty = self.cf.symbol(self.symbol_col, \"over_night_penalty\")\n",
    "        self.max_current_holding = self.cf.symbol(self.symbol_col, \"max_current_holding\")\n",
    "        # Drawdown Penalty Factor\n",
    "        self.drawdown_penalty_factor = self.cf.env_parameters(\"drawdown_penalty_factor\")\n",
    "\n",
    "\n",
    "    # Action နှင့် Observation Spaces ကို သတ်မှတ်သည်။\n",
    "    def _initialize_spaces(self):\n",
    "        # Continuous actions: [1 -> 0.5] LONG | [0.5 -> -0.5] HOLD |[-0.5 -> -1] SHORT\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1,\n",
    "            high=1,\n",
    "            shape=(1,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        # Transformer သုံးထားသော features တွေရဲ့ previous sequence length candle ကိုပါ တပြိုင်တည်းကြည့်\n",
    "        obs_shape = (self.sequence_length, len(self.features))\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=obs_shape,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    # Environment ကို အစပြုအခြေအနေသို့ ပြန်လည်သတ်မှတ်သည်။\n",
    "    def reset(self, *, seed = None, options = None):\n",
    "        super().reset(seed=seed, options=options)\n",
    "\n",
    "        self.ticket_id          =   0\n",
    "        self.ttl_rewards        =   0 # total rewards\n",
    "\n",
    "        self.balance            =   self.balance_initial\n",
    "        self.positions          =   []\n",
    "\n",
    "        # equity tracking\n",
    "        self.equity_curve       =   [self.balance_initial] # Starting with initial balance\n",
    "        # အမြင့်ဆုံးရောက်ဖူးတဲ့ eq value\n",
    "        self.peak_equity        =   self.balance_initial # Start with initial balance as peak\n",
    "\n",
    "        self.max_drawdown       =   0.0\n",
    "        self.current_drawdown   =   0.0\n",
    "\n",
    "        # transformer အသုံးပြုထားခြင်းကြောင့်\n",
    "        self.current_step       =   self.sequence_length\n",
    "        logger.info(f\"--- Environment reset. Starting at step {self.current_step} --total rewards: {self.ttl_rewards}\")\n",
    "\n",
    "        observation             =   self._next_observation()\n",
    "        info                    =   {}\n",
    "        return  observation, info\n",
    "\n",
    "\n",
    "    # AI model အတွက် လက်ရှိ market condition ကိုကိုယ်စားပြုတဲ့ observation data ကို ပြင်ဆင်ပေးဖို့ဖြစ်ပါတယ်။\n",
    "    def _next_observation(self):\n",
    "        # သင့်တော်တဲ့ obs Historical Data ယူခြင်း\n",
    "        obs = self.data.iloc[\n",
    "            self.current_step - self.sequence_length: self.current_step\n",
    "        ][self.features].values\n",
    "\n",
    "        # NumPy array → PyTorch tensor ပြောင်းမယ်\n",
    "        # Data type ကို float32 လုပ်မယ်\n",
    "        # GPU/CPU device ပေါ်ကို ရွှေ့မယ်\n",
    "        obs = torch.tensor(obs, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Data Validation စစ်ဆေးခြင်း\n",
    "        # NaN (Not a Number) values ရှိမရှိစစ်မယ်\n",
    "        # Infinite values ရှိမရှိစစ်မယ်\n",
    "        # Invalid data ရှိရင် error ပြမယ်\n",
    "        if torch.isnan(obs).any() or torch.isinf(obs).any():\n",
    "            logger.error(f\"Invalid observation at step {self.current_step}\")\n",
    "            raise ValueError(f\"Invalid observation at step {self.current_step}\")\n",
    "\n",
    "        # NumPy Array ပြန်ပြောင်းခြင်း\n",
    "        # GPU memory → CPU memory ပြန်ရွှေ့မယ်\n",
    "        # PyTorch tensor → NumPy array ပြန်ပြောင်းမယ်\n",
    "        # Gym environment က NumPy arrays ကို ပိုကြိုက်တယ်။ Memory management အတွက် ကောင်းတယ်\n",
    "        return obs.cpu().numpy()  # obs\n",
    "\n",
    "\n",
    "\n",
    "    def _get_action_name(self, _action):\n",
    "        \"\"\"Convert continuous action to discrete action name\"\"\"\n",
    "        if _action >= self.action_threshold:\n",
    "            return \"BUY\"\n",
    "        elif _action <= -self.action_threshold:\n",
    "            return \"SELL\"\n",
    "        else:\n",
    "            return \"HOLD\"\n",
    "\n",
    "    def step(self, action):\n",
    "        _o, _h, _l, _c, _t, _day    =   self.data.iloc[self.current_step][['open', 'high', 'low', 'close', 'time', 'day']]\n",
    "        reward                      =   0 # ဒီ step အတွက် စုစုပေါင်း reward\n",
    "        position_reward             =   0 # Position ပိတ်ရင် ရတဲ့ reward\n",
    "        action_hold_reward          =   0 # Hold action အတွက် reward/penalty\n",
    "\n",
    "        _msg                        =   []\n",
    "        _action                     =   action[0] # action value eg. [0.75]\n",
    "        open_position               =   0\n",
    "        for position in self.positions:\n",
    "            if position['Status']   ==  0:\n",
    "                position_reward, closed, _msg   =   self._calculate_reward(position)\n",
    "                if not closed: open_position += 1  # Count what we already knew\n",
    "                reward += position_reward\n",
    "\n",
    "        # Continuous actions: [1 -> 0.5] LONG | [0.5 -> -0.5] HOLD |[-0.5 -> -1] SHORT\n",
    "        action_name = self._get_action_name(_action)\n",
    "\n",
    "        if open_position < self.max_current_holding and action_name in ['BUY', 'SELL']:\n",
    "            self.ticket_id  +=  1\n",
    "            position        =   {\n",
    "                \"Ticket\"        :   self.ticket_id,\n",
    "                \"Symbol\"        :   self.symbol_col,\n",
    "                \"ActionTime\"    :   _t,\n",
    "                \"Type\"          :   action_name,\n",
    "                \"Lot\"           :   1,\n",
    "                \"ActionPrice\"   :   _c,\n",
    "                \"SL\"            :   self.stop_loss,\n",
    "                \"PT\"            :   self.profit_taken,\n",
    "                \"MaxDD\"         :   0,\n",
    "                \"Swap\"          :   0.0,\n",
    "                \"CloseTime\"     :   \"\",\n",
    "                \"ClosePrice\"    :   0.0,\n",
    "                \"Point\"         :   self.point,\n",
    "                \"Reward\"        :   self.transaction_fee,\n",
    "                \"DateDuration\"  :   _day,\n",
    "                \"Status\"        :   0, # 0 is Position is currently OPEN and active\n",
    "                #\"PIPS\"          :   self.transaction_fee, # Price Interest Point (profit/loss ကို measure လုပ်တဲ့ unit)\n",
    "                \"PIPS\"          :   0,\n",
    "                \"ActionStep\"    :   self.current_step,\n",
    "                \"CloseStep\"     :   -1, # Step number when position closed, not close yet is -1\n",
    "                \"DeltaStep\"     :   0,\n",
    "                \"OpenBal\"       :   self.balance - 100,\n",
    "                \"CloseBal\"       :   0,\n",
    "                \"HighestPrice\"  :   _c,\n",
    "                \"LowestPrice\"   :   _c,\n",
    "            }\n",
    "\n",
    "            self.positions.append(position)\n",
    "            # do not use transaction_fee penalty\n",
    "            # reward = self.transaction_fee #open cost\n",
    "            # model က အလွန်အကျွံ position တွေ မဖွင့်မိအောင် ထိန်းချုပ်တဲ့ mechanism ဖြစ်ပါတယ်။\n",
    "            # Real trading မှာ margin requirement ရှိသလိုမျိုး\n",
    "            # Position ဖွင့်ရင် capital ချုပ်ငြားနေရတယ်\n",
    "            # Position ပိတ်တဲ့အခါ ပြန်ပေါင်းထည့်ပေးတယ်\n",
    "            self.balance -= 100 # hold up, this will make sure model can not open a lot of\n",
    "            _msg.append(f'Step:{self.current_step} Tkt:{position[\"Ticket\"]} {position[\"Type\"]} Rwd:{position[\"PIPS\"]} SL:{position[\"SL\"]} PT:{position[\"PT\"]}')\n",
    "\n",
    "        # HOLD Penalty ကို အလွန်သေးငယ်သော တန်ဖိုး\n",
    "        # (ဥပမာ: -0.0001) သို့ ပြောင်းပါ။ အကောင်းဆုံးမှာ \n",
    "        # Trading မလုပ်ခြင်းအတွက် Penalty မပေးဘဲ action_hold_reward = 0 ထားပါ။\n",
    "        elif open_position < self.max_current_holding and action_name in (\"HOLD\"):\n",
    "            action_hold_reward  =   0  # no open any position, encourage open position\n",
    "        else:\n",
    "            action_hold_reward  =   0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        reward              +=  action_hold_reward\n",
    "        #self.ttl_rewards    +=  reward\n",
    "\n",
    "        # Move to the next time step\n",
    "        self.current_step   +=  1\n",
    "\n",
    "        # check if episode is done\n",
    "        done                =   self.current_step > self.max_steps or self.balance <= 0\n",
    "\n",
    "        # get next observation\n",
    "        obs                 =   self._next_observation()\n",
    "        _msg.append(f'---idle----step:{self.current_step}, RF:{action_name} Action:{_action} Balance: {self.balance} reward:{reward} total_rewards:{self.ttl_rewards} position_reward:{position_reward} action_hold_reward:{action_hold_reward}')\n",
    "\n",
    "\n",
    "        current_equity = self._calculate_current_equity()\n",
    "        self.equity_curve.append(current_equity)\n",
    "        self._calculate_drawdown()  # This updates peak_equity and drawdowns\n",
    "\n",
    "        # =========================================================================\n",
    "        # START: Drawdown Penalty Logic\n",
    "        # =========================================================================\n",
    "        # self.current_drawdown သည် Percentage (0.0 မှ 1.0) ဖြစ်သည်။\n",
    "\n",
    "        \n",
    "        drawdown_penalty = self.current_drawdown * self.drawdown_penalty_factor        \n",
    "        # Reward တွင် နုတ်ပေးခြင်း\n",
    "        reward -= drawdown_penalty\n",
    "        \n",
    "        # Log the penalty for debugging\n",
    "        _msg.append(f'Drawdown Penalty: -{drawdown_penalty:.4f} (DD:{self.current_drawdown:.4f})')\n",
    "        # =========================================================================\n",
    "        # END: Drawdown Penalty Logic\n",
    "        # =========================================================================\n",
    "        # Drawdown Penalty နုတ်ပြီးမှသာ စုစုပေါင်း Reward ကို အပ်ဒိတ်လုပ်ပါ\n",
    "        self.ttl_rewards += reward  # <--- ဤနေရာတွင် ပြန်ထည့်ပါ\n",
    "\n",
    "\n",
    "        if done:\n",
    "            buy_positions = [p for p in self.positions if p[\"Type\"] == \"BUY\"]\n",
    "            sell_positions = [p for p in self.positions if p[\"Type\"] == \"SELL\"]\n",
    "\n",
    "            buy_count = len(buy_positions)\n",
    "            sell_count = len(sell_positions)\n",
    "            total_positions = len(self.positions)\n",
    "\n",
    "            # Calculate win rates\n",
    "            buy_wins = len([p for p in buy_positions if p[\"PIPS\"] > 0])\n",
    "            sell_wins = len([p for p in sell_positions if p[\"PIPS\"] > 0])\n",
    "\n",
    "            buy_win_rate = buy_wins / buy_count if buy_count > 0 else 0\n",
    "            sell_win_rate = sell_wins / sell_count if sell_count > 0 else 0\n",
    "\n",
    "            _m = f'--- Positions: {total_positions} (Buy:{buy_count}, Sell:{sell_count}) | '\n",
    "            _m += f'WinRates: Buy:{buy_win_rate:.1%}, Sell:{sell_win_rate:.1%} | '\n",
    "            _m += f'TotalRewards: {self.ttl_rewards} Balance: {self.balance}'\n",
    "\n",
    "            logger.info(_m)\n",
    "            _msg.append(_m)\n",
    "\n",
    "        # Additional info\n",
    "        if self.logger_show:\n",
    "            for _m in _msg:\n",
    "                logger.info(_m)\n",
    "        info = {\n",
    "            \"info\": _msg,\n",
    "            \"sharpe_ratio\": self._calculate_sharpe(),  # ✅ Now works!\n",
    "            \"max_drawdown\": self.max_drawdown,         # ✅ Now accurate!\n",
    "            \"current_equity\": current_equity,          # ✅ For debugging\n",
    "            \"peak_equity\": self.peak_equity,           # ✅ For debugging\n",
    "            \"equity_curve_length\": len(self.equity_curve)  # ✅ Monitor growth\n",
    "        }\n",
    "        truncated = False\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _calculate_reward(self, position):\n",
    "        _o, _h, _l, _c, _t, _day    =   self.data.iloc[self.current_step][['open', 'high', 'low', 'close', 'time', 'day']]\n",
    "        _msg                        =   []\n",
    "\n",
    "        entry_price                 =   position['ActionPrice']\n",
    "        direction                   =   position['Type']\n",
    "        profit_target_price         =   entry_price + position['PT']/ self.point if direction == 'BUY' else entry_price - position['PT']/self.point\n",
    "        stop_loss_price             =   entry_price + position['SL']/ self.point if direction == 'BUY' else entry_price - position['SL']/self.point\n",
    "        closed                      =   False\n",
    "        close_position_reward       =   0.0\n",
    "        good_position_reward        =   0.0\n",
    "\n",
    "        # Check for stoploss hit\n",
    "        if (direction == 'BUY' and _l <= stop_loss_price) or (direction == 'SELL' and _h >= stop_loss_price):\n",
    "            close_position_reward   =   position['SL'] # position sl က minus value ဖြစ်တယ်\n",
    "\n",
    "            position['CloseTime']   =   _t\n",
    "            position['ClosePrice']  =   stop_loss_price\n",
    "            position['Status']      =   1   # Status က open ဆို 0 close ဆို 1\n",
    "            position['CloseStep']   =   self.current_step\n",
    "            position['PIPS']        =   close_position_reward - self.transaction_fee\n",
    "            position['DeltaStep']   =   self.current_step - position['ActionStep']\n",
    "            position['Info']        =   f'{profit_target_price:.5f} | {stop_loss_price:.5f}'\n",
    "\n",
    "            self.balance            +=  100 + position['PIPS'] # return 100 is margin hold\n",
    "            position['CloseBal']    =   self.balance\n",
    "            closed                  =   True\n",
    "            _msg.append(f'Step:{self.current_step} Tkt:{position[\"Ticket\"]}: Rwd:{position[\"PIPS\"]}, SL:{position[\"SL\"]}, DeltaStep:{position[\"DeltaStep\"]}')\n",
    "\n",
    "        elif (direction == 'BUY' and _h >= profit_target_price) or (direction == 'SELL' and _l <= profit_target_price):\n",
    "            close_position_reward   =    position['PT'] # position tp က plus value ဖြစ်တယ်\n",
    "\n",
    "            position['CloseTime']   =   _t\n",
    "            position['ClosePrice']  =   profit_target_price\n",
    "            position['Status']      =   2   # Status က open ဆို 0 close ဆို 1\n",
    "            position['CloseStep']   =   self.current_step\n",
    "            position['PIPS']        =   close_position_reward - self.transaction_fee\n",
    "            position['DeltaStep']   =   self.current_step - position['ActionStep']\n",
    "            position['Info']        =   f'{profit_target_price:.5f} | {stop_loss_price:.5f}'\n",
    "\n",
    "            self.balance            +=  100 + position['PIPS'] # return 100 is margin hold\n",
    "            position['CloseBal']    =   self.balance\n",
    "            closed                  =   True\n",
    "            _msg.append(f'Step:{self.current_step} Tkt:{position[\"Ticket\"]}: Rwd:{position[\"PIPS\"]}, SL:{position[\"SL\"]}, DeltaStep:{position[\"DeltaStep\"]}')\n",
    "\n",
    "        else:\n",
    "            if self.current_step + 5 + self.sequence_length >= len(self.data):\n",
    "                close_position_reward   =   (_c - position[\"ActionPrice\"] if direction == 'BUY' else position[\"ActionPrice\"] - _c)* self.point\n",
    "\n",
    "                position['CloseTime']   =   _t\n",
    "                position['ClosePrice']  =   _c\n",
    "                position['Status']      =   3   # Status က open ဆို 0 close ဆို 1, force close 2\n",
    "                position['CloseStep']   =   self.current_step\n",
    "                position['PIPS']        =   close_position_reward - self.transaction_fee\n",
    "                position['DeltaStep']   =   self.current_step - position['ActionStep']\n",
    "                position['Info']        =   f'{profit_target_price:.5f} | {stop_loss_price:.5f}'\n",
    "                position['CloseBal']    =   self.balance\n",
    "                self.balance            +=  100 + position[\"PIPS\"] # return 100 is margin hold\n",
    "                closed                  =   True\n",
    "                _msg.append(f'Step:{self.current_step} Tkt:{position[\"Ticket\"]}: Rwd:{position[\"PIPS\"]}, Cls:End, DeltaStep:{position[\"DeltaStep\"]}')\n",
    "\n",
    "            else:\n",
    "                # =========================================================================\n",
    "                # Real Trailing Stop Logic (အမြင့်ဆုံး ရောက်ဖူးသော ဈေးနှုန်းကို မှတ်တမ်းတင်ခြင်း)\n",
    "                # =========================================================================\n",
    "                # 1. Highest/Lowest Price Update\n",
    "\n",
    "                if direction == \"BUY\":\n",
    "                  # Buy position အတွက် အမြင့်ဆုံး ရောက်ဖူးသော ဈေးနှုန်းကို မှတ်တမ်းတင်\n",
    "                  if _c > position[\"HighestPrice\"]:\n",
    "                      position[\"HighestPrice\"] = _c\n",
    "\n",
    "                  # 2. New SL Target Price (Trailing Price) ကို တွက်ချက်ခြင်း\n",
    "                  # New_SL_Price = HighestPrice - (Trailing Distance Pips ကို Price Change သို့ ပြောင်း)\n",
    "                  trailing_price = position[\"HighestPrice\"] - self.trailing_distance / self.point\n",
    "\n",
    "                  # 3. SL ကို အဆင့်မြှင့်တင်ခြင်း\n",
    "                  # လက်ရှိ SL ထက် ပိုကောင်းမှသာ ရွေ့ပါ\n",
    "                  if trailing_price > stop_loss_price: \n",
    "\n",
    "                      stop_loss_price = trailing_price\n",
    "                      # SL_Price အသစ်ကို Points သို့ ပြန်ပြောင်းပြီး position['SL'] ကို အပ်ဒိတ်လုပ်ပါ\n",
    "                      position[\"SL\"] = (stop_loss_price - entry_price) * self.point \n",
    "                      trailing_happened = True\n",
    "                  else:\n",
    "                      trailing_happened = False\n",
    "\n",
    "\n",
    "                elif direction == \"SELL\":\n",
    "                  # Sell position အတွက် အနိမ့်ဆုံး ရောက်ဖူးသော ဈေးနှုန်းကို မှတ်တမ်းတင်\n",
    "                  if _c < position[\"LowestPrice\"]:\n",
    "                      position[\"LowestPrice\"] = _c\n",
    "                  \n",
    "                  # New SL Target Price (Trailing Price) ကို တွက်ချက်ခြင်း\n",
    "                  trailing_price = position[\"LowestPrice\"] + self.trailing_distance / self.point\n",
    "                  \n",
    "                  # SL ကို အဆင့်မြှင့်တင်ခြင်း\n",
    "                  if trailing_price < stop_loss_price: \n",
    "                      stop_loss_price = trailing_price\n",
    "                      # SL_Price အသစ်ကို Points သို့ ပြန်ပြောင်းပြီး position['SL'] ကို အပ်ဒိတ်လုပ်ပါ\n",
    "                      position[\"SL\"] = (entry_price - stop_loss_price) * self.point\n",
    "                      trailing_happened = True\n",
    "                  else:\n",
    "                      trailing_happened = False\n",
    "\n",
    "                # =========================================================================\n",
    "                # Reward Logic (Trailing လုပ်ခြင်းအတွက် Bonus ပေးခြင်း)\n",
    "                # =========================================================================\n",
    "                # Reward Sign ကို ယခင်အတိုင်း တွက်ပါ။\n",
    "                delta = _c - entry_price\n",
    "                if direction == \"BUY\":\n",
    "                    reward_sign = 1 if delta >= 0 else -1\n",
    "                elif direction == \"SELL\":\n",
    "                    reward_sign = -1 if delta >= 0 else 1\n",
    "\n",
    "                good_position_reward = reward_sign * self.good_position_reward_scale \n",
    "                \n",
    "                # Trailing အမှန်တကယ် ဖြစ်သွားမှသာ Bonus Reward ကို ပေးပါ\n",
    "                if trailing_happened: \n",
    "                    good_position_reward += 0.001\n",
    "\n",
    "                position['Info']        =   f'{profit_target_price:.5f} | {stop_loss_price:.5f}'\n",
    "                position['CloseBal']    =   self.balance\n",
    "                _msg.append(f'Step:{self.current_step} Tkt:{position[\"Ticket\"]}: NO_Close, PT:{position[\"PT\"]}, SL:{position[\"SL\"]}')\n",
    "\n",
    "        return close_position_reward + good_position_reward, closed, _msg\n",
    "\n",
    "\n",
    "    def _calculate_sharpe(self, risk_free_rate=0.0):\n",
    "        \"\"\"Calculate Sharpe ratio for the current episode\"\"\"\n",
    "        if len(self.equity_curve) < 2:\n",
    "            return 0.0\n",
    "\n",
    "        returns = np.diff(self.equity_curve) / self.equity_curve[:-1]\n",
    "\n",
    "        if np.std(returns) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        sharpe = (np.mean(returns) - risk_free_rate) / np.std(returns)\n",
    "        return float(sharpe * np.sqrt(288))  # Annualized (5-min bars → 288/day)\n",
    "\n",
    "    def _calculate_drawdown(self):\n",
    "        \"\"\"Update max drawdown during episode\"\"\"\n",
    "        current_equity          =   self.equity_curve[-1]\n",
    "        self.peak_equity        =   max(self.peak_equity, current_equity)\n",
    "        self.current_drawdown   =   (self.peak_equity - current_equity) / self.peak_equity\n",
    "        self.max_drawdown       =   max(self.max_drawdown, self.current_drawdown)\n",
    "\n",
    "\n",
    "    def _calculate_current_equity(self):\n",
    "        \"\"\"Calculate total current equity (balance + unrealized P/L)\"\"\"\n",
    "        total_equity = self.balance  # Start with cash balance\n",
    "\n",
    "        # Add unrealized P/L from open positions\n",
    "        for position in self.positions:\n",
    "            if position['Status'] == 0:  # Only open positions\n",
    "                current_price = self.data.iloc[self.current_step][\"close\"]\n",
    "                entry_price = position['ActionPrice']\n",
    "\n",
    "                if position['Type'] == 'BUY':\n",
    "                    unrealized_pnl = (current_price - entry_price) * self.point\n",
    "                else:  # Sell\n",
    "                    unrealized_pnl = (entry_price - current_price) * self.point\n",
    "\n",
    "                total_equity += unrealized_pnl\n",
    "\n",
    "        return total_equity\n",
    "\n",
    "    def render(self, mode='human', title=None, **kwargs):\n",
    "        # Render the environment to the screen\n",
    "        if mode in ('human', 'file'):\n",
    "            log_header      =   True\n",
    "            printout        =   False\n",
    "            if mode == 'human':\n",
    "                printout    =   True\n",
    "\n",
    "            log_file = self.csv_file.replace(\"split/\", \"log/\")\n",
    "            pm = {\n",
    "                \"log_header\": log_header,\n",
    "                \"log_filename\": log_file,\n",
    "                \"printout\": printout,\n",
    "                \"balance\": self.balance,\n",
    "                \"balance_initial\": self.balance_initial,\n",
    "                \"transaction_close_this_step\": self.positions,\n",
    "                \"done_information\": False\n",
    "            }\n",
    "            render_to_file(**pm)\n",
    "            if log_header:\n",
    "                    log_header = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5585228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import numpy as np\n",
    "\n",
    "class TrainingMetricsCallback(BaseCallback):\n",
    "    def __init__(self, check_freq=1000, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.sharpe_ratios = []\n",
    "        self.drawdowns = []\n",
    "        self.episode_count = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Track metrics only when episodes complete\n",
    "        if \"sharpe\" in self.locals['infos'][0] and \"max_drawdown\" in self.locals['infos'][0]:\n",
    "            self.episode_count += 1\n",
    "            self.sharpe_ratios.append(self.locals['infos'][0]['sharpe'])\n",
    "            self.drawdowns.append(self.locals['infos'][0]['max_drawdown'])\n",
    "\n",
    "            # Log to tensorboard every N episodes\n",
    "            if self.episode_count % 10 == 0:\n",
    "                self.logger.record('train/mean_sharpe', np.mean(self.sharpe_ratios[-10:]))\n",
    "                self.logger.record('train/max_drawdown', np.mean(self.drawdowns[-10:]))\n",
    "                self.logger.record('train/episodes', self.episode_count)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd75d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from src.utils.transformer import linear_schedule\n",
    "from src.utils.transformer import CustomCombinedExtractor\n",
    "from stable_baselines3 import PPO\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "BASE_SEED = 42\n",
    "number_envs = 4\n",
    "# Stable-Baselines3 ရဲ့ Global Seed ကို သတ်မှတ်ပါ\n",
    "set_random_seed(BASE_SEED)\n",
    "\n",
    "\n",
    "def single_csv_training(csv_file, env_config_file, asset, model_name ='', cf = None, number_envs = 1):\n",
    "    features = cf.env_parameters(\"observation_list\")\n",
    "    sequence_length = cf.env_parameters(\"backward_window\")\n",
    "    print(features)\n",
    "    lr_schedule = linear_schedule(3e-4, 1e-5, total_timesteps=1e6)\n",
    "    policy_kwargs = dict(\n",
    "        # Repo ရဲ့ custom feature extractor (Transformer + MLP ပေါင်းထားတာ၊ time series data အတွက် သင့်တော်တယ်)။\n",
    "        features_extractor_class=CustomCombinedExtractor,\n",
    "        # features_extractor_kwargs: Sequence length ကို ထည့်။\n",
    "        features_extractor_kwargs=dict(sequence_length=sequence_length),\n",
    "        # net_arch: Actor (pi - policy network) နဲ့ Critic (vf - value function) နှစ်ခု လုံး အတွက် hidden layers [256, 256] သုံး။\n",
    "        net_arch=[dict(pi=[256, 256], vf=[256, 256])],\n",
    "        # Activation function အနေနဲ့ ReLU သုံး (non-linear ဖြစ်အောင်)။\n",
    "        activation_fn=nn.ReLU,\n",
    "        # Orthogonal initialization မသုံး (financial data မှာ ပိုကောင်း တယ်လို့ comment မှာ ရေး ထားတယ်၊ ဒါက weights ကို ပိုရိုးရှင်း စ လုပ်တယ်)။\n",
    "        ortho_init=False # better for finacial data\n",
    "    )\n",
    "    # env = ForexTradingEnv(csv_file, cf, asset, features=features, sequence_length=sequence_length, logger_show= True)\n",
    "\n",
    "    # Environment Factories များ ဖန်တီးပါ\n",
    "    env_fns = [\n",
    "        lambda: ForexTradingEnv(\n",
    "            csv_file,\n",
    "            cf,\n",
    "            asset,\n",
    "            features=features,\n",
    "            sequence_length=sequence_length,\n",
    "            logger_show=True\n",
    "        )\n",
    "        for _ in range(number_envs)\n",
    "    ]\n",
    "    # DummyVecEnv ကို တည်ဆောက်ပါ\n",
    "    env = DummyVecEnv(env_fns)\n",
    "    # ဤနေရာသည် အဓိကကျသည်။ ၎င်းက Environment တစ်ခုချင်းစီကို\n",
    "    # BASE_SEED, BASE_SEED+1, BASE_SEED+2... စသည်ဖြင့် Seed များ သတ်မှတ်ပေးပြီး\n",
    "    # ၎င်းတို့၏ reset() ကို ပြန်လည်ခေါ်ပေးလိမ့်မည်။\n",
    "    env.seed(BASE_SEED)\n",
    "    # env.logger_show = True\n",
    "    if model_name:\n",
    "        model = PPO.load(model_name, env=env, learning_rate=lr_schedule)\n",
    "    else:\n",
    "        model = PPO(\n",
    "            # 'CnnPolicy' , # support GPU\n",
    "            'MlpPolicy', # CPU only\n",
    "            env,\n",
    "            device='cuda',\n",
    "            verbose=1,\n",
    "            vf_coef=0.5,  # Increase value loss weight\n",
    "            target_kl=0.05,  # Add target KL for early stopping\n",
    "            normalize_advantage=True,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            learning_rate=lr_schedule,  # Reduced learning rate\n",
    "            max_grad_norm=0.5,    # Gradient clipping,\n",
    "            seed=BASE_SEED,\n",
    "        )\n",
    "\n",
    "    # Train the agent\n",
    "    logger.info(\"Starting model training...\")\n",
    "    callback = TrainingMetricsCallback()\n",
    "    model.learn(\n",
    "        total_timesteps=500000,\n",
    "        callback=callback,\n",
    "        tb_log_name=f\"/content/drive/MyDrive/data/log/{asset}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "    )\n",
    "    logger.info(\"Model training complete\")\n",
    "    model_filename = csv_file.replace(\"split/\", \"model/\").replace(\".csv\", \"_single_test.zip\")\n",
    "    model.save(model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3215a21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean_std_open', 'mean_std_high', 'mean_std_low', 'mean_std_close', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'london_session', 'ny_session', 'overlap_session', 'macd', 'boll_ub', 'boll_lb', 'rsi_30', 'atr', 'volatility_ratio', 'close_30_sma', 'close_60_sma', 'returns_5', 'returns_24']\n",
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n",
      "c:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 260  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 31   |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 14            |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 1111          |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.004656008   |\n",
      "|    clip_fraction        | 0.0285        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 3.0398369e-05 |\n",
      "|    learning_rate        | 1.48e-05      |\n",
      "|    loss                 | 1.42e+05      |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | 0.00102       |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 3.28e+05      |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 8              |\n",
      "|    iterations           | 3              |\n",
      "|    time_elapsed         | 2751           |\n",
      "|    total_timesteps      | 24576          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.00084548135  |\n",
      "|    clip_fraction        | 4.88e-05       |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.42          |\n",
      "|    explained_variance   | 0.000104904175 |\n",
      "|    learning_rate        | 1.95e-05       |\n",
      "|    loss                 | 1.9e+04        |\n",
      "|    n_updates            | 20             |\n",
      "|    policy_gradient_loss | 0.000405       |\n",
      "|    std                  | 0.998          |\n",
      "|    value_loss           | 1.75e+05       |\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00masset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_2022_12.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# f'./data/model/{asset}/weekly/{asset}_2023_71'\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m single_csv_training(csv_file\u001b[38;5;241m=\u001b[39mcsv_file, env_config_file \u001b[38;5;241m=\u001b[39menv_config_file, asset\u001b[38;5;241m=\u001b[39m asset, model_name\u001b[38;5;241m=\u001b[39mmodel_name, cf\u001b[38;5;241m=\u001b[39mcf, number_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 47\u001b[0m, in \u001b[0;36msingle_csv_training\u001b[1;34m(csv_file, env_config_file, asset, model_name, cf, number_envs)\u001b[0m\n\u001b[0;32m     45\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting model training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m callback \u001b[38;5;241m=\u001b[39m TrainingMetricsCallback()\n\u001b[1;32m---> 47\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[0;32m     48\u001b[0m     total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500000\u001b[39m,\n\u001b[0;32m     49\u001b[0m     callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m     50\u001b[0m     tb_log_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.drive/MyDrive/data/log/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00masset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     51\u001b[0m )\n\u001b[0;32m     52\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel training complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m model_filename \u001b[38;5;241m=\u001b[39m csv_file\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_single_test.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[0;32m    316\u001b[0m         total_timesteps\u001b[38;5;241m=\u001b[39mtotal_timesteps,\n\u001b[0;32m    317\u001b[0m         callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    318\u001b[0m         log_interval\u001b[38;5;241m=\u001b[39mlog_interval,\n\u001b[0;32m    319\u001b[0m         tb_log_name\u001b[38;5;241m=\u001b[39mtb_log_name,\n\u001b[0;32m    320\u001b[0m         reset_num_timesteps\u001b[38;5;241m=\u001b[39mreset_num_timesteps,\n\u001b[0;32m    321\u001b[0m         progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:313\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    315\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:217\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_sde:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mreset_noise(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[1;32m--> 217\u001b[0m values, log_prob, entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mevaluate_actions(rollout_data\u001b[38;5;241m.\u001b[39mobservations, actions)\n\u001b[0;32m    218\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Normalize advantage\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:730\u001b[0m, in \u001b[0;36mActorCriticPolicy.evaluate_actions\u001b[1;34m(self, obs, actions)\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;124;03mEvaluate actions according to the current policy,\u001b[39;00m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;124;03mgiven the observations.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;124;03m    and entropy of the action distribution.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;66;03m# Preprocess the observation if needed\u001b[39;00m\n\u001b[1;32m--> 730\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_features(obs)\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[0;32m    732\u001b[0m     latent_pi, latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor(features)\n",
      "File \u001b[1;32mc:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:672\u001b[0m, in \u001b[0;36mActorCriticPolicy.extract_features\u001b[1;34m(self, obs, features_extractor)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;124;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;124;03m    features for the actor and the features for the critic.\u001b[39;00m\n\u001b[0;32m    670\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[1;32m--> 672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mextract_features(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_extractor \u001b[38;5;28;01mif\u001b[39;00m features_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m features_extractor)\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m features_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:131\u001b[0m, in \u001b[0;36mBaseModel.extract_features\u001b[1;34m(self, obs, features_extractor)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m:return: The extracted features\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    130\u001b[0m preprocessed_obs \u001b[38;5;241m=\u001b[39m preprocess_obs(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space, normalize_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_images)\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features_extractor(preprocessed_obs)\n",
      "File \u001b[1;32mc:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Finance\\git_tradesformer\\src\\utils\\transformer.py:149\u001b[0m, in \u001b[0;36mCustomCombinedExtractor.forward\u001b[1;34m(self, observations)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# Apply layer normalization\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Apply layer normalization, ဝင်လာတဲ့ observations ကို Transformer ရဲ့ device ပေါ်ကို ရွှေ့ပါ\u001b[39;00m\n\u001b[0;32m    147\u001b[0m normalized_observations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm_before(observations\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)) \u001b[38;5;66;03m# Ensure float type\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(normalized_observations)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(x)\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(x)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    151\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid values in transformer output\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Finance\\git_tradesformer\\src\\utils\\transformer.py:91\u001b[0m, in \u001b[0;36mTimeSeriesTransformer.forward\u001b[1;34m(self, src)\u001b[0m\n\u001b[0;32m     88\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(src) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Pass through the transformer encoder\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(src)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Pass through the decoder layer\u001b[39;00m\n\u001b[0;32m     94\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(output)\n",
      "File \u001b[1;32mc:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:524\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    521\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 524\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(\n\u001b[0;32m    525\u001b[0m         output,\n\u001b[0;32m    526\u001b[0m         src_mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[0;32m    527\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m    528\u001b[0m         src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask_for_layers,\n\u001b[0;32m    529\u001b[0m     )\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    532\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.0\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32mc:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:931\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_first:\n\u001b[0;32m    928\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(\n\u001b[0;32m    929\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x), src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal\n\u001b[0;32m    930\u001b[0m     )\n\u001b[1;32m--> 931\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    933\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[0;32m    934\u001b[0m         x\n\u001b[0;32m    935\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m    936\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:962\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 962\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(x))))\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[1;32mc:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[1;32mc:\\Users\\mgmgn\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1422\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 1422\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, p, training)\n\u001b[0;32m   1423\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.utils.read_config import EnvConfig\n",
    "\n",
    "asset = \"EURUSD\"\n",
    "env_config_file = '/content/drive/MyDrive/configure.json'\n",
    "cf = EnvConfig(env_config_file)\n",
    "split_cfg = cf.data_processing_parameters(\"train_eval_split\")\n",
    "base_path = split_cfg[\"base_path\"].format(symbol=asset)\n",
    "csv_file = f\"{base_path}/{split_cfg[\"train_dir\"]}/{asset}_2022_12.csv\"\n",
    "model_name = '' # f'./data/model/{asset}/weekly/{asset}_2023_71'\n",
    "single_csv_training(csv_file=csv_file, env_config_file =env_config_file, asset= asset, model_name=model_name, cf=cf, number_envs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834ae317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "def eval(data_directory, env_config_file, model_file, asset, mode='human', save_plot=False, sequence_length=None):\n",
    "    csv_files = glob.glob(os.path.join(data_directory, \"*.csv\"))\n",
    "    cf = EnvConfig(env_config_file)\n",
    "    features = cf.env_parameters(\"observation_list\")\n",
    "    if sequence_length is None:\n",
    "        sequence_length = cf.env_parameters(\"backward_window\")\n",
    "    print(f\"Using sequence_length: {sequence_length}\")\n",
    "\n",
    "    # Device setup - CONSISTENT device usage\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Force CPU if you're having device issues\n",
    "    # device = torch.device('cpu')\n",
    "    # print(\"Forcing CPU usage\")\n",
    "\n",
    "    file = '/content/drive/MyDrive/data/split/EURUSD/train/EURUSD_2022_14.csv'\n",
    "    env = ForexTradingEnv(file, cf, asset, features, sequence_length, save_plot=False)\n",
    "    # aggregator = ActionAggregator()\n",
    "    #aggregator = ActionAggregatorOptimized(base_window_size=10, volatility_threshold=0.01)\n",
    "\n",
    "    model = PPO.load(model_file, env=env, device=device)\n",
    "    model.policy.to(device)\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    done = False\n",
    "    total_buy = 0\n",
    "    total_sell = 0\n",
    "    total_rewards = 0\n",
    "    step = 0\n",
    "    step_log_data = []\n",
    "    while not done:\n",
    "        action, _states = model.predict(observation) #deterministic=True\n",
    "\n",
    "        # aggregated_action, reward = aggregator.add_action(action)\n",
    "        print(f\"Action: {action}\") # {aggregated_action}\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        print(f\"info: {info}\") # {aggregated_action}\n",
    "        # 3. Data ကို စုဆောင်းပါ\n",
    "        # step_log_data.append({\n",
    "        #     'Step': step,\n",
    "        #     'Action': action.tolist(),\n",
    "        #     'Reward': reward,\n",
    "        #     'New_Observation_State': observation.tolist(),\n",
    "        #     'Done': done,\n",
    "        #     'Info': info # Info dict ကို လိုအပ်သလို ထည့်နိုင်ပါတယ်\n",
    "        # })\n",
    "        step += 1\n",
    "        total_rewards += reward\n",
    "        if action >= 0.5: total_buy += 1\n",
    "        if action <= -0.5: total_sell += 1\n",
    "\n",
    "    env.render(mode = mode)\n",
    "    # print(f'------rewards:{total_rewards}-----buy:{total_buy}--sell:{total_sell}------')\n",
    "    # # စုဆောင်းထားတဲ့ data ကို DataFrame အဖြစ် ပြောင်းပါ\n",
    "    # df = pd.DataFrame(step_log_data)\n",
    "\n",
    "    # # CSV ဖိုင်အဖြစ် သိမ်းပါ\n",
    "    # csv_filename = \"/content/drive/MyDrive/data/rl_prediction_steps_log.csv\"\n",
    "    # df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    # print(f\"Data ကို {csv_filename} ထဲမှာ သိမ်းဆည်းပြီးပါပြီ။\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fcc02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset = \"EURUSD\"\n",
    "env_config_file = '/content/drive/MyDrive/configure.json'\n",
    "model_file = f'/content/drive/MyDrive/data/model/{asset}/train/{asset}_2022_12_single_test.zip'\n",
    "data_directory = f\"/content/drive/MyDrive/data/split/{asset}/train\"\n",
    "save_plot = False\n",
    "\n",
    "eval(data_directory, env_config_file, model_file, asset, mode='human', save_plot=save_plot)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
